{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\nuevas_investigaciones_alimentos_2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "import io\n",
    "import itertools\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Obtener la ruta del directorio actual\n",
    "os.chdir('..')\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Construir la ruta relativa al directorio que quieres agregar\n",
    "relative_dir = os.path.join(current_dir, 'mis_pkgs/')\n",
    "\n",
    "# Agregar la ruta relativa al sys.path\n",
    "sys.path.insert(0, relative_dir)\n",
    "\n",
    "from MIOPATIA_db import DB_management as db \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a quedarme con los 50 atunes P1 para obtener conjunto de training y validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add, Activation, Concatenate, Conv2D, Dropout \n",
    "from tensorflow.keras.layers import Flatten, Input, GlobalAveragePooling2D, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "__version__ = '0.0.1'\n",
    "\n",
    "\n",
    "def SqueezeNet(input_shape, nb_classes, use_bypass=False, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.0\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        use_bypass   : if true, bypass connections will be created at fire module 3, 5, 7, and 9 (default: False)\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(96*compression), (7,7), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool4')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire5', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool8')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(64*compression), name='fire9', use_bypass=use_bypass)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "    x = output(x, nb_classes)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "def SqueezeNet_11(input_shape, nb_classes, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.1\n",
    "    \n",
    "    2.4x less computation over SqueezeNet 1.0 implemented above.\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(64*compression), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool3')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    x = create_fire_module(x, int(32*compression), name='fire5')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool5')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire9')\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Creating last conv10\n",
    "    x = output(x, nb_classes)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "def output(x, nb_classes):\n",
    "    x = Conv2D(nb_classes, (1,1), strides=(1,1), padding='valid', name='conv10')(x)\n",
    "    x = GlobalAveragePooling2D(name='avgpool10')(x)\n",
    "    x = Activation(\"softmax\", name='softmax')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_fire_module(x, nb_squeeze_filter, name, use_bypass=False):\n",
    "    \"\"\"\n",
    "    Creates a fire module\n",
    "    \n",
    "    Arguments:\n",
    "        x                 : input\n",
    "        nb_squeeze_filter : number of filters of squeeze. The filtersize of expand is 4 times of squeeze\n",
    "        use_bypass        : if True then a bypass will be added\n",
    "        name              : name of module e.g. fire123\n",
    "    \n",
    "    Returns:\n",
    "        x                 : returns a fire module\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_expand_filter = 4 * nb_squeeze_filter\n",
    "    squeeze    = Conv2D(nb_squeeze_filter,(1,1), activation='relu', padding='same', name='%s_squeeze'%name)(x)\n",
    "    expand_1x1 = Conv2D(nb_expand_filter, (1,1), activation='relu', padding='same', name='%s_expand_1x1'%name)(squeeze)\n",
    "    expand_3x3 = Conv2D(nb_expand_filter, (3,3), activation='relu', padding='same', name='%s_expand_3x3'%name)(squeeze)\n",
    "    \n",
    "    axis = get_axis()\n",
    "    x_ret = Concatenate(axis=axis, name='%s_concatenate'%name)([expand_1x1, expand_3x3])\n",
    "    \n",
    "    if use_bypass:\n",
    "        x_ret = Add(name='%s_concatenate_bypass'%name)([x_ret, x])\n",
    "        \n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def get_axis():\n",
    "    axis = -1 if K.image_data_format() == 'channels_last' else 1\n",
    "    return axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 5)\n"
     ]
    }
   ],
   "source": [
    "filename = \"COPIA_PANDAS\\hdf_lomosP2_trainval_filtrado_def_good_ampliado_the_best7.hdf\"\n",
    "with pd.HDFStore(filename,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e1  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e1 = pre_p_e1.loc[pre_p_e1['Pollo'] != 0]\n",
    "    # p_e =pre_p_e1.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_train=np.zeros((pre_p_e1.shape[0],220,8))\n",
    "    y_train=np.zeros((pre_p_e1.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e1.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_train[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_train[x]=target\n",
    "        y_train_to_categorical = to_categorical(y_train)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_train_filtrado = X_train\n",
    "#y_train_filtrado = y_train\n",
    "y_train_filtrado = y_train_to_categorical\n",
    "\n",
    "# print(X_train_filtrado.shape)\n",
    "# print(y_train_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_2d = X_train_filtrado.reshape(-1, X_train_filtrado.shape[-1])\n",
    "normalized_data_2d = scaler.fit_transform(data_2d)\n",
    "X_train_Normalizado=normalized_data_2d.reshape(X_train_filtrado.shape)\n",
    "y_train_Normalizado=y_train_filtrado # los valores ya estaban normalizados\n",
    "print(y_train_Normalizado.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 220, 8)\n",
      "(89, 5)\n",
      "[[1.45375657e-01 4.74795267e-01 2.80281244e-02 ... 5.25204733e-01\n",
      "  1.45375842e-01 6.87729402e-01]\n",
      " [1.04024516e-01 4.57433188e-01 2.91808316e-02 ... 5.42566812e-01\n",
      "  1.04024006e-01 6.97578524e-01]\n",
      " [8.71835342e-02 4.33210717e-01 3.04208802e-02 ... 5.66789283e-01\n",
      "  8.71821912e-02 6.95625305e-01]\n",
      " ...\n",
      " [2.02802467e-04 3.24435285e-01 2.62623498e-02 ... 6.75564715e-01\n",
      "  2.02784951e-04 7.57016975e-01]\n",
      " [1.94554576e-04 3.21521964e-01 2.62630472e-02 ... 6.78478036e-01\n",
      "  1.94537255e-04 7.57024384e-01]\n",
      " [1.86099353e-04 3.17958187e-01 2.62645389e-02 ... 6.82041813e-01\n",
      "  1.86082165e-04 7.57031614e-01]]\n"
     ]
    }
   ],
   "source": [
    "filename = \"COPIA_PANDAS\\hdf_lomosP2_test_filtrado_def_good.hdf\"\n",
    "with pd.HDFStore(filename,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e1  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e1 = pre_p_e1.loc[pre_p_e1['Pollo'] != 0]\n",
    "    pre_p_e1 =pre_p_e1.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_test=np.zeros((pre_p_e1.shape[0],220,8))\n",
    "    y_test=np.zeros((pre_p_e1.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e1.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "           target = 1\n",
    "        else:\n",
    "           target = 0\n",
    "        target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_test[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_test[x]=target\n",
    "        y_test_to_categorical = to_categorical(y_test)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_test_filtrado = X_test\n",
    "#y_train_filtrado = y_train\n",
    "y_test_filtrado = y_test_to_categorical\n",
    "\n",
    "print(X_test_filtrado.shape)\n",
    "print(y_test_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "# concatenamos train y test\n",
    "#X_total=np.concatenate((X_train_filtrado,X_test_filtrado),axis=0)\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#data_2d_test = X_total.reshape(-1, X_total.shape[-1])\n",
    "data_2d_test = X_test_filtrado.reshape(-1, X_test_filtrado.shape[-1])\n",
    "normalized_data_2d_test = scaler.transform(data_2d_test)\n",
    "\n",
    "\n",
    "X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "# la alternativa es normalizar con el total\n",
    "# X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "\n",
    "y_test_def=y_test_filtrado # los valores ya estaban normalizados\n",
    "print(X_test_def[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer los conjuntos de entrenamiento validacion y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el dataset en entrenamiento y temporal (test+validación)\n",
    "# X_temp, X_test_def, y_temp, y_test_def = train_test_split(X_train_Normalizado, y_train_Normalizado, test_size=0.2, stratify=y_train_Normalizado, random_state=42)\n",
    "\n",
    "# Divide el dataset temporal en validación y test\n",
    "X_train_def, X_val_def, y_train_def, y_val_def = train_test_split(X_train_Normalizado, y_train_Normalizado, test_size=0.25, stratify=y_train_Normalizado, random_state=42)\n",
    "\n",
    "# Ahora, X_train, X_val y X_test contienen los datos de entrada para los conjuntos de entrenamiento, validación y prueba, respectivamente.\n",
    "# y_train, y_val y y_test contienen las clases correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 220, 8)\n",
      "(792, 220, 8)\n",
      "(89, 220, 8)\n",
      "(2376, 5)\n",
      "(792, 5)\n",
      "(89, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_def.shape)\n",
    "print(X_val_def.shape)\n",
    "print(X_test_def.shape)\n",
    "print(y_train_def.shape)\n",
    "print(y_val_def.shape)\n",
    "print(y_test_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "\n",
    "#%tensorboard --logdir logs\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\n",
    "    \"\"\"\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    \n",
    "    # Use plt.savefig to save the plot to a PNG in memory.\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Use tf.image.decode_png to convert the PNG buffer\n",
    "    # to a TF image. Make sure you use 4 channels.\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Use tf.expand_dims to add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    \n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    threshold = 0.5\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"red\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_aprendizaje=0.01\n",
    "dimension_LSTM=50\n",
    "dimension_dense=50\n",
    "algoritmo='adam'\n",
    "supermax=8*4\n",
    "lossfunction='categorical_crossentropy'\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(dimension_LSTM, return_sequences=True),input_shape=(220, 8)))\n",
    "\n",
    "    #model.add(SqueezeNet_11(input_shape=(220,50, 1), nb_classes=2, dropout_rate=0.5, compression=1.0))\n",
    "    model.add(SqueezeNet_11(input_shape=(220,100, 1), nb_classes=5, dropout_rate=0.5, compression=1.0))\n",
    "    model.compile(loss=lossfunction, optimizer=algoritmo, metrics=['accuracy'])\n",
    "    model.optimizer.lr=(factor_aprendizaje)\n",
    "    return model\n",
    "\n",
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experimento=\"LOMOS_P2_GRU_squezenet_5_clasesfiltrado_{}_dense_onehot_{}_loss_{}_lr_{}_algoritmo_{}\".format(dimension_LSTM,dimension_dense,lossfunction,factor_aprendizaje,algoritmo)\n",
    "logdir=\"./logs/defs/{}_{}\".format(experimento,datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[\"Buenos\",\"Malos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs):\n",
    "    \n",
    "    # Use the model to predict the values from the test_images.\n",
    "    y_pred = model.predict(X_test_def)\n",
    "    #y_pred1=y_pred[:,-1]\n",
    "    y_pred2=y_pred.argmax(axis=1)\n",
    "    #y_pred2=np.where(y_pred>0,1,0)\n",
    "    #y_pred2=y_pred2[:,-1]\n",
    "    #classes = [0, 1, 2, 3, 4] \n",
    "    classes = [0, 1]\n",
    "    y_test_def2=np.argmax(y_test_def,axis=1)  \n",
    "    #y_test_def2=np.where(y_test_def>0,1,0)\n",
    "    cm=confusion_matrix(y_test_def2, y_pred2,labels=classes)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    figura = plot_confusion_matrix(cm, class_names=class_names)\n",
    "    cm_image = plot_to_image(figura)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 5)\n",
      "(792, 5)\n"
     ]
    }
   ],
   "source": [
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "print(y_train_Normalizado.shape)\n",
    "print(y_val_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un callback para guardar los mejores pesos\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "3/3 [==============================] - 1s 88ms/step- loss: 2.4088 - accuracy: 0.\n",
      "159/159 [==============================] - 18s 71ms/step - loss: 2.4088 - accuracy: 0.4400 - val_loss: 1.3654 - val_accuracy: 0.4482\n",
      "Epoch 2/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3617 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3617 - accuracy: 0.4479 - val_loss: 1.3562 - val_accuracy: 0.4482\n",
      "Epoch 3/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3601 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3598 - accuracy: 0.4479 - val_loss: 1.3609 - val_accuracy: 0.4482\n",
      "Epoch 4/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3625 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3625 - accuracy: 0.4479 - val_loss: 1.3628 - val_accuracy: 0.4482\n",
      "Epoch 5/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3599 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3599 - accuracy: 0.4479 - val_loss: 1.3573 - val_accuracy: 0.4482\n",
      "Epoch 6/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3597 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3597 - accuracy: 0.4479 - val_loss: 1.3549 - val_accuracy: 0.4482\n",
      "Epoch 7/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3580 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3580 - accuracy: 0.4479 - val_loss: 1.3576 - val_accuracy: 0.4482\n",
      "Epoch 8/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3607 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3607 - accuracy: 0.4479 - val_loss: 1.3577 - val_accuracy: 0.4482\n",
      "Epoch 9/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3598 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3598 - accuracy: 0.4479 - val_loss: 1.3557 - val_accuracy: 0.4482\n",
      "Epoch 10/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3571 - accuracy: 0.44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\PIL\\ImageFile.py:536\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    537\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m early_stop\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_Normalizado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_Normalizado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcm_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_def\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Final evaluation of the model \u001b[39;00m\n\u001b[0;32m      4\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_def, y_test_def, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\keras\\engine\\training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m     }\n\u001b[0;32m   1622\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1624\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\keras\\callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    446\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 448\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36mlog_confusion_matrix\u001b[1;34m(epoch, logs)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m figura \u001b[38;5;241m=\u001b[39m plot_confusion_matrix(cm, class_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[1;32m---> 16\u001b[0m cm_image \u001b[38;5;241m=\u001b[39m \u001b[43mplot_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigura\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Log the confusion matrix as an image summary.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_writer_cm\u001b[38;5;241m.\u001b[39mas_default():\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mplot_to_image\u001b[1;34m(figure)\u001b[0m\n\u001b[0;32m      7\u001b[0m buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Use plt.savefig to save the plot to a PNG in memory.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Closing the figure prevents it from being displayed directly inside\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# the notebook.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(figure)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\pyplot.py:1134\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\figure.py:3390\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3388\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   3389\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[1;32m-> 3390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\backend_bases.py:2193\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2193\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2194\u001b[0m             filename,\n\u001b[0;32m   2195\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2196\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2197\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2198\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2042\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:497\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:446\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 446\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\matplotlib\\image.py:1656\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1654\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1655\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1656\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\PIL\\Image.py:2459\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2456\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2459\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\PIL\\PngImagePlugin.py:1412\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1409\u001b[0m         im, fp, chunk, rawmode, default_image, append_images\n\u001b[0;32m   1410\u001b[0m     )\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1412\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\PIL\\ImageFile.py:540\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    538\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 540\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    542\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\PIL\\ImageFile.py:559\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    560\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "model.fit(X_train_Normalizado, y_train_Normalizado, epochs=4000, batch_size=20, callbacks=[tensorboard_callback,cm_callback,early_stop], validation_data=(X_val_def, y_val_def))\n",
    "# Final evaluation of the model \n",
    "scores = model.evaluate(X_test_def, y_test_def, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_def)\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "#y_pred2=np.where(y_pred>0,1,0)\n",
    "#y_pred2=y_pred2[:,-1]\n",
    "y_test_def2=np.argmax(y_test_def,axis=1)\n",
    "#y_test_def2=np.where(y_test_def>0,1,0)\n",
    "print(y_pred.shape)\n",
    "print(y_pred2.shape)\n",
    "print(y_test_def2.shape)\n",
    "#print(y_test_def[25])\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "classes = [0, 1, 2, 3, 4]\n",
    "classes = [0, 1]\n",
    "cm=confusion_matrix(y_test_def2, y_pred2,labels=classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test_def2, y_pred2, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('modelos/modelote1203_200')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelos/modelo_perfecto_{}_{}'.format(experimento,datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_def)\n",
    "#y_pred2 = scaler_out.inverse_transform(y_pred) #valor denormalizado\n",
    "\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "n = len(y_pred2)\n",
    "reshaped = y_pred2[:n//4*4].reshape(-1, 4)\n",
    "mean_values = reshaped.mean(axis=1)\n",
    "\n",
    "mean_values = np.round(mean_values)\n",
    "mean_values = np.clip(mean_values, 0, 4)\n",
    "mean_values = mean_values.astype(int)\n",
    "print(mean_values)\n",
    "\n",
    "mode_values = stats.mode(reshaped, axis=1)[0]\n",
    "print(mode_values)\n",
    "\n",
    "# Convierte los arrays a DataFrames\n",
    "mean_df = pd.DataFrame(mean_values, columns=['mean'])\n",
    "mode_df = pd.DataFrame(mode_values, columns=['mode'])\n",
    "\n",
    "# Guarda los DataFrames en archivos Excel\n",
    "mean_df.to_excel(\"clasificacion_P1P2_mean_best7.xlsx\", index=False)\n",
    "mode_df.to_excel(\"clasificacion_P1_mode_best7.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename5 = \"lomosP1_20240430_clasificado_experto.hdf\"\n",
    "with pd.HDFStore(filename5,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e2  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e2 = pre_p_e2.loc[pre_p_e2['Pollo'] != 0]\n",
    "    pre_p_e2 =pre_p_e2.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_test2=np.zeros((pre_p_e2.shape[0],220,8))\n",
    "    y_test2=np.zeros((pre_p_e2.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e2.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "           target = 1\n",
    "        else:\n",
    "           target = 0\n",
    "        #target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_test2[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_test2[x]=target\n",
    "        y_test2_to_categorical = to_categorical(y_test2)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_test2_filtrado = X_test2\n",
    "#y_train_filtrado = y_train\n",
    "y_test2_filtrado = y_test2_to_categorical\n",
    "\n",
    "print(X_test2_filtrado.shape)\n",
    "print(y_test2_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "# concatenamos train y test\n",
    "#X_total=np.concatenate((X_train_filtrado,X_test_filtrado),axis=0)\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#data_2d_test = X_total.reshape(-1, X_total.shape[-1])\n",
    "data_2d_test = X_test2_filtrado.reshape(-1, X_test2_filtrado.shape[-1])\n",
    "normalized_data_2d_test = scaler.transform(data_2d_test)\n",
    "\n",
    "\n",
    "X_test2_def=normalized_data_2d_test.reshape(X_test2_filtrado.shape) \n",
    "# la alternativa es normalizar con el total\n",
    "# X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "\n",
    "y_test2_def=y_test2_filtrado # los valores ya estaban normalizados\n",
    "\n",
    "print(y_test2_def.shape)\n",
    "\n",
    "print(y_test2_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Crear un nuevo modelo con la misma arquitectura\n",
    "best_val_model = create_model()  # Reemplaza esto con la función que usaste para crear el modelo original\n",
    "\n",
    "# Cargar los mejores pesos\n",
    "best_val_model.load_weights('best_weights.h5')\n",
    "\n",
    "y_pred = best_val_model.predict(X_test2_def)\n",
    "#y_pred2 = scaler_out.inverse_transform(y_pred) #valor denormalizado\n",
    "\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "n = len(y_pred2)\n",
    "print(n)\n",
    "reshaped = y_pred2[:n//4*4].reshape(-1, 4)\n",
    "mean_values = reshaped.mean(axis=1)\n",
    "\n",
    "mean_values = np.round(mean_values)\n",
    "mean_values = np.clip(mean_values, 0, 4)\n",
    "mean_values = mean_values.astype(int)\n",
    "print(mean_values.shape)\n",
    "\n",
    "mode_values = stats.mode(reshaped, axis=1)[0]\n",
    "print(mode_values.shape)\n",
    "\n",
    "n = len(y_test2_def)\n",
    "y_test2_def2=np.argmax(y_test2_def,axis=1)\n",
    "print(y_test_def2.shape)\n",
    "print(n)\n",
    "reshaped2 = y_test2_def2[:n//4*4].reshape(-1, 4)\n",
    "target_mean_values = reshaped2.mean(axis=1)\n",
    "\n",
    "target_mean_values = np.round(target_mean_values)\n",
    "target_mean_values = np.clip(target_mean_values, 0, 4)\n",
    "target_mean_values = target_mean_values.astype(int)\n",
    "print(target_mean_values.shape)\n",
    "\n",
    "target_mode_values = stats.mode(reshaped2, axis=1)[0]\n",
    "print(target_mode_values.shape)\n",
    "print(reshaped)\n",
    "print(mode_values)\n",
    "print(target_mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "classes = [0, 1, 2, 3, 4]\n",
    "classes = [0, 1]\n",
    "cm=confusion_matrix(y_test2_def2, y_pred2,labels=classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.diagonal()/cm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(target_mean_values, mean_values, target_names=target_names, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
