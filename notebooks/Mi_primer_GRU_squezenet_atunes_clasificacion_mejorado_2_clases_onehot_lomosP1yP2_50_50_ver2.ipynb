{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\nuevas_investigaciones_alimentos_2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "import io\n",
    "import itertools\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Obtener la ruta del directorio actual\n",
    "os.chdir('..')\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Construir la ruta relativa al directorio que quieres agregar\n",
    "relative_dir = os.path.join(current_dir, 'mis_pkgs/')\n",
    "\n",
    "# Agregar la ruta relativa al sys.path\n",
    "sys.path.insert(0, relative_dir)\n",
    "\n",
    "from MIOPATIA_db import DB_management as db \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a quedarme con los 50 atunes P1 para obtener conjunto de training y validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add, Activation, Concatenate, Conv2D, Dropout \n",
    "from tensorflow.keras.layers import Flatten, Input, GlobalAveragePooling2D, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "__version__ = '0.0.1'\n",
    "\n",
    "\n",
    "def SqueezeNet(input_shape, nb_classes, use_bypass=False, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.0\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        use_bypass   : if true, bypass connections will be created at fire module 3, 5, 7, and 9 (default: False)\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(96*compression), (7,7), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool4')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire5', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7', use_bypass=use_bypass)\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool8')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(64*compression), name='fire9', use_bypass=use_bypass)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        \n",
    "    x = output(x, nb_classes)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "def SqueezeNet_11(input_shape, nb_classes, dropout_rate=None, compression=1.0):\n",
    "    \"\"\"\n",
    "    Creating a SqueezeNet of version 1.1\n",
    "    \n",
    "    2.4x less computation over SqueezeNet 1.0 implemented above.\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images e.g. (224,224,3)\n",
    "        nb_classes   : number of classes\n",
    "        dropout_rate : defines the dropout rate that is accomplished after last fire module (default: None)\n",
    "        compression  : reduce the number of feature-maps\n",
    "        \n",
    "    Returns:\n",
    "        Model        : Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(int(64*compression), (3,3), activation='relu', strides=(2,2), padding='same', name='conv1')(input_img)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool1')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(16*compression), name='fire2')\n",
    "    x = create_fire_module(x, int(16*compression), name='fire3')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool3')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(32*compression), name='fire4')\n",
    "    x = create_fire_module(x, int(32*compression), name='fire5')\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2), name='maxpool5')(x)\n",
    "    \n",
    "    x = create_fire_module(x, int(48*compression), name='fire6')\n",
    "    x = create_fire_module(x, int(48*compression), name='fire7')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire8')\n",
    "    x = create_fire_module(x, int(64*compression), name='fire9')\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Creating last conv10\n",
    "    x = output(x, nb_classes)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n",
    "def output(x, nb_classes):\n",
    "    x = Conv2D(nb_classes, (1,1), strides=(1,1), padding='valid', name='conv10')(x)\n",
    "    x = GlobalAveragePooling2D(name='avgpool10')(x)\n",
    "    x = Activation(\"softmax\", name='softmax')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_fire_module(x, nb_squeeze_filter, name, use_bypass=False):\n",
    "    \"\"\"\n",
    "    Creates a fire module\n",
    "    \n",
    "    Arguments:\n",
    "        x                 : input\n",
    "        nb_squeeze_filter : number of filters of squeeze. The filtersize of expand is 4 times of squeeze\n",
    "        use_bypass        : if True then a bypass will be added\n",
    "        name              : name of module e.g. fire123\n",
    "    \n",
    "    Returns:\n",
    "        x                 : returns a fire module\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_expand_filter = 4 * nb_squeeze_filter\n",
    "    squeeze    = Conv2D(nb_squeeze_filter,(1,1), activation='relu', padding='same', name='%s_squeeze'%name)(x)\n",
    "    expand_1x1 = Conv2D(nb_expand_filter, (1,1), activation='relu', padding='same', name='%s_expand_1x1'%name)(squeeze)\n",
    "    expand_3x3 = Conv2D(nb_expand_filter, (3,3), activation='relu', padding='same', name='%s_expand_3x3'%name)(squeeze)\n",
    "    \n",
    "    axis = get_axis()\n",
    "    x_ret = Concatenate(axis=axis, name='%s_concatenate'%name)([expand_1x1, expand_3x3])\n",
    "    \n",
    "    if use_bypass:\n",
    "        x_ret = Add(name='%s_concatenate_bypass'%name)([x_ret, x])\n",
    "        \n",
    "    return x_ret\n",
    "\n",
    "\n",
    "def get_axis():\n",
    "    axis = -1 if K.image_data_format() == 'channels_last' else 1\n",
    "    return axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 5)\n"
     ]
    }
   ],
   "source": [
    "filename = \"COPIA_PANDAS\\hdf_lomosP2_trainval_filtrado_def_good_ampliado_the_best7.hdf\"\n",
    "with pd.HDFStore(filename,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e1  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e1 = pre_p_e1.loc[pre_p_e1['Pollo'] != 0]\n",
    "    # p_e =pre_p_e1.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_train=np.zeros((pre_p_e1.shape[0],220,8))\n",
    "    y_train=np.zeros((pre_p_e1.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e1.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_train[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_train[x]=target\n",
    "        y_train_to_categorical = to_categorical(y_train)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_train_filtrado = X_train\n",
    "#y_train_filtrado = y_train\n",
    "y_train_filtrado = y_train_to_categorical\n",
    "\n",
    "# print(X_train_filtrado.shape)\n",
    "# print(y_train_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_2d = X_train_filtrado.reshape(-1, X_train_filtrado.shape[-1])\n",
    "normalized_data_2d = scaler.fit_transform(data_2d)\n",
    "X_train_Normalizado=normalized_data_2d.reshape(X_train_filtrado.shape)\n",
    "y_train_Normalizado=y_train_filtrado # los valores ya estaban normalizados\n",
    "print(y_train_Normalizado.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 220, 8)\n",
      "(89, 5)\n",
      "[[1.45375657e-01 4.74795267e-01 2.80281244e-02 ... 5.25204733e-01\n",
      "  1.45375842e-01 6.87729402e-01]\n",
      " [1.04024516e-01 4.57433188e-01 2.91808316e-02 ... 5.42566812e-01\n",
      "  1.04024006e-01 6.97578524e-01]\n",
      " [8.71835342e-02 4.33210717e-01 3.04208802e-02 ... 5.66789283e-01\n",
      "  8.71821912e-02 6.95625305e-01]\n",
      " ...\n",
      " [2.02802467e-04 3.24435285e-01 2.62623498e-02 ... 6.75564715e-01\n",
      "  2.02784951e-04 7.57016975e-01]\n",
      " [1.94554576e-04 3.21521964e-01 2.62630472e-02 ... 6.78478036e-01\n",
      "  1.94537255e-04 7.57024384e-01]\n",
      " [1.86099353e-04 3.17958187e-01 2.62645389e-02 ... 6.82041813e-01\n",
      "  1.86082165e-04 7.57031614e-01]]\n"
     ]
    }
   ],
   "source": [
    "filename = \"COPIA_PANDAS\\hdf_lomosP2_test_filtrado_def_good.hdf\"\n",
    "with pd.HDFStore(filename,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e1  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e1 = pre_p_e1.loc[pre_p_e1['Pollo'] != 0]\n",
    "    pre_p_e1 =pre_p_e1.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_test=np.zeros((pre_p_e1.shape[0],220,8))\n",
    "    y_test=np.zeros((pre_p_e1.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e1.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "           target = 1\n",
    "        else:\n",
    "           target = 0\n",
    "        target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_test[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_test[x]=target\n",
    "        y_test_to_categorical = to_categorical(y_test)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_test_filtrado = X_test\n",
    "#y_train_filtrado = y_train\n",
    "y_test_filtrado = y_test_to_categorical\n",
    "\n",
    "print(X_test_filtrado.shape)\n",
    "print(y_test_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "# concatenamos train y test\n",
    "#X_total=np.concatenate((X_train_filtrado,X_test_filtrado),axis=0)\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#data_2d_test = X_total.reshape(-1, X_total.shape[-1])\n",
    "data_2d_test = X_test_filtrado.reshape(-1, X_test_filtrado.shape[-1])\n",
    "normalized_data_2d_test = scaler.transform(data_2d_test)\n",
    "\n",
    "\n",
    "X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "# la alternativa es normalizar con el total\n",
    "# X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "\n",
    "y_test_def=y_test_filtrado # los valores ya estaban normalizados\n",
    "print(X_test_def[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer los conjuntos de entrenamiento validacion y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el dataset en entrenamiento y temporal (test+validación)\n",
    "# X_temp, X_test_def, y_temp, y_test_def = train_test_split(X_train_Normalizado, y_train_Normalizado, test_size=0.2, stratify=y_train_Normalizado, random_state=42)\n",
    "\n",
    "# Divide el dataset temporal en validación y test\n",
    "X_train_def, X_val_def, y_train_def, y_val_def = train_test_split(X_train_Normalizado, y_train_Normalizado, test_size=0.25, stratify=y_train_Normalizado, random_state=42)\n",
    "\n",
    "# Ahora, X_train, X_val y X_test contienen los datos de entrada para los conjuntos de entrenamiento, validación y prueba, respectivamente.\n",
    "# y_train, y_val y y_test contienen las clases correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 220, 8)\n",
      "(792, 220, 8)\n",
      "(89, 220, 8)\n",
      "(2376, 5)\n",
      "(792, 5)\n",
      "(89, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_def.shape)\n",
    "print(X_val_def.shape)\n",
    "print(X_test_def.shape)\n",
    "print(y_train_def.shape)\n",
    "print(y_val_def.shape)\n",
    "print(y_test_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "\n",
    "#%tensorboard --logdir logs\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\n",
    "    \"\"\"\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    \n",
    "    # Use plt.savefig to save the plot to a PNG in memory.\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Use tf.image.decode_png to convert the PNG buffer\n",
    "    # to a TF image. Make sure you use 4 channels.\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Use tf.expand_dims to add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    \n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    threshold = 0.5\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"red\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_aprendizaje=0.01\n",
    "dimension_LSTM=50\n",
    "dimension_dense=50\n",
    "algoritmo='adam'\n",
    "supermax=8*4\n",
    "lossfunction='categorical_crossentropy'\n",
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(dimension_LSTM, return_sequences=True),input_shape=(220, 8)))\n",
    "\n",
    "    #model.add(SqueezeNet_11(input_shape=(220,50, 1), nb_classes=2, dropout_rate=0.5, compression=1.0))\n",
    "    model.add(SqueezeNet_11(input_shape=(220,100, 1), nb_classes=5, dropout_rate=0.5, compression=1.0))\n",
    "    model.compile(loss=lossfunction, optimizer=algoritmo, metrics=['accuracy'])\n",
    "    model.optimizer.lr=(factor_aprendizaje)\n",
    "    return model\n",
    "\n",
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experimento=\"LOMOS_P2_GRU_squezenet_5_clasesfiltrado_{}_dense_onehot_{}_loss_{}_lr_{}_algoritmo_{}\".format(dimension_LSTM,dimension_dense,lossfunction,factor_aprendizaje,algoritmo)\n",
    "logdir=\"./logs/defs/{}_{}\".format(experimento,datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[\"Buenos\",\"Malos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs):\n",
    "    \n",
    "    # Use the model to predict the values from the test_images.\n",
    "    y_pred = model.predict(X_test_def)\n",
    "    #y_pred1=y_pred[:,-1]\n",
    "    y_pred2=y_pred.argmax(axis=1)\n",
    "    #y_pred2=np.where(y_pred>0,1,0)\n",
    "    #y_pred2=y_pred2[:,-1]\n",
    "    #classes = [0, 1, 2, 3, 4] \n",
    "    classes = [0, 1]\n",
    "    y_test_def2=np.argmax(y_test_def,axis=1)  \n",
    "    #y_test_def2=np.where(y_test_def>0,1,0)\n",
    "    cm=confusion_matrix(y_test_def2, y_pred2,labels=classes)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    figura = plot_confusion_matrix(cm, class_names=class_names)\n",
    "    cm_image = plot_to_image(figura)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 5)\n",
      "(792, 5)\n"
     ]
    }
   ],
   "source": [
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "print(y_train_Normalizado.shape)\n",
    "print(y_val_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un callback para guardar los mejores pesos\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "3/3 [==============================] - 1s 80ms/step- loss: 1.8842 - accuracy: 0.\n",
      "159/159 [==============================] - 17s 67ms/step - loss: 1.8842 - accuracy: 0.4403 - val_loss: 1.3578 - val_accuracy: 0.4482\n",
      "Epoch 2/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3635 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3638 - accuracy: 0.4479 - val_loss: 1.3641 - val_accuracy: 0.4482\n",
      "Epoch 3/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3629 - accuracy: 0.43\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3629 - accuracy: 0.4384 - val_loss: 1.3592 - val_accuracy: 0.4482\n",
      "Epoch 4/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3632 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 58ms/step - loss: 1.3619 - accuracy: 0.4479 - val_loss: 1.3557 - val_accuracy: 0.4482\n",
      "Epoch 5/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3600 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3600 - accuracy: 0.4479 - val_loss: 1.3623 - val_accuracy: 0.4482\n",
      "Epoch 6/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3628 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3628 - accuracy: 0.4479 - val_loss: 1.3555 - val_accuracy: 0.4482\n",
      "Epoch 7/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3597 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3597 - accuracy: 0.4479 - val_loss: 1.3570 - val_accuracy: 0.4482\n",
      "Epoch 8/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3621 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3616 - accuracy: 0.4479 - val_loss: 1.3552 - val_accuracy: 0.4482\n",
      "Epoch 9/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3592 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3592 - accuracy: 0.4479 - val_loss: 1.3554 - val_accuracy: 0.4482\n",
      "Epoch 10/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3577 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3577 - accuracy: 0.4479 - val_loss: 1.3566 - val_accuracy: 0.4482\n",
      "Epoch 11/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3610 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3610 - accuracy: 0.4479 - val_loss: 1.3559 - val_accuracy: 0.4482\n",
      "Epoch 12/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3571 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3574 - accuracy: 0.4479 - val_loss: 1.3552 - val_accuracy: 0.4482\n",
      "Epoch 13/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3564 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3564 - accuracy: 0.4479 - val_loss: 1.3566 - val_accuracy: 0.4482\n",
      "Epoch 14/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3568 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 56ms/step - loss: 1.3575 - accuracy: 0.4479 - val_loss: 1.3556 - val_accuracy: 0.4482\n",
      "Epoch 15/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3571 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3571 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 16/4000\n",
      "3/3 [==============================] - 0s 20ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3556 - val_accuracy: 0.4482\n",
      "Epoch 17/4000\n",
      "3/3 [==============================] - 0s 21ms/step- loss: 1.3568 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3568 - accuracy: 0.4479 - val_loss: 1.3568 - val_accuracy: 0.4482\n",
      "Epoch 18/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3571 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3571 - accuracy: 0.4479 - val_loss: 1.3551 - val_accuracy: 0.4482\n",
      "Epoch 19/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3566 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3566 - accuracy: 0.4479 - val_loss: 1.3560 - val_accuracy: 0.4482\n",
      "Epoch 20/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3569 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3561 - accuracy: 0.4479 - val_loss: 1.3574 - val_accuracy: 0.4482\n",
      "Epoch 21/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3566 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3566 - accuracy: 0.4479 - val_loss: 1.3551 - val_accuracy: 0.4482\n",
      "Epoch 22/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 23/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3571 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3566 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 24/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 25/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3562 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3562 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 26/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 27/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3562 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 28/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 29/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3536 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3543 - accuracy: 0.4479 - val_loss: 1.3573 - val_accuracy: 0.4482\n",
      "Epoch 30/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3550 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 31/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3551 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 32/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3550 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 33/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 10s 63ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 34/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3568 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 50ms/step - loss: 1.3560 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 35/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3562 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 36/4000\n",
      "3/3 [==============================] - 0s 20ms/step- loss: 1.3561 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3561 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 37/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 38/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3554 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 39/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 40/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3561 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 41/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 54ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3559 - val_accuracy: 0.4482\n",
      "Epoch 42/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3560 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3560 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 43/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3552 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 56ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 44/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3549 - val_accuracy: 0.4482\n",
      "Epoch 45/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 46/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 47/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3552 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 48/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 51ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 49/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 50/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 51/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3548 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3548 - accuracy: 0.4479 - val_loss: 1.3551 - val_accuracy: 0.4482\n",
      "Epoch 52/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3554 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 53/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3536 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 58ms/step - loss: 1.3552 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 54/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 55/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3564 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 56/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3557 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 57/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3557 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 58/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3553 - val_accuracy: 0.4482\n",
      "Epoch 59/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 60/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3549 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3549 - accuracy: 0.4479 - val_loss: 1.3560 - val_accuracy: 0.4482\n",
      "Epoch 61/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3552 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3563 - accuracy: 0.4479 - val_loss: 1.3550 - val_accuracy: 0.4482\n",
      "Epoch 62/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3554 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 63/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3543 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 64/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3549 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 65/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3547 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3552 - accuracy: 0.4479 - val_loss: 1.3555 - val_accuracy: 0.4482\n",
      "Epoch 66/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 67/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3548 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 68/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3545 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 69/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3564 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 70/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 71/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3538 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3556 - val_accuracy: 0.4482\n",
      "Epoch 72/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3562 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 73/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3550 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3550 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 74/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 75/4000\n",
      "3/3 [==============================] - 0s 20ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3558 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 76/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3552 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3552 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 77/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3560 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 78/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 79/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 80/4000\n",
      "3/3 [==============================] - 0s 20ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3546 - val_accuracy: 0.4482\n",
      "Epoch 81/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3545 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 82/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 83/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3549 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3549 - accuracy: 0.4479 - val_loss: 1.3551 - val_accuracy: 0.4482\n",
      "Epoch 84/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3563 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3549 - val_accuracy: 0.4482\n",
      "Epoch 85/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3555 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3552 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 86/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3557 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 87/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3561 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 88/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 54ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 89/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3557 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 56ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 90/4000\n",
      "3/3 [==============================] - 0s 19ms/step- loss: 1.3543 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 56ms/step - loss: 1.3550 - accuracy: 0.4479 - val_loss: 1.3553 - val_accuracy: 0.4482\n",
      "Epoch 91/4000\n",
      "3/3 [==============================] - 0s 36ms/step- loss: 1.3555 - accuracy: 0.\n",
      "159/159 [==============================] - 14s 87ms/step - loss: 1.3555 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 92/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3548 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 58ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 93/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 10s 65ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 94/4000\n",
      "3/3 [==============================] - 0s 22ms/step- loss: 1.3562 - accuracy: 0.44\n",
      "159/159 [==============================] - 13s 81ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 95/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3553 - accuracy: 0.44\n",
      "159/159 [==============================] - 12s 74ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 96/4000\n",
      "3/3 [==============================] - 0s 21ms/step- loss: 1.3552 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 59ms/step - loss: 1.3552 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 97/4000\n",
      "3/3 [==============================] - 0s 22ms/step- loss: 1.3554 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 58ms/step - loss: 1.3554 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 98/4000\n",
      "3/3 [==============================] - 0s 15ms/step- loss: 1.3559 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 54ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 99/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3565 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3564 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 100/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3549 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3559 - accuracy: 0.4479 - val_loss: 1.3544 - val_accuracy: 0.4482\n",
      "Epoch 101/4000\n",
      "3/3 [==============================] - 0s 17ms/step- loss: 1.3556 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 53ms/step - loss: 1.3556 - accuracy: 0.4479 - val_loss: 1.3548 - val_accuracy: 0.4482\n",
      "Epoch 102/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 53ms/step - loss: 1.3551 - accuracy: 0.4479 - val_loss: 1.3545 - val_accuracy: 0.4482\n",
      "Epoch 103/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3550 - accuracy: 0.44\n",
      "159/159 [==============================] - 9s 56ms/step - loss: 1.3550 - accuracy: 0.4479 - val_loss: 1.3547 - val_accuracy: 0.4482\n",
      "Epoch 104/4000\n",
      "3/3 [==============================] - 0s 16ms/step- loss: 1.3558 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3553 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 105/4000\n",
      "3/3 [==============================] - 0s 18ms/step- loss: 1.3557 - accuracy: 0.44\n",
      "159/159 [==============================] - 8s 52ms/step - loss: 1.3557 - accuracy: 0.4479 - val_loss: 1.3543 - val_accuracy: 0.4482\n",
      "Epoch 106/4000\n",
      "141/159 [=========================>....] - ETA: 0s - loss: 1.3617 - accuracy: 0.4475"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m early_stop\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_Normalizado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_Normalizado\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcm_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_def\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Final evaluation of the model \u001b[39;00m\n\u001b[0;32m      4\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_def, y_test_def, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\rgadea\\AppData\\Local\\miniconda3\\envs\\tensorflow_gpu_2024\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "model.fit(X_train_Normalizado, y_train_Normalizado, epochs=4000, batch_size=20, callbacks=[tensorboard_callback,cm_callback,early_stop], validation_data=(X_val_def, y_val_def))\n",
    "# Final evaluation of the model \n",
    "scores = model.evaluate(X_test_def, y_test_def, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_def)\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "#y_pred2=np.where(y_pred>0,1,0)\n",
    "#y_pred2=y_pred2[:,-1]\n",
    "y_test_def2=np.argmax(y_test_def,axis=1)\n",
    "#y_test_def2=np.where(y_test_def>0,1,0)\n",
    "print(y_pred.shape)\n",
    "print(y_pred2.shape)\n",
    "print(y_test_def2.shape)\n",
    "#print(y_test_def[25])\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "classes = [0, 1, 2, 3, 4]\n",
    "classes = [0, 1]\n",
    "cm=confusion_matrix(y_test_def2, y_pred2,labels=classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test_def2, y_pred2, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('modelos/modelote1203_200')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelos/modelo_perfecto_{}_{}'.format(experimento,datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_def)\n",
    "#y_pred2 = scaler_out.inverse_transform(y_pred) #valor denormalizado\n",
    "\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "n = len(y_pred2)\n",
    "reshaped = y_pred2[:n//4*4].reshape(-1, 4)\n",
    "mean_values = reshaped.mean(axis=1)\n",
    "\n",
    "mean_values = np.round(mean_values)\n",
    "mean_values = np.clip(mean_values, 0, 4)\n",
    "mean_values = mean_values.astype(int)\n",
    "print(mean_values)\n",
    "\n",
    "mode_values = stats.mode(reshaped, axis=1)[0]\n",
    "print(mode_values)\n",
    "\n",
    "# Convierte los arrays a DataFrames\n",
    "mean_df = pd.DataFrame(mean_values, columns=['mean'])\n",
    "mode_df = pd.DataFrame(mode_values, columns=['mode'])\n",
    "\n",
    "# Guarda los DataFrames en archivos Excel\n",
    "mean_df.to_excel(\"clasificacion_P1P2_mean_best7.xlsx\", index=False)\n",
    "mode_df.to_excel(\"clasificacion_P1_mode_best7.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename5 = \"lomosP1_20240430_clasificado_experto.hdf\"\n",
    "with pd.HDFStore(filename5,complib=\"zlib\",complevel=4) as hdf_db:\n",
    "    pre_p_e2  = hdf_db.get('data/pollos_estado')\n",
    "    pre_p_e2 = pre_p_e2.loc[pre_p_e2['Pollo'] != 0]\n",
    "    pre_p_e2 =pre_p_e2.drop_duplicates(subset = ['Pollo', 'Medida'],  keep = 'last').reset_index(drop = True)\n",
    "    t    = hdf_db.get('data/tabla')\n",
    "    X_test2=np.zeros((pre_p_e2.shape[0],220,8))\n",
    "    y_test2=np.zeros((pre_p_e2.shape[0],1))\n",
    "    x=0\n",
    "    for index, row in pre_p_e2.iterrows():   # El primer registro no se toma en cuenta porque es basura\n",
    "        Primero = int(row['Primero'])\n",
    "        Ultimo  = int(row['Ultimo'])\n",
    "        estado  = int(row['Estado'])\n",
    "        #print(Primero)\n",
    "        #print(Ultimo)\n",
    "        #print(estado)\n",
    "        if estado == 0 or estado== 1:\n",
    "           target = 1\n",
    "        else:\n",
    "           target = 0\n",
    "        #target=estado\n",
    "        pepito=np.array(t.iloc[Primero:Ultimo+1])\n",
    "        # #print(pepito.shape)\n",
    "        X_test2[x]=pepito[:,3:11]\n",
    "        #print(X_train[x][0:4,:])       \n",
    "        y_test2[x]=target\n",
    "        y_test2_to_categorical = to_categorical(y_test2)\n",
    "        x=x+1\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train_to_categorical.shape)\n",
    "# #print(X_train[0:4,:,:])\n",
    "# #print(X_train[1][0:4][:])\n",
    "# print(y_train[1:20])\n",
    "# print(y_train_to_categorical[1:20])\n",
    "# # #Aqui filtrariamos si hay filas que no nos interesan. En este caso dejo pasar todos los casos\n",
    "# print(p_e)\n",
    "# # X_train_filtrado = X_train[2:][:,:]\n",
    "# # y_train_filtrado = y_train[2:]\n",
    "X_test2_filtrado = X_test2\n",
    "#y_train_filtrado = y_train\n",
    "y_test2_filtrado = y_test2_to_categorical\n",
    "\n",
    "print(X_test2_filtrado.shape)\n",
    "print(y_test2_filtrado.shape)\n",
    "# print(X_train_filtrado[0][:,:])\n",
    "# # # Vamos a normalizar o escalar los datos\n",
    "# concatenamos train y test\n",
    "#X_total=np.concatenate((X_train_filtrado,X_test_filtrado),axis=0)\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#data_2d_test = X_total.reshape(-1, X_total.shape[-1])\n",
    "data_2d_test = X_test2_filtrado.reshape(-1, X_test2_filtrado.shape[-1])\n",
    "normalized_data_2d_test = scaler.transform(data_2d_test)\n",
    "\n",
    "\n",
    "X_test2_def=normalized_data_2d_test.reshape(X_test2_filtrado.shape) \n",
    "# la alternativa es normalizar con el total\n",
    "# X_test_def=normalized_data_2d_test.reshape(X_test_filtrado.shape) \n",
    "\n",
    "y_test2_def=y_test2_filtrado # los valores ya estaban normalizados\n",
    "\n",
    "print(y_test2_def.shape)\n",
    "\n",
    "print(y_test2_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Crear un nuevo modelo con la misma arquitectura\n",
    "best_val_model = create_model()  # Reemplaza esto con la función que usaste para crear el modelo original\n",
    "\n",
    "# Cargar los mejores pesos\n",
    "best_val_model.load_weights('best_weights.h5')\n",
    "\n",
    "y_pred = best_val_model.predict(X_test2_def)\n",
    "#y_pred2 = scaler_out.inverse_transform(y_pred) #valor denormalizado\n",
    "\n",
    "#y_pred1=y_pred[:,-1]\n",
    "y_pred2=np.argmax(y_pred,axis=1)\n",
    "n = len(y_pred2)\n",
    "print(n)\n",
    "reshaped = y_pred2[:n//4*4].reshape(-1, 4)\n",
    "mean_values = reshaped.mean(axis=1)\n",
    "\n",
    "mean_values = np.round(mean_values)\n",
    "mean_values = np.clip(mean_values, 0, 4)\n",
    "mean_values = mean_values.astype(int)\n",
    "print(mean_values.shape)\n",
    "\n",
    "mode_values = stats.mode(reshaped, axis=1)[0]\n",
    "print(mode_values.shape)\n",
    "\n",
    "n = len(y_test2_def)\n",
    "y_test2_def2=np.argmax(y_test2_def,axis=1)\n",
    "print(y_test_def2.shape)\n",
    "print(n)\n",
    "reshaped2 = y_test2_def2[:n//4*4].reshape(-1, 4)\n",
    "target_mean_values = reshaped2.mean(axis=1)\n",
    "\n",
    "target_mean_values = np.round(target_mean_values)\n",
    "target_mean_values = np.clip(target_mean_values, 0, 4)\n",
    "target_mean_values = target_mean_values.astype(int)\n",
    "print(target_mean_values.shape)\n",
    "\n",
    "target_mode_values = stats.mode(reshaped2, axis=1)[0]\n",
    "print(target_mode_values.shape)\n",
    "print(reshaped)\n",
    "print(mode_values)\n",
    "print(target_mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "classes = [0, 1, 2, 3, 4]\n",
    "classes = [0, 1]\n",
    "cm=confusion_matrix(y_test2_def2, y_pred2,labels=classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.diagonal()/cm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(target_mean_values, mean_values, target_names=target_names, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
