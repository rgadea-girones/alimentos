{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AUTOENCODER for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python36.zip', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/lib-dynload', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/extensions', '/home/rgadea3/.ipython']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en matlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66498, 640)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import hdf5storage\n",
    "datos_matlab = hdf5storage.loadmat('../datos_octubre_2018/conjunto_entrenamiento_octubre_2018_red_pitch5mm_rad161mm_total.mat')\n",
    "conjunto_datos= datos_matlab.get('photodefA')\n",
    "conjunto_datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6320, 3840)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dir_name='../datos_octubre_2018'\n",
    "base_filename='p_OF_5mm_161mm'\n",
    "filename_suffix='.h5'\n",
    "file=os.path.join(dir_name, base_filename+ \"{0:03d}\".format(0) + filename_suffix)\n",
    "conjunto_datos_waves=pd.read_hdf(file,'MC')\n",
    "datos_waves=conjunto_datos_waves.values\n",
    "datos_waves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12641, 3840)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    file=os.path.join(dir_name, base_filename+ \"{0:03d}\".format(i) + filename_suffix)\n",
    "    #print(file)\n",
    "    veamos=pd.read_hdf(file,'MC')\n",
    "    veamos_array=veamos.values\n",
    "    datos_waves=np.concatenate((datos_waves,veamos_array),axis=0)\n",
    "datos_waves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12641, 3840)\n"
     ]
    }
   ],
   "source": [
    "L1A=6;\n",
    "# hay tres L1 con 640 sensores (40*16)\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 16, 40\n",
    "\n",
    "X_trained=datos_waves;\n",
    "x_trained=X_trained;\n",
    "\n",
    "for i in range (X_trained.shape[0]):\n",
    "    idea1=X_trained[i,:].reshape(img_rows,(L1A*img_cols));\n",
    "    ideat=idea1.transpose();\n",
    "    idea2=ideat.reshape(1,(L1A*img_cols)*img_rows);\n",
    "    x_trained[i,:] =idea2;\n",
    "\n",
    "print(x_trained.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_dim_A=img_rows*img_cols\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "\n",
    "conjunto_datos=np.zeros((x_trained.shape[0]*L1A,input_output_dim_A))\n",
    "for i in range(x_trained.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_trained[i,k*input_output_dim_A:k*input_output_dim_A+input_output_dim_A]\n",
    "    conjunto_datos[(i)*L1A :(i+1)*L1A,:] = ideaA    \n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_regularizer = True\n",
    "my_regularizer = None\n",
    "my_epochs = 50\n",
    "features_path = 'simple_autoe_features.pickle'\n",
    "labels_path = 'simple_autoe_labels.pickle'\n",
    "\n",
    "if use_regularizer:\n",
    "    # add a sparsity constraint on the encoded representations\n",
    "    # note use of 10e-5 leads to blurred results\n",
    "    my_regularizer = regularizers.l2(0.001)\n",
    "    # and a larger number of epochs as the added regularization the model\n",
    "    # is less likely to overfit and can be trained longer\n",
    "    my_epochs = 100\n",
    "    features_path = 'sparse_autoe_features.pickle'\n",
    "    labels_path = 'sparse_autoe_labels.pickle'\n",
    "\n",
    "   \n",
    "    \n",
    "encoding_dim = 320  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "\n",
    "input_img = Input(shape=(img_rows*img_cols,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh', use_bias=False,bias_initializer='random_uniform')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(img_cols*img_rows, activation='tanh',use_bias=True,bias_initializer='random_uniform')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "\n",
    "\n",
    "#autoencoder=Sequential([\n",
    "#    Dense(encoding_dim, kernel_regularizer=regularizers.l2(0.001), use_bias=True,bias_initializer='random_uniform',input_shape=(640,)),\n",
    "#    Activation('sigmoid'),\n",
    "#    Dense(img_cols*img_rows, use_bias=True,bias_initializer='random_uniform'),\n",
    "#    Activation('linear'),\n",
    "#])\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75846\n",
      "conjunto_datos shape: (75846, 640)\n",
      "45507\n",
      "15169\n",
      "15170\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "numero_muestras=conjunto_datos.shape[0]\n",
    "print(numero_muestras)\n",
    "print('conjunto_datos shape:', conjunto_datos.shape)\n",
    "\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "X_train=conjunto_datos[:tamanyo_tr,:]\n",
    "X_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,:]\n",
    "X_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_train=conjunto_datos[:tamanyo_tr,1] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,1] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,1] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows,1)\n",
    "\n",
    "\n",
    "input_shape = (img_cols, img_rows,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45507, 40, 16, 1)\n",
      "45507 train samples\n",
      "15169 validation samples\n",
      "15170 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7467\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2689\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35804\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,X_train.shape[0])\n",
    "    plt.imshow(np.reshape(X_train[idea].transpose(), [16, 40]), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    plt.show()\n",
    "    print(idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar las matrices de datos para la red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45507, 640)\n",
      "(15170, 640)\n",
      "(15170, 640)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "x_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "prueba=x_train[0:15170,:]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(prueba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "# min_max_scaler = preprocessing.QuantileTransformer().fit(x_train)\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler().fit(x_train)\n",
    "# min_max_scaler = preprocessing.StandardScaler(with_mean=False).fit(x_train)\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(x_train)\n",
    "#min_max_scaler = preprocessing.RobustScaler().fit(x_train)\n",
    "supermax=100\n",
    "factor_aprendizaje=0.0001\n",
    "print(min_max_scaler)\n",
    "#x_train_scaled = min_max_scaler.transform(x_train)\n",
    "#x_test_scaled = min_max_scaler.transform(x_test)\n",
    "x_train_scaled=(2*x_train/supermax)-1\n",
    "x_test_scaled=(2*x_test/supermax)-1\n",
    "#min_max_scaler.scale_\n",
    "#x_train[29413]\n",
    "#x_train_scaled[29413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our metrics, for example energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as KK\n",
    "import keras.callbacks as KKcall\n",
    "import tensorflow as tf\n",
    "\n",
    "print (tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(KKcall.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.asarray(self.model.predict(X_val))\n",
    "\n",
    "        y_val = np.argmax((y_val+1)*supermax/2, axis=1)\n",
    "        y_predict = np.argmax((y_predict+1)*supermax/2, axis=1)\n",
    "\n",
    "        self._data.append({\n",
    "            'val_energy': np.mean(y_predict-y_val),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "\n",
    "metrics = Metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_error(y_true, y_pred):\n",
    "    veamos_energia=(KK.sum(y_pred, axis=1)-KK.sum(y_true,axis=1))\n",
    "    return KK.mean(veamos_energia,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='RMSprop', loss='mse', metrics=[energy_error])\n",
    "\n",
    "autoencoder.optimizer.lr=(factor_aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45507 samples, validate on 15170 samples\n",
      "Epoch 1/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 0.0629 - energy_error: 73.1017 - val_loss: 0.0012 - val_energy_error: 0.9013\n",
      "Epoch 2/10000\n",
      "45507/45507 [==============================] - 1s 18us/step - loss: 0.0014 - energy_error: 0.1417 - val_loss: 0.0012 - val_energy_error: -0.0164\n",
      "Epoch 3/10000\n",
      "45507/45507 [==============================] - 1s 19us/step - loss: 0.0014 - energy_error: -0.0389 - val_loss: 0.0012 - val_energy_error: -0.0304\n",
      "Epoch 4/10000\n",
      "45507/45507 [==============================] - 1s 21us/step - loss: 0.0014 - energy_error: -0.0424 - val_loss: 0.0012 - val_energy_error: -0.0286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-69c29bd959bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 validation_data=(x_test_scaled, x_test_scaled))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    217\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-34937b3e1536>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msupermax\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "algoritmo='RMSprop'\n",
    "experimento=\"scaled_{}_encoder_without_bias_tanh_tanh_lr_{}\".format(supermax,factor_aprendizaje)\n",
    "tensorboard=TensorBoard(log_dir=\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/{}{}{}{}\".format(encoding_dim,algoritmo,experimento,datetime.now()))\n",
    "#modelCheckpoint=ModelCheckpoint(\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None)\n",
    "history=autoencoder.fit(x_train_scaled, x_train_scaled,\n",
    "                epochs=10000,\n",
    "                batch_size=200,\n",
    "                shuffle=False,\n",
    "                callbacks=[tensorboard, early_stop, metrics],\n",
    "                validation_data=(x_test_scaled, x_test_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15170/15170 [==============================] - 1s 83us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoencoder.evaluate(x=x_test_scaled,y=x_test_scaled)\n",
    "D=metrics.get_data()\n",
    "\n",
    "\n",
    "data=pd.DataFrame(D).values.reshape(len(D))\n",
    "valores=len(data)\n",
    "plt.bar(range(valores),data), \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in D:\n",
    "    print(item['val_energy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights('../redes_compresoras/compresor_python_{}{}{}{}'.format(encoding_dim,algoritmo,experimento,datetime.now()))\n",
    "#np.savez('../redes_compresoras/maxmin_python_ver_rms_prop_scaled_min_max_ver2', min_max_scaler.data_max_, min_max_scaler.data_min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scores = encoder.predict(x_test_scaled).ravel()\n",
    "#regularized_scores = encoded_regularized.predict(x_test).ravel()\n",
    "sns.distplot(standard_scores, hist=True, label='standard model')\n",
    "#sns.distplot(regularized_scores, hist=False, label='regularized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some images\n",
    "# note that we take them from the *test* set\n",
    "# encoded_imgs = encoder.predict(x_test_min_max)\n",
    "# decoded_imgs_scaled = decoder.predict(encoded_imgs)\n",
    "#decoded_imgs_scaled = autoencoder.predict(x_test_min_max)\n",
    "decoded_imgs_scaled = autoencoder.predict(x_test_scaled)\n",
    "decoded_imgs = supermax*(decoded_imgs_scaled+1)/2\n",
    "#decoded_imgs = min_max_scaler.inverse_transform(decoded_imgs_scaled)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_test.shape[0])\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[idea].reshape(40, 16).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[idea].reshape(40, 16).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print(idea)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = '../datos_octubre_2018/p_OF_5mm_161mm003.h5'\n",
    "conjunto_datos_test=pd.read_hdf(filename,'MC');\n",
    "conjunto_datos_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1A=6;\n",
    "# hay tres L1 con 640 sensores (40*16)\n",
    "L1B=0;\n",
    "# hay dos L1 con 640 sensores (40*16)\n",
    "X_trained=conjunto_datos_test.values;\n",
    "x_trained=X_trained;\n",
    "\n",
    "for i in range (X_trained.shape[0]):\n",
    "    idea1=X_trained[i,:].reshape(img_rows,(L1A*img_cols));\n",
    "    ideat=idea1.transpose();\n",
    "    idea2=ideat.reshape(1,(L1A*img_cols)*img_rows);\n",
    "    x_trained[i,:] =idea2;\n",
    "x_tested = x_trained;\n",
    "print(x_trained.shape)\n",
    "print(x_tested.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a procesar y comprimir con la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora los particionamos y pasamos por las redes de compresión. Hay una red la A que se utiliza 5 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "\n",
    "cara_externa=x_tested[:,0: L1A*input_output_dim_A] \n",
    "cara_externa_reconstruida=np.zeros((x_tested.shape[0],L1A*input_output_dim_A))\n",
    "for i in range(x_tested.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_tested[i,k*input_output_dim_A:k*input_output_dim_A+input_output_dim_A]\n",
    "    #ideaA_scaled=min_max_scaler.transform(ideaA)\n",
    "    ideaA_scaled=(2*ideaA/(supermax)) -1\n",
    "    salida_reconstructed_1_scaled = autoencoder.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled+1)/2\n",
    "    #salida_reconstructed_1 = min_max_scaler.inverse_transform(salida_reconstructed_1_scaled)     \n",
    "    #salida_reconstructed_1 = ideaA\n",
    "    \n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "    #entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "    #encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "    #decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "    #print(decoded_imgs_A.shape)\n",
    "    #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "    #salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    " \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "\n",
    "    #print(hola.shape)\n",
    "    salida_total=hola1\n",
    "    #salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos todos los sensores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 1  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_tested.shape[0])\n",
    "    idea=1890\n",
    "    idea= 4299\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_cols, img_rows).transpose(), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_cols, img_rows).transpose(), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos ahora L1 a L1, teniendo en cuenta que hay de dos tipos:\n",
    "L1A (con 36 columnas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = L1A  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_cols, img_rows).transpose()[:,i*img_cols:(i+1)*img_cols] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_cols, img_rows).transpose()[:,i*img_cols:(i+1)*img_cols] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "print(cara_externa[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:])\n",
    "print(np.sum(cara_externa[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cara_externa_reconstruida[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:].astype(int))\n",
    "print(np.sum(cara_externa_reconstruida[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idea)\n",
    "np.sum(cara_externa_reconstruida,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamos_energia=(np.sum(cara_externa_reconstruida, axis=1))-(np.sum(cara_externa, axis=1))\n",
    "n, bins, patches = plt.hist(veamos_energia, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=200, cen=0, wid=100)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "1249px",
    "right": "57px",
    "top": "240px",
    "width": "390px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
