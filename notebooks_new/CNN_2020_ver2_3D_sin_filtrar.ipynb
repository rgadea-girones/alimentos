{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rgadea/anaconda3/envs/tensorflow3/lib/python36.zip', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/lib-dynload', '', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/IPython/extensions', '/volumedisk0/home/rgadea/.ipython', '/home/rgadea/lmfit-py/', '/home/rgadea/experimentos/viherbos/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "# from sklearn import preprocessing\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "sys.path.append(\"/home/rgadea/experimentos/viherbos/\")\n",
    "\n",
    "print(sys.path)\n",
    "import json \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,Conv3D, MaxPooling3D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en pyhton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as sio\n",
    "# import Event_Handle_red as  eh\n",
    "# # import hdf5storage\n",
    "# # datos_matlab = hdf5storage.loadmat('../datos_junio_2019/conjunto_entrenamiento_junio_2019_pitch7mm_rad165mm_29_total_2_distancias_red.mat')\n",
    "# # conjunto_datos= datos_matlab.get('photodefbox2_todo_fil5')\n",
    "\n",
    "# # npzfile = np.load('../conjuntos_datos_reconstruidos/fil5_pith7mm_rad165mm_scaled2_sig_sig_1200.npz')\n",
    "# # npzfile.files\n",
    "# path = \"/home/rgadea/experimentos/viherbos/\"\n",
    "# data_file = \"MOVIE_DATA_batch1.h5\"\n",
    "# json_file = \"CUBE.json\"\n",
    "# def cart2pol(x, y,z):\n",
    "#     rho = np.sqrt(x**2 + y**2)\n",
    "#     phi = np.arctan2(y, x)\n",
    "#     return(rho, phi,z)\n",
    "\n",
    "\n",
    "# pepito=eh.Event_Handler(path, data_file, json_file)\n",
    "# prueba=pepito(7)\n",
    "# veamos=prueba['TRUE1']\n",
    "# polares=cart2pol(veamos[0], veamos[1], veamos[2])\n",
    "# print(np.asarray(polares))\n",
    "\n",
    "# idea=pepito.DATA_A.shape #obtengo el numero de muestras\n",
    "\n",
    "# entradas_sensorsA1=np.zeros([idea[0],20,175])\n",
    "# entradas_sensorsB1=np.zeros([idea[0],20,175])\n",
    "# coordenadas1=np.zeros([idea[0],3])\n",
    "# entradas_sensorsA2=np.zeros([idea[0],20,175])\n",
    "# entradas_sensorsB2=np.zeros([idea[0],20,175])\n",
    "# coordenadas2=np.zeros([idea[0],3])\n",
    "# for i in range(idea[0]):\n",
    "#     DATA=pepito(i)\n",
    "#     entradas_sensorsA1[i]=DATA['DATA_A1'];\n",
    "#     entradas_sensorsB1[i]=DATA['DATA_B1'];    \n",
    "#     veamos=DATA['TRUE1']\n",
    "#     polares=cart2pol(veamos[0], veamos[1], veamos[2])\n",
    "#     coordenadas1[i]=np.asarray(polares)\n",
    "#     entradas_sensorsA2[i]=DATA['DATA_A2'];\n",
    "#     entradas_sensorsB2[i]=DATA['DATA_B2'];    \n",
    "#     veamos=DATA['TRUE2']\n",
    "#     polares=cart2pol(veamos[0], veamos[1], veamos[2])\n",
    "#     coordenadas2[i]=np.asarray(polares)\n",
    "\n",
    "# np.savez('../conjuntos_datos_nuevos_2020/9_12_2019', entradas_sensorsA1,entradas_sensorsB1, coordenadas1,entradas_sensorsA2,entradas_sensorsB2, coordenadas2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto_datos_entradas A shape: (68002, 20, 175)\n",
      "conjunto_datos_entradas B shape: (68002, 20, 175)\n",
      "conjunto_datos_salidas shape: (68002, 3)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('../conjuntos_datos_nuevos_2020/11_12_2019.npz')\n",
    "npzfile.files\n",
    "\n",
    "# conjunto_datos_entradasA=npzfile['arr_0']\n",
    "# conjunto_datos_entradasB=npzfile['arr_1']\n",
    "# conjunto_datos_salidas=npzfile['arr_2']\n",
    "entradas_sensorsA1=npzfile['arr_0']\n",
    "entradas_sensorsB1=npzfile['arr_1']\n",
    "coordenadas1=npzfile['arr_2']\n",
    "entradas_sensorsA2=npzfile['arr_3']\n",
    "entradas_sensorsB2=npzfile['arr_4']\n",
    "coordenadas2=npzfile['arr_5']\n",
    "\n",
    "conjunto_datos_entradasA=np.concatenate((entradas_sensorsA1,entradas_sensorsA2),axis=0)\n",
    "conjunto_datos_entradasB=np.concatenate((entradas_sensorsB1,entradas_sensorsB2),axis=0)\n",
    "conjunto_datos_salidas=np.concatenate((coordenadas1,coordenadas2),axis=0)\n",
    "\n",
    "\n",
    "print('conjunto_datos_entradas A shape:', conjunto_datos_entradasA.shape)\n",
    "print('conjunto_datos_entradas B shape:', conjunto_datos_entradasB.shape)\n",
    "print('conjunto_datos_salidas shape:', conjunto_datos_salidas.shape)\n",
    "batch_size = 250\n",
    "nb_classes = 10\n",
    "nb_epoch = 2000\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 20, 41\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (1,2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (2, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector A shape: (68002, 20, 41)\n",
      "conjunto_datos_nuevo A: (68002, 820)\n",
      "sector B shape: (68002, 20, 41)\n",
      "conjunto_datos_nuevo B: (68002, 820)\n"
     ]
    }
   ],
   "source": [
    "muestras=conjunto_datos_entradasA.shape[0]\n",
    "# veamos=idea.reshape(idea.shape[0],175, 20)\n",
    "\n",
    "\n",
    "veamos2=np.zeros([muestras,20,175])\n",
    "veamos2_3=np.zeros([muestras,20,525])\n",
    "sector2A=np.zeros([muestras,20,img_cols])\n",
    "sector2B=np.zeros([muestras,20,img_cols])\n",
    "veamos3=np.zeros([muestras,175])\n",
    "# for i in range(idea.shape[0]):\n",
    "for i in range(muestras):\n",
    "    veamos2[i]=conjunto_datos_entradasA[i]\n",
    "    veamos3[i]=np.sum(veamos2[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    veamos2_3[i]=np.concatenate((veamos2[i],veamos2[i],veamos2[i]),axis=1) \n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    sector2A[i]=veamos2_3[i,:,indice_inferior:indice_superior]\n",
    "for i in range(muestras):\n",
    "    veamos2[i]=conjunto_datos_entradasB[i]\n",
    "    veamos3[i]=np.sum(veamos2[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    veamos2_3[i]=np.concatenate((veamos2[i],veamos2[i],veamos2[i]),axis=1) \n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    sector2B[i]=veamos2_3[i,:,indice_inferior:indice_superior]    \n",
    "\n",
    "print('sector A shape:', sector2A.shape)\n",
    "conjunto_datos_nuevoA=sector2A.reshape(sector2A.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo A:', conjunto_datos_nuevoA.shape)\n",
    "\n",
    "print('sector B shape:', sector2B.shape)\n",
    "conjunto_datos_nuevoB=sector2B.reshape(sector2B.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo B:', conjunto_datos_nuevoB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5  # how many digits we will display\n",
    "\n",
    "fig = plt.figure(figsize=(8,20))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ideas=np.random.randint(1,muestras)\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+1)\n",
    "    plt.imshow(sector2A[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+2)\n",
    "    plt.imshow(sector2B[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,sector2B.shape[0])\n",
    "    plt.imshow(sector2B[idea], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40801\n",
      "13600\n",
      "13601\n",
      "(5161, 1640)\n",
      "(2890, 1640)\n",
      "(2197, 1640)\n",
      "(1718, 1640)\n",
      "(1635, 1640)\n"
     ]
    }
   ],
   "source": [
    "numero_muestras=muestras\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "conjunto_datos_nuevo2=np.concatenate((conjunto_datos_salidas,conjunto_datos_nuevoB, conjunto_datos_nuevoA), axis=1)\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "XY_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "XY_test_bin0=XY_test[np.where((XY_test[:,0]>=164.9999) * (XY_test[:,0]<171.000))]\n",
    "XY_test_bin1=XY_test[np.where((XY_test[:,0]>=171.000) * (XY_test[:,0]<177.000))]\n",
    "XY_test_bin2=XY_test[np.where((XY_test[:,0]>=177.000) * (XY_test[:,0]<183.0000))]\n",
    "XY_test_bin3=XY_test[np.where((XY_test[:,0]>=183.000) * (XY_test[:,0]<189.0000))]\n",
    "XY_test_bin4=XY_test[np.where((XY_test[:,0]>=189.0000))]\n",
    "\n",
    "X_train=conjunto_datos_nuevo2[:tamanyo_tr,3:]\n",
    "X_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,3:]\n",
    "X_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,3:]\n",
    "\n",
    "X_test_bin0=XY_test_bin0[:,3:]\n",
    "Y_test_bin0=XY_test_bin0[:,0]\n",
    "print(X_test_bin0.shape)\n",
    "X_test_bin1=XY_test_bin1[:,3:]\n",
    "Y_test_bin1=XY_test_bin1[:,0]\n",
    "print(X_test_bin1.shape)\n",
    "X_test_bin2=XY_test_bin2[:,3:]\n",
    "Y_test_bin2=XY_test_bin2[:,0]\n",
    "print(X_test_bin2.shape)\n",
    "X_test_bin3=XY_test_bin3[:,3:]\n",
    "Y_test_bin3=XY_test_bin3[:,0]\n",
    "print(X_test_bin3.shape)\n",
    "X_test_bin4=XY_test_bin4[:,3:]\n",
    "Y_test_bin4=XY_test_bin4[:,0]\n",
    "print(X_test_bin4.shape)\n",
    "\n",
    "\n",
    "\n",
    "Y_train=conjunto_datos_nuevo2[:tamanyo_tr,0] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,0] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,0] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],2, img_rows, img_cols,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 2,img_rows, img_cols,1)\n",
    "\n",
    "X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], 2, img_rows, img_cols,1)\n",
    "\n",
    "input_shape = (2, img_rows, img_cols,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40801, 2, 20, 41, 1)\n",
      "40801 train samples\n",
      "13600 validation samples\n",
      "13601 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert class vectors to binary class matrices\n",
    "# #Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# #Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "# for i in range(1,5):\n",
    "#     idea=np.random.randint(1,X_train.shape[0])\n",
    "#     plt.imshow(np.reshape(X_train[idea], [img_rows, img_cols]), cmap='viridis')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=kernel_size,\n",
    "                        padding='same',\n",
    "                        data_format='channels_last',\n",
    "                        input_shape=(2,img_rows,img_cols,1)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "experimento=\"CNN_kernel_{}x{}x{}_con_batchnormalization_sector_{}x{}x{}\".format(kernel_size[0],kernel_size[1],kernel_size[2],img_rows,img_cols,1)\n",
    "algoritmo='Nadam'\n",
    "optimizador=Nadam(beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"../logs/defs/{}{}{}\".format(experimento,algoritmo,dt))\n",
    "best_model_name='../redes_CNN_R/models_best/CNN_regression_R_{}_{}_{}_{}_{}.h5'.format(nb_epoch,batch_size,experimento,algoritmo,dt)\n",
    "model_check=ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizador)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 2, 20, 41, 8)      264       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 20, 41, 8)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 10, 20, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 2, 10, 20, 16)     4112      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 10, 20, 16)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 5, 10, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 5, 10, 32)      16416     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 5, 10, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 5, 10, 64)      65600     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 5, 10, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 2, 5, 10, 128)     262272    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 12801     \n",
      "=================================================================\n",
      "Total params: 361,465\n",
      "Trainable params: 361,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40801 samples, validate on 13600 samples\n",
      "Epoch 1/2000\n",
      "40801/40801 [==============================] - 10s 237us/step - loss: 874.8143 - val_loss: 105.9901\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 105.99015, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 2/2000\n",
      "40801/40801 [==============================] - 3s 84us/step - loss: 69.1483 - val_loss: 54.8083\n",
      "\n",
      "Epoch 00002: val_loss improved from 105.99015 to 54.80825, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 3/2000\n",
      "40801/40801 [==============================] - 3s 84us/step - loss: 65.5044 - val_loss: 38.3456\n",
      "\n",
      "Epoch 00003: val_loss improved from 54.80825 to 38.34556, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 4/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 59.2962 - val_loss: 30.7163\n",
      "\n",
      "Epoch 00004: val_loss improved from 38.34556 to 30.71626, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 5/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 56.9360 - val_loss: 26.1580\n",
      "\n",
      "Epoch 00005: val_loss improved from 30.71626 to 26.15797, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 6/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 47.5374 - val_loss: 46.2089\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 26.15797\n",
      "Epoch 7/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 49.5966 - val_loss: 63.7743\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 26.15797\n",
      "Epoch 8/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 44.6441 - val_loss: 39.2428\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 26.15797\n",
      "Epoch 9/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 36.9184 - val_loss: 23.1724\n",
      "\n",
      "Epoch 00009: val_loss improved from 26.15797 to 23.17242, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 10/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 387.8647 - val_loss: 77.2154\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 23.17242\n",
      "Epoch 11/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 74.7649 - val_loss: 64.7048\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 23.17242\n",
      "Epoch 12/2000\n",
      "40801/40801 [==============================] - 3s 86us/step - loss: 72.7905 - val_loss: 80.9338\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 23.17242\n",
      "Epoch 13/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 82.9042 - val_loss: 112.7086\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 23.17242\n",
      "Epoch 14/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 77.8256 - val_loss: 115.9124\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 23.17242\n",
      "Epoch 15/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 81.6467 - val_loss: 52.4747\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 23.17242\n",
      "Epoch 16/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 67.7897 - val_loss: 80.5539\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 23.17242\n",
      "Epoch 17/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 70.7321 - val_loss: 42.0900\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 23.17242\n",
      "Epoch 18/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 72.9885 - val_loss: 98.2975\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 23.17242\n",
      "Epoch 19/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 65.2875 - val_loss: 57.1086\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 23.17242\n",
      "Epoch 20/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 57.6186 - val_loss: 35.7360\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 23.17242\n",
      "Epoch 21/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 72.1824 - val_loss: 43.0049\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 23.17242\n",
      "Epoch 22/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 61.8450 - val_loss: 64.7246\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 23.17242\n",
      "Epoch 23/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 64.5954 - val_loss: 34.5151\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23.17242\n",
      "Epoch 24/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 54.9313 - val_loss: 37.4609\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23.17242\n",
      "Epoch 25/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 56.0582 - val_loss: 44.0421\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.17242\n",
      "Epoch 26/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 50.9639 - val_loss: 30.2198\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23.17242\n",
      "Epoch 27/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 64.0407 - val_loss: 76.3955\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.17242\n",
      "Epoch 28/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 73.8359 - val_loss: 131.8434\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.17242\n",
      "Epoch 29/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 63.9951 - val_loss: 36.8473\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.17242\n",
      "Epoch 30/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 49.4657 - val_loss: 33.3319\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.17242\n",
      "Epoch 31/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 43.9680 - val_loss: 32.5549\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.17242\n",
      "Epoch 32/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 42.1584 - val_loss: 26.5555\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23.17242\n",
      "Epoch 33/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 36.5495 - val_loss: 20.2099\n",
      "\n",
      "Epoch 00033: val_loss improved from 23.17242 to 20.20987, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 34/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 38.0320 - val_loss: 23.0831\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 20.20987\n",
      "Epoch 35/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 36.3961 - val_loss: 18.7122\n",
      "\n",
      "Epoch 00035: val_loss improved from 20.20987 to 18.71221, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 36/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 33.8569 - val_loss: 22.0247\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 18.71221\n",
      "Epoch 37/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 40.0470 - val_loss: 33.9285\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 18.71221\n",
      "Epoch 38/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 35.5457 - val_loss: 154.6552\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 18.71221\n",
      "Epoch 39/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 28.6179 - val_loss: 17.5649\n",
      "\n",
      "Epoch 00039: val_loss improved from 18.71221 to 17.56494, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 40/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 36.5247 - val_loss: 50.9179\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 17.56494\n",
      "Epoch 41/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 30.8787 - val_loss: 17.2124\n",
      "\n",
      "Epoch 00041: val_loss improved from 17.56494 to 17.21240, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 42/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 34.8322 - val_loss: 29.9012\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 17.21240\n",
      "Epoch 43/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 30.4811 - val_loss: 18.1563\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 17.21240\n",
      "Epoch 44/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 27.9100 - val_loss: 20.6126\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 17.21240\n",
      "Epoch 45/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 31.2644 - val_loss: 41.4189\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 17.21240\n",
      "Epoch 46/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 30.8178 - val_loss: 29.4007\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 17.21240\n",
      "Epoch 47/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 29.7501 - val_loss: 39.9019\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 17.21240\n",
      "Epoch 48/2000\n",
      "40801/40801 [==============================] - 3s 86us/step - loss: 30.2712 - val_loss: 105.3835\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 17.21240\n",
      "Epoch 49/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 27.4886 - val_loss: 21.1338\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 17.21240\n",
      "Epoch 50/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 26.4109 - val_loss: 19.8504\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 17.21240\n",
      "Epoch 51/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 26.3607 - val_loss: 15.3360\n",
      "\n",
      "Epoch 00051: val_loss improved from 17.21240 to 15.33601, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 52/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 29.3162 - val_loss: 17.9340\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 15.33601\n",
      "Epoch 53/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 27.8748 - val_loss: 100.2488\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 15.33601\n",
      "Epoch 54/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 28.1610 - val_loss: 20.5712\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 15.33601\n",
      "Epoch 55/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 22.1056 - val_loss: 15.3832\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 15.33601\n",
      "Epoch 56/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 28.9867 - val_loss: 20.0288\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 15.33601\n",
      "Epoch 57/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 24.1792 - val_loss: 23.5862\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 15.33601\n",
      "Epoch 58/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 25.0752 - val_loss: 41.4709\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 15.33601\n",
      "Epoch 59/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 25.3297 - val_loss: 17.0035\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 15.33601\n",
      "Epoch 60/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 22.7718 - val_loss: 23.0992\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 15.33601\n",
      "Epoch 61/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 26.6815 - val_loss: 16.1074\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 15.33601\n",
      "Epoch 62/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 21.8536 - val_loss: 22.9558\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 15.33601\n",
      "Epoch 63/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 26.2383 - val_loss: 30.5242\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 15.33601\n",
      "Epoch 64/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 21.7665 - val_loss: 18.8942\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 15.33601\n",
      "Epoch 65/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 24.7180 - val_loss: 22.4432\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 15.33601\n",
      "Epoch 66/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 21.5504 - val_loss: 18.7860\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 15.33601\n",
      "Epoch 67/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 23.4191 - val_loss: 28.5583\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 15.33601\n",
      "Epoch 68/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 25.5934 - val_loss: 16.1223\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 15.33601\n",
      "Epoch 69/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 19.3338 - val_loss: 13.7632\n",
      "\n",
      "Epoch 00069: val_loss improved from 15.33601 to 13.76320, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 70/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 21.2331 - val_loss: 15.1303\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 13.76320\n",
      "Epoch 71/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 20.4838 - val_loss: 38.0854\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 13.76320\n",
      "Epoch 72/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 24.2201 - val_loss: 25.0697\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 13.76320\n",
      "Epoch 73/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 21.1811 - val_loss: 84.0953\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 13.76320\n",
      "Epoch 74/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 20.8945 - val_loss: 13.5392\n",
      "\n",
      "Epoch 00074: val_loss improved from 13.76320 to 13.53920, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 75/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 21.0915 - val_loss: 13.7172\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 13.53920\n",
      "Epoch 76/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 20.8035 - val_loss: 25.4689\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 13.53920\n",
      "Epoch 77/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 21.2560 - val_loss: 17.8262\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 13.53920\n",
      "Epoch 78/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 19.7070 - val_loss: 19.7601\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 13.53920\n",
      "Epoch 79/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 21.7037 - val_loss: 16.5170\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 13.53920\n",
      "Epoch 80/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 21.1488 - val_loss: 16.0996\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 13.53920\n",
      "Epoch 81/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 19.8222 - val_loss: 28.4623\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 13.53920\n",
      "Epoch 82/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 17.2008 - val_loss: 14.0946\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 13.53920\n",
      "Epoch 83/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 22.2575 - val_loss: 16.1044\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 13.53920\n",
      "Epoch 84/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 20.4198 - val_loss: 13.1905\n",
      "\n",
      "Epoch 00084: val_loss improved from 13.53920 to 13.19047, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 85/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 19.0161 - val_loss: 12.8019\n",
      "\n",
      "Epoch 00085: val_loss improved from 13.19047 to 12.80187, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 86/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 18.8375 - val_loss: 61.4432\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 12.80187\n",
      "Epoch 87/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 19.2546 - val_loss: 26.7889\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 12.80187\n",
      "Epoch 88/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 20.7957 - val_loss: 13.3978\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 12.80187\n",
      "Epoch 89/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 16.2141 - val_loss: 13.1368\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 12.80187\n",
      "Epoch 90/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 20.3977 - val_loss: 18.1403\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 12.80187\n",
      "Epoch 91/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 19.0870 - val_loss: 14.9071\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 12.80187\n",
      "Epoch 92/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.8300 - val_loss: 42.3401\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 12.80187\n",
      "Epoch 93/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 18.6385 - val_loss: 13.9833\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 12.80187\n",
      "Epoch 94/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.3453 - val_loss: 18.2265\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 12.80187\n",
      "Epoch 95/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 17.2335 - val_loss: 15.5365\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 12.80187\n",
      "Epoch 96/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 19.8585 - val_loss: 34.8899\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 12.80187\n",
      "Epoch 97/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 16.9209 - val_loss: 38.4303\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 12.80187\n",
      "Epoch 98/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 17.9216 - val_loss: 20.6539\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 12.80187\n",
      "Epoch 99/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 17.9627 - val_loss: 21.6117\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 12.80187\n",
      "Epoch 100/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 18.4508 - val_loss: 13.1254\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 12.80187\n",
      "Epoch 101/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.5906 - val_loss: 19.5133\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 12.80187\n",
      "Epoch 102/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 17.7966 - val_loss: 17.4136\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 12.80187\n",
      "Epoch 103/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 16.3916 - val_loss: 12.9424\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 12.80187\n",
      "Epoch 104/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.3059 - val_loss: 12.1591\n",
      "\n",
      "Epoch 00104: val_loss improved from 12.80187 to 12.15913, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 105/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 17.2201 - val_loss: 16.4635\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 12.15913\n",
      "Epoch 106/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 17.9900 - val_loss: 12.5185\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 12.15913\n",
      "Epoch 107/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 15.6559 - val_loss: 14.3192\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 12.15913\n",
      "Epoch 108/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.2361 - val_loss: 32.9524\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 12.15913\n",
      "Epoch 109/2000\n",
      "40801/40801 [==============================] - 3s 86us/step - loss: 17.4926 - val_loss: 15.2128\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 12.15913\n",
      "Epoch 110/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 15.6905 - val_loss: 14.7032\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 12.15913\n",
      "Epoch 111/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 17.0680 - val_loss: 12.4342\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 12.15913\n",
      "Epoch 112/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 15.3958 - val_loss: 40.4427\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 12.15913\n",
      "Epoch 113/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 16.2393 - val_loss: 25.0966\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 12.15913\n",
      "Epoch 114/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 17.0161 - val_loss: 11.8203\n",
      "\n",
      "Epoch 00114: val_loss improved from 12.15913 to 11.82033, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 115/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.0574 - val_loss: 13.2214\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 11.82033\n",
      "Epoch 116/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 17.1318 - val_loss: 12.8744\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 11.82033\n",
      "Epoch 117/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 15.5715 - val_loss: 13.5167\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 11.82033\n",
      "Epoch 118/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 30.1528 - val_loss: 14.9932\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 11.82033\n",
      "Epoch 119/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 16.4582 - val_loss: 21.1144\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 11.82033\n",
      "Epoch 120/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 16.3549 - val_loss: 15.8518\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 11.82033\n",
      "Epoch 121/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.6090 - val_loss: 13.4784\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 11.82033\n",
      "Epoch 122/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 16.2611 - val_loss: 15.5346\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 11.82033\n",
      "Epoch 123/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 17.0846 - val_loss: 18.9620\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 11.82033\n",
      "Epoch 124/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 17.1462 - val_loss: 18.8302\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 11.82033\n",
      "Epoch 125/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 15.7546 - val_loss: 41.9982\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 11.82033\n",
      "Epoch 126/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 17.3593 - val_loss: 14.6862\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 11.82033\n",
      "Epoch 127/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 16.7155 - val_loss: 12.4363\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 11.82033\n",
      "Epoch 128/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.8335 - val_loss: 15.1325\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 11.82033\n",
      "Epoch 129/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 17.8455 - val_loss: 12.0179\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 11.82033\n",
      "Epoch 130/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 15.9067 - val_loss: 12.3358\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 11.82033\n",
      "Epoch 131/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 17.6797 - val_loss: 15.1069\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 11.82033\n",
      "Epoch 132/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.3737 - val_loss: 24.7496\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 11.82033\n",
      "Epoch 133/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 15.5768 - val_loss: 14.0625\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 11.82033\n",
      "Epoch 134/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 16.5087 - val_loss: 13.3394\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 11.82033\n",
      "Epoch 135/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 15.7168 - val_loss: 26.2626\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 11.82033\n",
      "Epoch 136/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 16.5033 - val_loss: 29.1742\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 11.82033\n",
      "Epoch 137/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.2854 - val_loss: 17.3468\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 11.82033\n",
      "Epoch 138/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 15.6651 - val_loss: 12.2814\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 11.82033\n",
      "Epoch 139/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 14.8433 - val_loss: 19.9992\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 11.82033\n",
      "Epoch 140/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 17.1234 - val_loss: 12.8684\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 11.82033\n",
      "Epoch 141/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.5946 - val_loss: 13.6845\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 11.82033\n",
      "Epoch 142/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.3962 - val_loss: 11.8828\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 11.82033\n",
      "Epoch 143/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 16.1502 - val_loss: 12.3381\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 11.82033\n",
      "Epoch 144/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.7071 - val_loss: 28.2128\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 11.82033\n",
      "Epoch 145/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 15.3471 - val_loss: 14.9613\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 11.82033\n",
      "Epoch 146/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.7331 - val_loss: 17.6414\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 11.82033\n",
      "Epoch 147/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.7532 - val_loss: 11.9825\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 11.82033\n",
      "Epoch 148/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.9812 - val_loss: 12.4413\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 11.82033\n",
      "Epoch 149/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.6072 - val_loss: 11.6896\n",
      "\n",
      "Epoch 00149: val_loss improved from 11.82033 to 11.68962, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_Nadam_2019-12-16 16:17:00.h5\n",
      "Epoch 150/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 15.4477 - val_loss: 12.6614\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 11.68962\n",
      "Epoch 151/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 14.2639 - val_loss: 13.8310\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 11.68962\n",
      "Epoch 152/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 13.6838 - val_loss: 23.7495\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 11.68962\n",
      "Epoch 153/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.9348 - val_loss: 12.9115\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 11.68962\n",
      "Epoch 154/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 14.0543 - val_loss: 16.8089\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 11.68962\n",
      "Epoch 155/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.9409 - val_loss: 12.4105\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 11.68962\n",
      "Epoch 156/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 13.8128 - val_loss: 14.5135\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 11.68962\n",
      "Epoch 157/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.2909 - val_loss: 14.3782\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 11.68962\n",
      "Epoch 158/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 15.2321 - val_loss: 22.1866\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 11.68962\n",
      "Epoch 159/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 14.3645 - val_loss: 13.4031\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 11.68962\n",
      "Epoch 160/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 12.5645 - val_loss: 16.8772\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 11.68962\n",
      "Epoch 161/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 13.7124 - val_loss: 11.7719\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 11.68962\n",
      "Epoch 162/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 13.9149 - val_loss: 12.9595\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 11.68962\n",
      "Epoch 163/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 14.4747 - val_loss: 12.2556\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 11.68962\n",
      "Epoch 164/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 13.5502 - val_loss: 12.6231\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 11.68962\n",
      "Epoch 165/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 14.4976 - val_loss: 12.3884\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 11.68962\n",
      "Epoch 166/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 13.6275 - val_loss: 16.5982\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 11.68962\n",
      "Epoch 167/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 13.1390 - val_loss: 24.3220\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 11.68962\n",
      "Epoch 168/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 14.7464 - val_loss: 19.4469\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 11.68962\n",
      "Epoch 169/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 12.3862 - val_loss: 28.0776\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 11.68962\n",
      "Epoch 170/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 13.8149 - val_loss: 14.7536\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 11.68962\n",
      "Epoch 171/2000\n",
      "40801/40801 [==============================] - 4s 95us/step - loss: 14.2196 - val_loss: 11.8561\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 11.68962\n",
      "Epoch 172/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 12.4672 - val_loss: 18.3245\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 11.68962\n",
      "Epoch 173/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 13.3291 - val_loss: 13.1417\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 11.68962\n",
      "Epoch 174/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.6502 - val_loss: 19.3692\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 11.68962\n",
      "Epoch 175/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 13.9518 - val_loss: 12.4511\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 11.68962\n",
      "Epoch 176/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 13.5702 - val_loss: 16.1107\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 11.68962\n",
      "Epoch 177/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.8809 - val_loss: 12.9886\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 11.68962\n",
      "Epoch 178/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 12.9608 - val_loss: 12.1084\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 11.68962\n",
      "Epoch 179/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 12.4561 - val_loss: 11.9045\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 11.68962\n",
      "Epoch 180/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 13.5914 - val_loss: 15.3023\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 11.68962\n",
      "Epoch 181/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.8669 - val_loss: 12.2503\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 11.68962\n",
      "Epoch 182/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 13.4100 - val_loss: 20.9728\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 11.68962\n",
      "Epoch 183/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 12.6188 - val_loss: 26.6607\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 11.68962\n",
      "Epoch 184/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.1621 - val_loss: 23.0424\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 11.68962\n",
      "Epoch 185/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 11.6419 - val_loss: 13.4107\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 11.68962\n",
      "Epoch 186/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 13.2612 - val_loss: 15.9248\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 11.68962\n",
      "Epoch 187/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.5993 - val_loss: 13.9112\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 11.68962\n",
      "Epoch 188/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 11.9335 - val_loss: 12.1616\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 11.68962\n",
      "Epoch 189/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 13.3867 - val_loss: 25.6325\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 11.68962\n",
      "Epoch 190/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 11.3722 - val_loss: 22.6686\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 11.68962\n",
      "Epoch 191/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 14.2420 - val_loss: 23.2176\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 11.68962\n",
      "Epoch 192/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.8839 - val_loss: 24.6660\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 11.68962\n",
      "Epoch 193/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 13.3380 - val_loss: 16.0291\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 11.68962\n",
      "Epoch 194/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.2710 - val_loss: 25.9617\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 11.68962\n",
      "Epoch 195/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.9374 - val_loss: 16.7307\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 11.68962\n",
      "Epoch 196/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.9140 - val_loss: 12.6092\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 11.68962\n",
      "Epoch 197/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.6736 - val_loss: 19.4762\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 11.68962\n",
      "Epoch 198/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 12.6866 - val_loss: 13.3226\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 11.68962\n",
      "Epoch 199/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.2959 - val_loss: 13.0617\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 11.68962\n",
      "Epoch 200/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 11.0892 - val_loss: 17.1313\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 11.68962\n",
      "Epoch 201/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 12.3304 - val_loss: 26.3921\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 11.68962\n",
      "Epoch 202/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 11.6094 - val_loss: 17.9039\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 11.68962\n",
      "Epoch 203/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 10.6151 - val_loss: 22.0087\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 11.68962\n",
      "Epoch 204/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 13.0769 - val_loss: 16.8092\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 11.68962\n",
      "Epoch 205/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 11.2583 - val_loss: 13.7195\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 11.68962\n",
      "Epoch 206/2000\n",
      "40801/40801 [==============================] - 3s 85us/step - loss: 11.0748 - val_loss: 15.3518\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 11.68962\n",
      "Epoch 207/2000\n",
      "40801/40801 [==============================] - 3s 85us/step - loss: 11.9563 - val_loss: 16.0722\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 11.68962\n",
      "Epoch 208/2000\n",
      "40801/40801 [==============================] - 3s 86us/step - loss: 12.1462 - val_loss: 16.5167\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 11.68962\n",
      "Epoch 209/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 11.0893 - val_loss: 21.1651\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 11.68962\n",
      "Epoch 210/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 11.2367 - val_loss: 22.0226\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 11.68962\n",
      "Epoch 211/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 11.1744 - val_loss: 19.4689\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 11.68962\n",
      "Epoch 212/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 11.4551 - val_loss: 13.9915\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 11.68962\n",
      "Epoch 213/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 11.6668 - val_loss: 13.2376\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 11.68962\n",
      "Epoch 214/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 11.0245 - val_loss: 13.7906\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 11.68962\n",
      "Epoch 215/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 11.2171 - val_loss: 14.4868\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 11.68962\n",
      "Epoch 216/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 11.1113 - val_loss: 12.8070\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 11.68962\n",
      "Epoch 217/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 11.5874 - val_loss: 15.0247\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 11.68962\n",
      "Epoch 218/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 8.7397 - val_loss: 16.3334\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 11.68962\n",
      "Epoch 219/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 12.8351 - val_loss: 17.0844\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 11.68962\n",
      "Epoch 220/2000\n",
      "40801/40801 [==============================] - 4s 96us/step - loss: 9.3129 - val_loss: 16.3770\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 11.68962\n",
      "Epoch 221/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 12.1625 - val_loss: 14.5387\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 11.68962\n",
      "Epoch 222/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 11.5398 - val_loss: 13.5406\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 11.68962\n",
      "Epoch 223/2000\n",
      "40801/40801 [==============================] - 4s 98us/step - loss: 9.9625 - val_loss: 13.5341\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 11.68962\n",
      "Epoch 224/2000\n",
      "40801/40801 [==============================] - 4s 98us/step - loss: 10.6056 - val_loss: 35.8614\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 11.68962\n",
      "Epoch 225/2000\n",
      "40801/40801 [==============================] - 4s 97us/step - loss: 11.0354 - val_loss: 15.0490\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 11.68962\n",
      "Epoch 226/2000\n",
      "40801/40801 [==============================] - 4s 95us/step - loss: 10.0893 - val_loss: 18.1636\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 11.68962\n",
      "Epoch 227/2000\n",
      "40801/40801 [==============================] - 4s 95us/step - loss: 10.9447 - val_loss: 14.5974\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 11.68962\n",
      "Epoch 228/2000\n",
      "40801/40801 [==============================] - 4s 97us/step - loss: 10.8177 - val_loss: 19.4403\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 11.68962\n",
      "Epoch 229/2000\n",
      "40801/40801 [==============================] - 4s 96us/step - loss: 10.4358 - val_loss: 22.4904\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 11.68962\n",
      "Epoch 230/2000\n",
      "40801/40801 [==============================] - 4s 95us/step - loss: 10.2866 - val_loss: 17.2452\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 11.68962\n",
      "Epoch 231/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 10.7683 - val_loss: 15.2232\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 11.68962\n",
      "Epoch 232/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 11.2985 - val_loss: 14.1974\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 11.68962\n",
      "Epoch 233/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 9.7855 - val_loss: 20.4391\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 11.68962\n",
      "Epoch 234/2000\n",
      "40801/40801 [==============================] - 4s 96us/step - loss: 11.6862 - val_loss: 13.2623\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 11.68962\n",
      "Epoch 235/2000\n",
      "40801/40801 [==============================] - 4s 94us/step - loss: 8.4428 - val_loss: 35.1459\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 11.68962\n",
      "Epoch 236/2000\n",
      "40801/40801 [==============================] - 4s 95us/step - loss: 11.6938 - val_loss: 13.7440\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 11.68962\n",
      "Epoch 237/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 9.4874 - val_loss: 14.2055\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 11.68962\n",
      "Epoch 238/2000\n",
      "40801/40801 [==============================] - 4s 93us/step - loss: 9.8573 - val_loss: 16.5738\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 11.68962\n",
      "Epoch 239/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 11.4518 - val_loss: 13.6403\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 11.68962\n",
      "Epoch 240/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 11.0070 - val_loss: 14.6614\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 11.68962\n",
      "Epoch 241/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 9.3808 - val_loss: 26.3070\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 11.68962\n",
      "Epoch 242/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.6029 - val_loss: 14.4048\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 11.68962\n",
      "Epoch 243/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 11.0043 - val_loss: 13.6533\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 11.68962\n",
      "Epoch 244/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 8.7877 - val_loss: 23.2620\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 11.68962\n",
      "Epoch 245/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.3825 - val_loss: 24.8912\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 11.68962\n",
      "Epoch 246/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.9522 - val_loss: 23.5518\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 11.68962\n",
      "Epoch 247/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.0523 - val_loss: 23.1870\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 11.68962\n",
      "Epoch 248/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 10.5954 - val_loss: 18.0815\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 11.68962\n",
      "Epoch 249/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 8.5597 - val_loss: 20.2250\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 11.68962\n",
      "Epoch 250/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 11.2247 - val_loss: 18.9382\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 11.68962\n",
      "Epoch 251/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.9927 - val_loss: 28.0826\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 11.68962\n",
      "Epoch 252/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.6706 - val_loss: 14.7118\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 11.68962\n",
      "Epoch 253/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 11.1542 - val_loss: 18.2998\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 11.68962\n",
      "Epoch 254/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 10.3961 - val_loss: 13.1932\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 11.68962\n",
      "Epoch 255/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 9.2190 - val_loss: 13.4829\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 11.68962\n",
      "Epoch 256/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.2932 - val_loss: 31.2952\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 11.68962\n",
      "Epoch 257/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 10.6934 - val_loss: 15.4550\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 11.68962\n",
      "Epoch 258/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 10.2106 - val_loss: 21.1128\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 11.68962\n",
      "Epoch 259/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 8.9240 - val_loss: 14.5602\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 11.68962\n",
      "Epoch 260/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 8.7391 - val_loss: 15.3323\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 11.68962\n",
      "Epoch 261/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.1660 - val_loss: 13.9485\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 11.68962\n",
      "Epoch 262/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.7009 - val_loss: 14.6093\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 11.68962\n",
      "Epoch 263/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.5608 - val_loss: 13.5729\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 11.68962\n",
      "Epoch 264/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 10.0564 - val_loss: 13.6012\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 11.68962\n",
      "Epoch 265/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 8.5895 - val_loss: 14.4552\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 11.68962\n",
      "Epoch 266/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 8.4861 - val_loss: 13.7974\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 11.68962\n",
      "Epoch 267/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 10.8681 - val_loss: 18.2876\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 11.68962\n",
      "Epoch 268/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 8.9022 - val_loss: 39.4724\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 11.68962\n",
      "Epoch 269/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 9.6368 - val_loss: 14.1853\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 11.68962\n",
      "Epoch 270/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 9.6031 - val_loss: 13.7805\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 11.68962\n",
      "Epoch 271/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 9.4531 - val_loss: 21.9218\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 11.68962\n",
      "Epoch 272/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 9.5419 - val_loss: 17.0609\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 11.68962\n",
      "Epoch 273/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 9.4111 - val_loss: 15.1710\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 11.68962\n",
      "Epoch 274/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 8.3522 - val_loss: 13.7969\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 11.68962\n",
      "Epoch 275/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.7447 - val_loss: 13.5282\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 11.68962\n",
      "Epoch 276/2000\n",
      "40801/40801 [==============================] - 4s 92us/step - loss: 10.1517 - val_loss: 14.3172\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 11.68962\n",
      "Epoch 277/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 7.0960 - val_loss: 19.9532\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 11.68962\n",
      "Epoch 278/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 10.3513 - val_loss: 15.6165\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 11.68962\n",
      "Epoch 279/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 8.4348 - val_loss: 13.8062\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 11.68962\n",
      "Epoch 280/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 10.3469 - val_loss: 15.5769\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 11.68962\n",
      "Epoch 281/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 8.3458 - val_loss: 13.6220\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 11.68962\n",
      "Epoch 282/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 7.9915 - val_loss: 14.4986\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 11.68962\n",
      "Epoch 283/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.4720 - val_loss: 16.3794\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 11.68962\n",
      "Epoch 284/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 9.6428 - val_loss: 17.4757\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 11.68962\n",
      "Epoch 285/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.1487 - val_loss: 17.7996\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 11.68962\n",
      "Epoch 286/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 10.1674 - val_loss: 24.5218\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 11.68962\n",
      "Epoch 287/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.8172 - val_loss: 13.9980\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 11.68962\n",
      "Epoch 288/2000\n",
      "40801/40801 [==============================] - 4s 91us/step - loss: 8.9843 - val_loss: 18.1137\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 11.68962\n",
      "Epoch 289/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 9.5555 - val_loss: 14.7487\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 11.68962\n",
      "Epoch 290/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.5367 - val_loss: 25.4529\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 11.68962\n",
      "Epoch 291/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 7.5936 - val_loss: 16.6986\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 11.68962\n",
      "Epoch 292/2000\n",
      "40801/40801 [==============================] - 4s 88us/step - loss: 9.0917 - val_loss: 33.6536\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 11.68962\n",
      "Epoch 293/2000\n",
      "40801/40801 [==============================] - 4s 86us/step - loss: 8.0836 - val_loss: 15.2372\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 11.68962\n",
      "Epoch 294/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 10.7999 - val_loss: 14.9698\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 11.68962\n",
      "Epoch 295/2000\n",
      "40801/40801 [==============================] - 4s 90us/step - loss: 7.0862 - val_loss: 14.5439\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 11.68962\n",
      "Epoch 296/2000\n",
      "40801/40801 [==============================] - 4s 89us/step - loss: 8.2236 - val_loss: 14.3050\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 11.68962\n",
      "Epoch 297/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 9.0406 - val_loss: 19.1263\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 11.68962\n",
      "Epoch 298/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.1872 - val_loss: 14.4465\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 11.68962\n",
      "Epoch 299/2000\n",
      "40801/40801 [==============================] - 4s 87us/step - loss: 8.0343 - val_loss: 19.0265\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 11.68962\n",
      "Epoch 300/2000\n",
      "13750/40801 [=========>....................] - ETA: 2s - loss: 12.2836"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val),\n",
    "                     callbacks=[tensorboard,model_check,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "model.save_weights('../redes_CNN_R/defs/CNN_regression_R_{}_{}_{}_{}_{}'.format(nb_epoch,batch_size,experimento,algoritmo,dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(best_model_name)\n",
    "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test mse:', score)\n",
    "# print('Test mae:', score[1])\n",
    "Y_test_predicted=model.predict(X_test)\n",
    "print(Y_test_predicted[:10].flatten())\n",
    "print(Y_test[:10])\n",
    "error_prediction=Y_test-Y_test_predicted.flatten()\n",
    "\n",
    "print(error_prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(error_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(error_prediction, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWHM=result.params['wid'].value*2*sqrt(log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FWHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin0_predicted=model.predict(X_test_bin0)\n",
    "print(Y_test_bin0_predicted)\n",
    "error_prediction_bin0=Y_test_bin0-Y_test_bin0_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin0, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin0=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin1_predicted=model.predict(X_test_bin1)\n",
    "#print(Y_test_bin1_predicted)\n",
    "error_prediction_bin1=Y_test_bin1-Y_test_bin1_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin1, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin1=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin2_predicted=model.predict(X_test_bin2)\n",
    "#print(Y_test_bin2_predicted)\n",
    "error_prediction_bin2=Y_test_bin2-Y_test_bin2_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin2, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin2=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin3_predicted=model.predict(X_test_bin3)\n",
    "#print(Y_test_bin3_predicted)\n",
    "error_prediction_bin3=Y_test_bin3-Y_test_bin3_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin3, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin3=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin4_predicted=model.predict(X_test_bin4)\n",
    "#print(Y_test_bin4_predicted)\n",
    "error_prediction_bin4=Y_test_bin4-Y_test_bin4_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin4, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin4=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin4)\n",
    "print(FWHM_bin3)\n",
    "print(FWHM_bin2)\n",
    "print(FWHM_bin1)\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora los histogramnas 2d que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3] *",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "909px",
    "right": "57px",
    "top": "246px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
