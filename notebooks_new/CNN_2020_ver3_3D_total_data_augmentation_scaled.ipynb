{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio \n",
    "\n",
    "Comentarios:\n",
    "\n",
    "Intenté escalar ty fue un verdadero fracaso, así que no utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rgadea/anaconda3/envs/tensorflow3/lib/python36.zip', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/lib-dynload', '', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/IPython/extensions', '/volumedisk0/home/rgadea/.ipython', '/home/rgadea/lmfit-py/', '/home/rgadea/experimentos/viherbos/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "sys.path.append(\"/home/rgadea/experimentos/viherbos/\")\n",
    "\n",
    "print(sys.path)\n",
    "import json \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,Conv3D, MaxPooling3D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en pyhton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto_datos_entradas A shape: (39012, 20, 175)\n",
      "conjunto_datos_entradas B shape: (39012, 20, 175)\n",
      "conjunto_datos_salidas shape: (39012, 3)\n"
     ]
    }
   ],
   "source": [
    "filtro=2\n",
    "if filtro==1:\n",
    "    npzfile = np.load('../conjuntos_datos_nuevos_2020/20_12_2019_comptom_filt.npz')\n",
    "    npzfile.files\n",
    "    conjunto_datos_entradasA=npzfile['arr_0']\n",
    "    conjunto_datos_entradasB=npzfile['arr_1']\n",
    "    conjunto_datos_salidas=npzfile['arr_2']\n",
    "else:\n",
    "    if filtro==2:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/29_12_2019_comptom_filt4.npz')\n",
    "        npzfile.files\n",
    "        conjunto_datos_entradasA=npzfile['arr_0']\n",
    "        conjunto_datos_entradasB=npzfile['arr_1']\n",
    "        conjunto_datos_salidas=npzfile['arr_2']\n",
    "    else:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/11_12_2019.npz')\n",
    "        npzfile.files\n",
    "        entradas_sensorsA1=npzfile['arr_0']\n",
    "        entradas_sensorsB1=npzfile['arr_1']\n",
    "        coordenadas1=npzfile['arr_2']\n",
    "        entradas_sensorsA2=npzfile['arr_3']\n",
    "        entradas_sensorsB2=npzfile['arr_4']\n",
    "        coordenadas2=npzfile['arr_5']\n",
    "        conjunto_datos_entradasA=np.concatenate((entradas_sensorsA1,entradas_sensorsA2),axis=0)\n",
    "        conjunto_datos_entradasB=np.concatenate((entradas_sensorsB1,entradas_sensorsB2),axis=0)\n",
    "        conjunto_datos_salidas=np.concatenate((coordenadas1,coordenadas2),axis=0)\n",
    "\n",
    "\n",
    "print('conjunto_datos_entradas A shape:', conjunto_datos_entradasA.shape)\n",
    "print('conjunto_datos_entradas B shape:', conjunto_datos_entradasB.shape)\n",
    "print('conjunto_datos_salidas shape:', conjunto_datos_salidas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179.48995755 232.36752293  28.5995636 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179.04059897 235.0181863  -14.96378708]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191.78897104  87.78691228  -3.52812505]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABICAYAAADvR65LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAKhElEQVR4nO3df4wcdRnH8ffj9Qe0Ctd6FAsltNBCRHPyo8hVkSBQgYYAJiYUSSwR00giwRqVNk0I+hegUTQxYKMIaoViRWgaDAIi/kNLKcrRAkcPATl+9U4BtSWxDY9/zHfa2bmZ293b7czO9fNKNrPzndmdp8/efPvsd7+za+6OiIhUzwfKDkBERMZHHbiISEWpAxcRqSh14CIiFaUOXESkotSBi4hUVEsduJldYGYDZjZoZivbFZSIiNRn450HbmZdwAvAYmAI2AJc7u7Pti88ERHJ00oF/klg0N3/7u7/A+4GLmlPWCIiUs+kFh57NPBqYn0IOCO9k5ktB5YDdNF12jQOa+GQItJ20w+Nlrvea3rfvT3TAZg0sqt2eyz1nHt7pjPpvfdrt6WPH9b3HhrVl3X3Pwj8h7dH3P2IdHsrHbhltI0aj3H3NcAagMNspp9h57ZwSBFpu91hGZ/Rfb2wqT97397eaBlvXxDW/9lf+1x9UfvgZdMAmL8u2jDSO53Jlw4DMDzQE21bsan2+LtTy77UMdPxHgQe9vWvZLW3MoQyBByTWJ8DvN7C84mISBNaqcC3AAvMbB7wGrAU+GJbohKR9okr2FhedZ21PV39ph8br+fs1927qGb3nv5d0B9V5Ydv2pQdZ7340tX9ik2jttV9jgli3B24u+81s68BDwJdwO3uvr1tkYmIyJhaqcBx9weAB9oUi4i0U5HVaM4x3vlo9LHYldc+BsDaV07n8O9GlfPgD/sA6H4uGszeesOtAJx2w9U1z9Gz5nEARpYvqlmfHxfeyTH7g6TyjulKTBGRimqpAheRDtZoNZqu1DNmoaSr31GPjffrjaYV9vRH0wrj2SfXXBZNorjjR0tgU/QcR1w/H4BhotkoceUdPzaOIa7U56+oPfa+mOL9D0KqwEVEKmrcl9KPh+aBy4TUyFjzRJkdkVWtk6i8U+PV8Rh4PM49+dJhrjh2CxCq8cQ+cbU+SoOzUjIfM0Hy/rCv3+ruC9PtqsBFRCpKFbjIRFdEFZqqxGNXXhtNUrvljxeOesiLl90GwPHrvlrTPqoST1fTaa38uypSoasCFxGZYDQLRWSiy6sux1F95s5GCdIzSNZeenrN9vnrdvPu9VGFff5RJwPQvTznS03y4kq3j/XdLfV0eOVdjypwEZGKUgUuIvWFaj1deacr8n1ztsNVkvFVl3t691fZcdu+50zP486qsBPtme8CxjuWXZEx8Dx1K3Azu93MdprZtkTbTDN7yMx2hOWMAxumiIik1Z2FYmZnAf8FfunuHw9tNwP/cvcbw29hznD36+odTLNQRCogOaacM+87d2ZIqpK9aPvbwP453z39u0ZdYRnPEd9XUTdbFY81D3yCyJuFUncIxd3/YmZzU82XAGeH+3cCfwbqduAi0gHyOt2sjrPBr57dN6yR+obYjR8Lb86Xj35o0x13I0MpeZr9St2KGO+HmEe6+xsAYTmrfSGJiEgjGrqQJ1TgGxNDKO+4e3di+9vunjkOnvxNzEOYdtqZtqQNYYtIKZr80YWGvl4glvfhZd725H4TpKLO0+4Led4ys9kAYbkzb0d3X+PuC9194WSmjvNwIiKSNt5phBuAZcCNYXl/2yISkc5Vb3w6b7+xxtdbvUR+glffY2lkGuFdwOPAiWY2ZGZXEXXci81sB7A4rIuISIEamYVyec4mzQcUmQjacTFLM5fr1/uh5HoqfvFNO+lSehGRitKl9CIHi7zK9UBWslnP3erxVHnvowpcRKSiVIGLHCw6vXLV2HbTVIGLiFSUKnAR6QzN/nixKnVV4CIiVaUKXESqQRX3KKrARUQqSh24iEhFqQMXEakodeAiIhWlDlxEpKIa+kWeth3MbBjYBYwUdtDG9aC4mtWpsSmu5nRqXNC5sRUd17HufkS6sdAOHMDMnsz6aaCyKa7mdWpsiqs5nRoXdG5snRKXhlBERCpKHbiISEWV0YGvKeGYjVBczevU2BRXczo1Lujc2DoirsLHwEVEpD00hCIiUlHqwEVEKqqwDtzMLjCzATMbNLOVRR03I45jzOxRM3vOzLab2bWhfaaZPWRmO8JyRknxdZnZX81sY1ifZ2abQ1zrzGxKSXF1m9l6M3s+5G5RJ+TMzFaE13Gbmd1lZoeUlTMzu93MdprZtkRbZo4s8uNwPvSb2akFx/W98Fr2m9nvzaw7sW1ViGvAzM4vMq7Etm+amZtZT1gvLF9jxWZm14S8bDezmxPtheRsFHc/4DegC3gROA6YAjwNnFTEsTNimQ2cGu5/CHgBOAm4GVgZ2lcCN5UU3zeA3wAbw/o9wNJw/zbg6pLiuhP4Srg/BeguO2fA0cBLwKGJXF1ZVs6As4BTgW2JtswcAUuAPwAG9AGbC47rc8CkcP+mRFwnhfNzKjAvnLddRcUV2o8BHgReAXqKztcYOfss8DAwNazPKjpno+Is5CCwCHgwsb4KWFXEsRuI7X5gMTAAzA5ts4GBEmKZAzwCnANsDH+sI4kTrSaPBcZ1WOgoLdVeas5CB/4qMJPou+03AueXmTNgbuqkz8wR8FPg8qz9iogrte3zwNpwv+bcDB3poiLjAtYDnwBeTnTgheYr57W8BzgvY79Cc5a8FTWEEp9osaHQViozmwucAmwGjnT3NwDCclYJId0CfBt4P6x/GHjH3feG9bLydhwwDPwiDO/8zMymU3LO3P014PvAP4A3gHeBrXRGzmJ5Oeqkc+LLRNUtlByXmV0MvObuT6c2dUK+TgA+E4bnHjOz08uOragO3DLaSp2/aGYfBH4HfN3d/11mLCGei4Cd7r412Zyxaxl5m0T0dvJWdz+F6PtsSvscIxbGky8hett6FDAduDBj106cK9sRr62ZrQb2AmvjpozdConLzKYBq4HrszZntBWdr0nADKIhnG8B95iZUWJsRXXgQ0TjWrE5wOsFHXsUM5tM1Hmvdfd7Q/NbZjY7bJ8N7Cw4rE8DF5vZy8DdRMMotwDdZhb/9F1ZeRsChtx9c1hfT9Shl52z84CX3H3Y3fcA9wKfojNyFsvLUennhJktAy4CrvDw3r/kuI4n+s/46XAezAGeMrOPlBxXbAi41yNPEL1T7ikztqI68C3AgjA7YAqwFNhQ0LFrhP8xfw485+4/SGzaACwL95cRjY0Xxt1Xufscd59LlJ8/ufsVwKPAF8qKK8T2JvCqmZ0Yms4FnqXknBENnfSZ2bTwusZxlZ6zhLwcbQC+FGZX9AHvxkMtRTCzC4DrgIvdfXcq3qVmNtXM5gELgCeKiMndn3H3We4+N5wHQ0QTDt6k5HwF9xEVVpjZCUQf5o9QYs4O+CB7YmB/CdGMjxeB1UUdNyOOM4ne3vQDfwu3JUTjzY8AO8JyZokxns3+WSjHhT+GQeC3hE/AS4jpZODJkLf7iN5Klp4z4DvA88A24FdEMwFKyRlwF9FY/B6izueqvBwRve3+STgfngEWFhzXING4bXwO3JbYf3WIawC4sMi4UttfZv+HmIXla4ycTQF+Hf7WngLOKTpn6ZsupRcRqShdiSkiUlHqwEVEKkoduIhIRakDFxGpKHXgIiIVpQ5cRKSi1IGLiFTU/wEkeqqP+UXPyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166.73997562 273.6496088  -28.85614204]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181.79951424 155.86824087   1.53249276]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179.97673628 298.39953708  14.31007481]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165.82072958 175.23075543 -47.84857941]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABICAYAAADvR65LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMEUlEQVR4nO3de4xcZRnH8e/DbHdLL7RbSmFL0XaRizUEuQkIGK5CCQGJYlpJLBFDYoLxEpUSEhMT/wBUQgxGaBQERS4ityCIXMUQU+6FFii9UGChtFTWBcqltH38433P9szZme7szuw5c+T3STZnznvOnPPMsz3vPvPOe6bm7oiISPnsVHQAIiIyOurARURKSh24iEhJqQMXESkpdeAiIiWlDlxEpKSa6sDN7BQzW2Fmq8xsUauCEhGR4dlo54GbWQV4CTgJ6AMeBxa4+/OtC09EROpppgL/ArDK3de4+2bgRuCM1oQlIiLD6WjiuXsCr6XW+4DDszuZ2XnAeQAVKodMYJcmTinSfqxSqVr3rVuHbK/VVus51tVZ8xz+0eawnDwBgK3jjcqH4d2zbd0GwJaJ4Zgdm8KxtnWG9Z02V69XPvi46piDsXSkYtoSnuNd46rOMeQ5dV6ztNa79G90992y7c104Fajbch4jLsvBhYD7GLT/HA7oYlTirShbdWrlWndAGzt79++PV4tle7uqn2tewoAW9asDesHfA4Af3I5AB29swH4cPauAIwb+DAcp38TG+f1AND5XgjgzbM+AqBraejkP9gjtM98JFyWE199D4BNn5pUtd4/NxRV0x9dNxhLEmfyGpI4kjiT1zykXcbE/X7LK7XamxlC6QP2Sq3PAt5o4ngiIjICzVTgjwP7mNkc4HVgPvCNlkQlUiLZajVZZtvTBtvi0g6prrwHn9s9EYDxa/8DbK/EkyoaoH/f6uGMce+G5VcWPArA31YeA8CE8wfC/v+YEvcMx0gqeO8P2zt6Zw+eJzlv9rUmkudk1Xvt6efXyouMzKg7cHffYmbnA/cCFeBqd1/esshERGSHmqnAcfe7gbtbFItIqQw3/putxGu1Jes7rQmjj8lHgdt6Z1a1E8fK1x/WFVZf2jpYOW8ifli5emcAZnz1VQB+PuM5AG46+hAA3nl2FgA7x3PssnR9dbypc46PbdnXVq+yTnKRaPR50hzdiSkiUlJNVeAin2T1Ku/sODEMrTyT2SeVzH6Dz11TPR8gGZOe+EaYUdI3bxuz7qmuvzbvvgWAr898AoB9Hj4HgKN7V4cdesPimesOqDrm2/uHqn7m38M5naFj20mFnYzJd/RPqdqe3b9WDrLbVI03TxW4iEhJqQIXaVJSUWbndGe3px8nFeuQyjzORknGvpOx8K6n18Q9Qhm9+Zku+uaFG3L2uyrM5+b+sFj8rzMBmHzW21XHPnrKSgBWrZsLQN9x4aahObfHWGJ1vVP/wJCx+iReT15bnfH/RqrrJE+oAm+aKnARkZJSBS7SpOyc7lqyM1aSSjVbsSbzwC0zq6OW3hvDLJS+E0NF+9GB7wMweVKovAdWh2OvumYqAA/P2x8Id9wB7H3ZiwC8et5ngTCzBWByUuyzvVpOqvNK3JZ9Hdk58DuiuzZbRxW4iEhJqQIXGWPWPWWw6mx0vnR2zPndM8P3xHUOhJkmPTe8MDg+Pu7dMItk1q/CmPj6w8Psks/E+eCrYs099+K3ANh4VM9gXACfWvwCsH28fVvvzMFKOxtH9iur6t1puaO7UfX9Ka2jClxEpKRUgYuMsXSlWa/qTKrS7OyUbMWeVunfBMCkdZOq1qe9GO6jfP+KPQGYGb8kNDn2pNdDhZ6edQLV38OSVOfJcxqprGuptb3eMWXkhq3AzexqM9tgZstSbdPM7D4zWxmX9Wfti4jImGikAv8DcAVwXaptEfCAu18c/y/MRcAFrQ9P5P9L9g7Fwdknw9zJmHx3d1Jlp++W7BwIFXVyZ+V7e4b53cl3pUy4bQmwfWZL8g2DSQWerfa9f6DuLJPh1hupplVxt86wHbi7P2JmszPNZwDHxsfXAg+jDlw+oRr5UG64r1et26nVGUrZ2t8/+NyOB5+s2taduRko+eBxcMgktie38W+Jy/QfkewfkHpxqjMu1mg/xNzd3dcBxOWM1oUkIiKNGPMPMdP/J+Z4Joz16URyV2+4IVHp7q5bqQ5XwSbHrHcDUK19s1+ENfgOIX5ISZ0vmkoPizRaWeuDyGKNtgJfb2Y9AHG5od6O7r7Y3Q9190PH0TXK04mISNZoK/A7gYXAxXF5R8siEimp0VbZIznmjo413Dj6kP0aiHe4CluVd7EamUZ4A/BvYD8z6zOzcwkd90lmthI4Ka6LiEiOGpmFsqDOphNaHItIKdWbWpfW6Fh3vf2KGmtWhd3edCu9iEhJ6VZ6kSaN9PbyHR1jtNvbXTo3mrnSOqrARURKShW4SIuooqxvNHPMZXiqwEVESkoduIiMWqW7u+asm3pUfbeWOnARkZLSGLiIjJoq6mKpAhcRKSl14CIiJaUOXESkpNSBi4yxkczSEBkJdeAiIiVl7p7fyczeAjYBG3M7aeOmo7hGql1jU1wj065xQfvGlndcn3b33bKNuXbgAGb2hLsfmutJG6C4Rq5dY1NcI9OucUH7xtYucWkIRUSkpNSBi4iUVBEd+OICztkIxTVy7Rqb4hqZdo0L2je2togr9zFwERFpDQ2hiIiUlDpwEZGSyq0DN7NTzGyFma0ys0V5nbdGHHuZ2UNm9oKZLTez78X2aWZ2n5mtjMtCbp8zs4qZPW1md8X1OWa2JMZ1k5l1FhTXVDO7xcxejLk7sh1yZmY/iL/HZWZ2g5mNLypnZna1mW0ws2Wptpo5suDX8Xp41swOzjmuX8Tf5bNmdpuZTU1tuzDGtcLMTs4zrtS2H5mZm9n0uJ5bvnYUm5l9N+ZluZldmmrPJWdDuPuY/wAVYDXQC3QCS4G5eZy7Riw9wMHx8WTgJWAucCmwKLYvAi4pKL4fAn8G7orrNwPz4+Mrge8UFNe1wLfj405gatE5A/YEXgZ2TuXqnKJyBnwJOBhYlmqrmSPgVOAewIAjgCU5x/VloCM+viQV19x4fXYBc+J1W8krrti+F3Av8AowPe987SBnxwH3A11xfUbeORsSZy4ngSOBe1PrFwIX5nHuBmK7AzgJWAH0xLYeYEUBscwCHgCOB+6K/1g3pi60qjzmGNcusaO0THuhOYsd+GvANMJ3298FnFxkzoDZmYu+Zo6Aq4AFtfbLI67MtjOB6+PjqmszdqRH5hkXcAtwILA21YHnmq86v8ubgRNr7JdrztI/eQ2hJBdaoi+2FcrMZgMHAUuA3d19HUBcziggpMuBnwDb4vquwH/dfUtcLypvvcBbwDVxeOd3ZjaRgnPm7q8DvwReBdYBA8CTtEfOEvVy1E7XxLcI1S0UHJeZnQ687u5LM5vaIV/7AsfE4bl/mtlhRceWVwduNdoKnb9oZpOAvwLfd/d3iowlxnMasMHdn0w319i1iLx1EN5O/tbdDyJ8n01hn2Mk4njyGYS3rTOBicC8Gru241zZtvjdmtlFwBbg+qSpxm65xGVmE4CLgJ/W2lyjLe98dQDdhCGcHwM3m5lRYGx5deB9hHGtxCzgjZzOPYSZjSN03te7+62xeb2Z9cTtPcCGnMM6CjjdzNYCNxKGUS4HpppZ8l/fFZW3PqDP3ZfE9VsIHXrROTsReNnd33L3j4FbgS/SHjlL1MtR4deEmS0ETgPO9vjev+C49ib8MV4ar4NZwFNmtkfBcSX6gFs9eIzwTnl6kbHl1YE/DuwTZwd0AvOBO3M6d5X4F/P3wAvufllq053Awvh4IWFsPDfufqG7z3L32YT8POjuZwMPAV8rKq4Y25vAa2a2X2w6AXiegnNGGDo5wswmxN9rElfhOUupl6M7gW/G2RVHAAPJUEsezOwU4ALgdHd/PxPvfDPrMrM5wD7AY3nE5O7PufsMd58dr4M+woSDNyk4X9HthMIKM9uX8GH+RgrM2ZgPsqcG9k8lzPhYDVyU13lrxHE04e3Ns8Az8edUwnjzA8DKuJxWYIzHsn0WSm/8x7AK+AvxE/ACYvo88ETM2+2Et5KF5wz4GfAisAz4I2EmQCE5A24gjMV/TOh8zq2XI8Lb7t/E6+E54NCc41pFGLdNroErU/tfFONaAczLM67M9rVs/xAzt3ztIGedwJ/iv7WngOPzzln2R7fSi4iUlO7EFBEpKXXgIiIlpQ5cRKSk1IGLiJSUOnARkZJSBy4iUlLqwEVESup/IGGMCnAviAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167.86591545 234.0620098  -39.39222336]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174.36473219 143.47169795 -48.28456116]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193.87985536  15.81019628  41.63842773]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    idea=np.random.randint(1,conjunto_datos_entradasB.shape[0])\n",
    "    plt.imshow(conjunto_datos_entradasB[idea], cmap='viridis')\n",
    "    plt.show()\n",
    "    print(conjunto_datos_salidas[idea,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 250\n",
    "nb_classes = 10\n",
    "nb_epoch = 2000\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 20, 41\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (1,2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (2, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "muestras=conjunto_datos_entradasA.shape[0]\n",
    "# veamos=idea.reshape(idea.shape[0],175, 20)\n",
    "\n",
    "\n",
    "veamos2A=np.zeros([muestras,20,175])\n",
    "veamos2_3A=np.zeros([muestras,20,525])\n",
    "veamos2B=np.zeros([muestras,20,175])\n",
    "veamos2_3B=np.zeros([muestras,20,525])\n",
    "sector2A=np.zeros([muestras,20,img_cols])\n",
    "sector2B=np.zeros([muestras,20,img_cols])\n",
    "veamos3=np.zeros([muestras,175])\n",
    "# for i in range(idea.shape[0]):\n",
    "for i in range(muestras):\n",
    "    veamos2A[i]=conjunto_datos_entradasA[i]\n",
    "    veamos2B[i]=conjunto_datos_entradasB[i]\n",
    "    veamos3[i]=np.sum(veamos2A[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    veamos2_3A[i]=np.concatenate((veamos2A[i],veamos2A[i],veamos2A[i]),axis=1) \n",
    "    veamos2_3B[i]=np.concatenate((veamos2B[i],veamos2B[i],veamos2B[i]),axis=1) \n",
    "    sector2A[i]=veamos2_3A[i,:,indice_inferior:indice_superior]\n",
    "    sector2B[i]=veamos2_3B[i,:,indice_inferior:indice_superior]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation mediante flip horizontal y vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atencion: la z es copiada pero en realidad es incorrecta. Podemos asegurar que el radio y phi si que son las mismas; pero la z claramente al hacer un flip vertical no puede ser la misma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamosA=np.zeros([3*muestras,20,img_cols])\n",
    "veamosB=np.zeros([3*muestras,20,img_cols])\n",
    "conjunto_datos_salidas_nuevo=np.zeros([3*muestras,3])\n",
    "for i in range(muestras):\n",
    "    veamosA[i*3]=sector2A[i]   \n",
    "    veamosA[i*3+1]=np.flipud(sector2A[i])   \n",
    "    veamosA[i*3+2]=np.fliplr(sector2A[i])     \n",
    "    veamosB[i*3]=sector2B[i]   \n",
    "    veamosB[i*3+1]=np.flipud(sector2B[i])   \n",
    "    veamosB[i*3+2]=np.fliplr(sector2B[i])   \n",
    "    conjunto_datos_salidas_nuevo[i*3]=conjunto_datos_salidas[i]\n",
    "    conjunto_datos_salidas_nuevo[i*3+1]=conjunto_datos_salidas[i]    \n",
    "    conjunto_datos_salidas_nuevo[i*3+2]=conjunto_datos_salidas[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector A shape: (117036, 20, 41)\n",
      "conjunto_datos_nuevo A: (117036, 820)\n",
      "sector B shape: (117036, 20, 41)\n",
      "conjunto_datos_nuevo B: (117036, 820)\n",
      "conjunto_datos_salidas_nuevo: (117036, 3)\n"
     ]
    }
   ],
   "source": [
    "print('sector A shape:', veamosA.shape)\n",
    "conjunto_datos_nuevoA=veamosA.reshape(veamosA.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo A:', conjunto_datos_nuevoA.shape)\n",
    "\n",
    "print('sector B shape:', veamosB.shape)\n",
    "conjunto_datos_nuevoB=veamosB.reshape(veamosB.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo B:', conjunto_datos_nuevoB.shape)\n",
    "print('conjunto_datos_salidas_nuevo:', conjunto_datos_salidas_nuevo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5  # how many digits we will display\n",
    "\n",
    "fig = plt.figure(figsize=(8,20))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ideas=np.random.randint(1,muestras)\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+1)\n",
    "    plt.imshow(sector2A[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+2)\n",
    "    plt.imshow(sector2B[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,sector2B.shape[0])\n",
    "    plt.imshow(sector2B[idea], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70221\n",
      "23407\n",
      "23408\n",
      "(8211, 1640)\n",
      "(4509, 1640)\n",
      "(3807, 1640)\n",
      "(3236, 1640)\n",
      "(3645, 1640)\n"
     ]
    }
   ],
   "source": [
    "numero_muestras=3*muestras\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "conjunto_datos_nuevo2=np.concatenate((conjunto_datos_salidas_nuevo,conjunto_datos_nuevoB, conjunto_datos_nuevoA), axis=1)\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "XY_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "XY_test_bin0=XY_test[np.where((XY_test[:,0]>=164.9999) * (XY_test[:,0]<171.000))]\n",
    "XY_test_bin1=XY_test[np.where((XY_test[:,0]>=171.000) * (XY_test[:,0]<177.000))]\n",
    "XY_test_bin2=XY_test[np.where((XY_test[:,0]>=177.000) * (XY_test[:,0]<183.0000))]\n",
    "XY_test_bin3=XY_test[np.where((XY_test[:,0]>=183.000) * (XY_test[:,0]<189.0000))]\n",
    "XY_test_bin4=XY_test[np.where((XY_test[:,0]>=189.0000))]\n",
    "\n",
    "X_train=conjunto_datos_nuevo2[:tamanyo_tr,3:]\n",
    "X_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,3:]\n",
    "X_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,3:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_bin0=XY_test_bin0[:,3:]\n",
    "Y_test_bin0=XY_test_bin0[:,0]\n",
    "print(X_test_bin0.shape)\n",
    "X_test_bin1=XY_test_bin1[:,3:]\n",
    "Y_test_bin1=XY_test_bin1[:,0]\n",
    "print(X_test_bin1.shape)\n",
    "X_test_bin2=XY_test_bin2[:,3:]\n",
    "Y_test_bin2=XY_test_bin2[:,0]\n",
    "print(X_test_bin2.shape)\n",
    "X_test_bin3=XY_test_bin3[:,3:]\n",
    "Y_test_bin3=XY_test_bin3[:,0]\n",
    "print(X_test_bin3.shape)\n",
    "X_test_bin4=XY_test_bin4[:,3:]\n",
    "Y_test_bin4=XY_test_bin4[:,0]\n",
    "print(X_test_bin4.shape)\n",
    "\n",
    "Y_train=conjunto_datos_nuevo2[:tamanyo_tr,0] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,0] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,0] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a normalizar las salidas por si luego nos interea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_total=conjunto_datos_nuevo2[:numero_muestras,0]\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(Y_total.reshape(-1, 1))\n",
    "\n",
    "X_total=conjunto_datos_nuevo2[:numero_muestras,3:]\n",
    "min_max_scalerx = preprocessing.MinMaxScaler().fit(X_total)\n",
    "\n",
    "X_train = min_max_scalerx.transform(X_train)\n",
    "X_val = min_max_scalerx.transform(X_val)\n",
    "X_test = min_max_scalerx.transform(X_test)\n",
    "\n",
    "Y_train_scaled = min_max_scaler.transform(Y_train.reshape(-1, 1))\n",
    "Y_val_scaled = min_max_scaler.transform(Y_val.reshape(-1, 1))\n",
    "Y_test_scaled = min_max_scaler.transform(Y_test.reshape(-1, 1))\n",
    "\n",
    "Y_test_bin4_scaled=min_max_scaler.transform(Y_test_bin4.reshape(-1, 1))\n",
    "Y_test_bin3_scaled=min_max_scaler.transform(Y_test_bin3.reshape(-1, 1))\n",
    "Y_test_bin2_scaled=min_max_scaler.transform(Y_test_bin2.reshape(-1, 1))\n",
    "Y_test_bin1_scaled=min_max_scaler.transform(Y_test_bin1.reshape(-1, 1))\n",
    "Y_test_bin0_scaled=min_max_scaler.transform(Y_test_bin0.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],2, img_rows, img_cols,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 2,img_rows, img_cols,1)\n",
    "\n",
    "X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], 2, img_rows, img_cols,1)\n",
    "\n",
    "input_shape = (2, img_rows, img_cols,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (70221, 2, 20, 41, 1)\n",
      "70221 train samples\n",
      "23407 validation samples\n",
      "23408 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                            vertical_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=kernel_size,\n",
    "                        padding='same',\n",
    "                        data_format='channels_last',\n",
    "                        input_shape=(2,img_rows,img_cols,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "experimento=\"CNN_kernel_{}x{}x{}_con_batchnormalization_sector_{}x{}x{}_elu\".format(kernel_size[0],kernel_size[1],kernel_size[2],img_rows,img_cols,1)\n",
    "algoritmo='RMSprop'\n",
    "optimizador=Nadam(beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"../logs/defs/{}{}{}\".format(experimento,algoritmo,dt))\n",
    "best_model_name='../redes_CNN_R/models_best/CNN_regression_R_{}_{}_{}_{}_{}.h5'.format(nb_epoch,batch_size,experimento,algoritmo,dt)\n",
    "model_check=ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=600, verbose=1, mode='auto', baseline=None)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizador)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 2, 20, 41, 16)     528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 20, 41, 16)     64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 20, 41, 16)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 10, 20, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 2, 10, 20, 16)     8208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 10, 20, 16)     64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 10, 20, 16)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 5, 10, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 5, 10, 32)      16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 5, 10, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 5, 10, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 5, 10, 32)      32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 5, 10, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 5, 10, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 2, 5, 10, 64)      65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 5, 10, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 5, 10, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 2, 5, 10, 128)     262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 5, 10, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 12801     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 399,777\n",
      "Trainable params: 399,201\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 70221 samples, validate on 23407 samples\n",
      "Epoch 1/2000\n",
      "70221/70221 [==============================] - 13s 181us/step - loss: 0.4510 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44554, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_RMSprop_2019-12-24 12:22:00.h5\n",
      "Epoch 2/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44554\n",
      "Epoch 3/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44554\n",
      "Epoch 4/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44554\n",
      "Epoch 5/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44554\n",
      "Epoch 6/2000\n",
      "70221/70221 [==============================] - 10s 147us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.44554\n",
      "Epoch 7/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.44554\n",
      "Epoch 8/2000\n",
      "70221/70221 [==============================] - 10s 147us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44554\n",
      "Epoch 9/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.44554\n",
      "Epoch 10/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44554\n",
      "Epoch 11/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44554\n",
      "Epoch 12/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44554\n",
      "Epoch 13/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44554\n",
      "Epoch 14/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44554\n",
      "Epoch 15/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44554\n",
      "Epoch 16/2000\n",
      "70221/70221 [==============================] - 10s 144us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44554\n",
      "Epoch 17/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44554\n",
      "Epoch 18/2000\n",
      "70221/70221 [==============================] - 10s 148us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44554\n",
      "Epoch 19/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44554\n",
      "Epoch 20/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.44554\n",
      "Epoch 21/2000\n",
      "70221/70221 [==============================] - 10s 147us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44554\n",
      "Epoch 22/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.44554\n",
      "Epoch 23/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.44554\n",
      "Epoch 24/2000\n",
      "70221/70221 [==============================] - 10s 146us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.44554\n",
      "Epoch 25/2000\n",
      "70221/70221 [==============================] - 10s 144us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.44554\n",
      "Epoch 26/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.44554\n",
      "Epoch 27/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.44554\n",
      "Epoch 28/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.44554\n",
      "Epoch 29/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44554\n",
      "Epoch 30/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44554\n",
      "Epoch 31/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.44554\n",
      "Epoch 32/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.44554\n",
      "Epoch 33/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44554\n",
      "Epoch 34/2000\n",
      "70221/70221 [==============================] - 10s 145us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44554\n",
      "Epoch 35/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44554\n",
      "Epoch 36/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.44554\n",
      "Epoch 37/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.44554\n",
      "Epoch 38/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.44554\n",
      "Epoch 39/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.44554\n",
      "Epoch 40/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.44554\n",
      "Epoch 41/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.44554\n",
      "Epoch 42/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.44554\n",
      "Epoch 43/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.44554\n",
      "Epoch 44/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.44554\n",
      "Epoch 45/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.44554\n",
      "Epoch 46/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.44554\n",
      "Epoch 47/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.44554\n",
      "Epoch 48/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.44554\n",
      "Epoch 49/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.44554\n",
      "Epoch 50/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.44554\n",
      "Epoch 51/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.44554\n",
      "Epoch 52/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.44554\n",
      "Epoch 53/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.44554\n",
      "Epoch 54/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.44554\n",
      "Epoch 55/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.44554\n",
      "Epoch 56/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.44554\n",
      "Epoch 57/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.44554\n",
      "Epoch 58/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.44554\n",
      "Epoch 59/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.44554\n",
      "Epoch 60/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.44554\n",
      "Epoch 61/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.44554\n",
      "Epoch 62/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.44554\n",
      "Epoch 63/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.44554\n",
      "Epoch 64/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.44554\n",
      "Epoch 65/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.44554\n",
      "Epoch 66/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.44554\n",
      "Epoch 67/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.44554\n",
      "Epoch 68/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.44554\n",
      "Epoch 69/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.44554\n",
      "Epoch 70/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.44554\n",
      "Epoch 71/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.44554\n",
      "Epoch 72/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.44554\n",
      "Epoch 73/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.44554\n",
      "Epoch 74/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.44554\n",
      "Epoch 75/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.44554\n",
      "Epoch 76/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.44554\n",
      "Epoch 77/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.44554\n",
      "Epoch 78/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.44554\n",
      "Epoch 79/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.44554\n",
      "Epoch 80/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.44554\n",
      "Epoch 81/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.44554\n",
      "Epoch 82/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.44554\n",
      "Epoch 83/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.44554\n",
      "Epoch 84/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.44554\n",
      "Epoch 85/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.44554\n",
      "Epoch 86/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.44554\n",
      "Epoch 87/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.44554\n",
      "Epoch 88/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.44554\n",
      "Epoch 89/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.44554\n",
      "Epoch 90/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.44554\n",
      "Epoch 91/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.44554\n",
      "Epoch 92/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.44554\n",
      "Epoch 93/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.44554\n",
      "Epoch 94/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.44554\n",
      "Epoch 95/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.44554\n",
      "Epoch 96/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.44554\n",
      "Epoch 97/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.44554\n",
      "Epoch 98/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.44554\n",
      "Epoch 99/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.44554\n",
      "Epoch 100/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.44554\n",
      "Epoch 101/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.44554\n",
      "Epoch 102/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.44554\n",
      "Epoch 103/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.44554\n",
      "Epoch 104/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.44554\n",
      "Epoch 105/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.44554\n",
      "Epoch 106/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.44554\n",
      "Epoch 107/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.44554\n",
      "Epoch 108/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.44554\n",
      "Epoch 109/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.44554\n",
      "Epoch 110/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.44554\n",
      "Epoch 111/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.44554\n",
      "Epoch 112/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.44554\n",
      "Epoch 113/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.44554\n",
      "Epoch 114/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.44554\n",
      "Epoch 115/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.44554\n",
      "Epoch 116/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.44554\n",
      "Epoch 117/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.44554\n",
      "Epoch 118/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.44554\n",
      "Epoch 119/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.44554\n",
      "Epoch 120/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.44554\n",
      "Epoch 121/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.44554\n",
      "Epoch 122/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.44554\n",
      "Epoch 123/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.44554\n",
      "Epoch 124/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.44554\n",
      "Epoch 125/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.44554\n",
      "Epoch 126/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.44554\n",
      "Epoch 127/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.44554\n",
      "Epoch 128/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.44554\n",
      "Epoch 129/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.44554\n",
      "Epoch 130/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.44554\n",
      "Epoch 131/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.44554\n",
      "Epoch 132/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.44554\n",
      "Epoch 133/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.44554\n",
      "Epoch 134/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.44554\n",
      "Epoch 135/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.44554\n",
      "Epoch 136/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.44554\n",
      "Epoch 137/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.44554\n",
      "Epoch 138/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.44554\n",
      "Epoch 139/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.44554\n",
      "Epoch 140/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.44554\n",
      "Epoch 141/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.44554\n",
      "Epoch 142/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.44554\n",
      "Epoch 143/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.44554\n",
      "Epoch 144/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.44554\n",
      "Epoch 145/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.44554\n",
      "Epoch 146/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.44554\n",
      "Epoch 147/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.44554\n",
      "Epoch 148/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.44554\n",
      "Epoch 149/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.44554\n",
      "Epoch 150/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.44554\n",
      "Epoch 151/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.44554\n",
      "Epoch 152/2000\n",
      "70221/70221 [==============================] - 10s 144us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.44554\n",
      "Epoch 153/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.44554\n",
      "Epoch 154/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.44554\n",
      "Epoch 155/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.44554\n",
      "Epoch 156/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.44554\n",
      "Epoch 157/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.44554\n",
      "Epoch 158/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.44554\n",
      "Epoch 159/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.44554\n",
      "Epoch 160/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.44554\n",
      "Epoch 161/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.44554\n",
      "Epoch 162/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.44554\n",
      "Epoch 163/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.44554\n",
      "Epoch 164/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.44554\n",
      "Epoch 165/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.44554\n",
      "Epoch 166/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.44554\n",
      "Epoch 167/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.44554\n",
      "Epoch 168/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.44554\n",
      "Epoch 169/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.44554\n",
      "Epoch 170/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.44554\n",
      "Epoch 171/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.44554\n",
      "Epoch 172/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.44554\n",
      "Epoch 173/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.44554\n",
      "Epoch 174/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.44554\n",
      "Epoch 175/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.44554\n",
      "Epoch 176/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.44554\n",
      "Epoch 177/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.44554\n",
      "Epoch 178/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.44554\n",
      "Epoch 179/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.44554\n",
      "Epoch 180/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.44554\n",
      "Epoch 181/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.44554\n",
      "Epoch 182/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.44554\n",
      "Epoch 183/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.44554\n",
      "Epoch 184/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.44554\n",
      "Epoch 185/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.44554\n",
      "Epoch 186/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.44554\n",
      "Epoch 187/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.44554\n",
      "Epoch 188/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.44554\n",
      "Epoch 189/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.44554\n",
      "Epoch 190/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.44554\n",
      "Epoch 191/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.44554\n",
      "Epoch 192/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.44554\n",
      "Epoch 193/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.44554\n",
      "Epoch 194/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.44554\n",
      "Epoch 195/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.44554\n",
      "Epoch 196/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.44554\n",
      "Epoch 197/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.44554\n",
      "Epoch 198/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.44554\n",
      "Epoch 199/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.44554\n",
      "Epoch 200/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.44554\n",
      "Epoch 201/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.44554\n",
      "Epoch 202/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.44554\n",
      "Epoch 203/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.44554\n",
      "Epoch 204/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.44554\n",
      "Epoch 205/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.44554\n",
      "Epoch 206/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.44554\n",
      "Epoch 207/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.44554\n",
      "Epoch 208/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.44554\n",
      "Epoch 209/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.44554\n",
      "Epoch 210/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.44554\n",
      "Epoch 211/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.44554\n",
      "Epoch 212/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.44554\n",
      "Epoch 213/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.44554\n",
      "Epoch 214/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.44554\n",
      "Epoch 215/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.44554\n",
      "Epoch 216/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.44554\n",
      "Epoch 217/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.44554\n",
      "Epoch 218/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.44554\n",
      "Epoch 219/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.44554\n",
      "Epoch 220/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.44554\n",
      "Epoch 221/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.44554\n",
      "Epoch 222/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.44554\n",
      "Epoch 223/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.44554\n",
      "Epoch 224/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.44554\n",
      "Epoch 225/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.44554\n",
      "Epoch 226/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.44554\n",
      "Epoch 227/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.44554\n",
      "Epoch 228/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.44554\n",
      "Epoch 229/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.44554\n",
      "Epoch 230/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.44554\n",
      "Epoch 231/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.44554\n",
      "Epoch 232/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.44554\n",
      "Epoch 233/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.44554\n",
      "Epoch 234/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.44554\n",
      "Epoch 235/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.44554\n",
      "Epoch 236/2000\n",
      "70221/70221 [==============================] - 10s 144us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.44554\n",
      "Epoch 237/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.44554\n",
      "Epoch 238/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.44554\n",
      "Epoch 239/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.44554\n",
      "Epoch 240/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.44554\n",
      "Epoch 241/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.44554\n",
      "Epoch 242/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.44554\n",
      "Epoch 243/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.44554\n",
      "Epoch 244/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.44554\n",
      "Epoch 245/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.44554\n",
      "Epoch 246/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.44554\n",
      "Epoch 247/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.44554\n",
      "Epoch 248/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.44554\n",
      "Epoch 249/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.44554\n",
      "Epoch 250/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.44554\n",
      "Epoch 251/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.44554\n",
      "Epoch 252/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.44554\n",
      "Epoch 253/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.44554\n",
      "Epoch 254/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.44554\n",
      "Epoch 255/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.44554\n",
      "Epoch 256/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.44554\n",
      "Epoch 257/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.44554\n",
      "Epoch 258/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.44554\n",
      "Epoch 259/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.44554\n",
      "Epoch 260/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.44554\n",
      "Epoch 261/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.44554\n",
      "Epoch 262/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.44554\n",
      "Epoch 263/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.44554\n",
      "Epoch 264/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.44554\n",
      "Epoch 265/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.44554\n",
      "Epoch 266/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.44554\n",
      "Epoch 267/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.44554\n",
      "Epoch 268/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.44554\n",
      "Epoch 269/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.44554\n",
      "Epoch 270/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.44554\n",
      "Epoch 271/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.44554\n",
      "Epoch 272/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.44554\n",
      "Epoch 273/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.44554\n",
      "Epoch 274/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.44554\n",
      "Epoch 275/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.44554\n",
      "Epoch 276/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.44554\n",
      "Epoch 277/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.44554\n",
      "Epoch 278/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.44554\n",
      "Epoch 279/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.44554\n",
      "Epoch 280/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.44554\n",
      "Epoch 281/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.44554\n",
      "Epoch 282/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.44554\n",
      "Epoch 283/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.44554\n",
      "Epoch 284/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.44554\n",
      "Epoch 285/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.44554\n",
      "Epoch 286/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.44554\n",
      "Epoch 287/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.44554\n",
      "Epoch 288/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.44554\n",
      "Epoch 289/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.44554\n",
      "Epoch 290/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.44554\n",
      "Epoch 291/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.44554\n",
      "Epoch 292/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.44554\n",
      "Epoch 293/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.44554\n",
      "Epoch 294/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.44554\n",
      "Epoch 295/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.44554\n",
      "Epoch 296/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.44554\n",
      "Epoch 297/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.44554\n",
      "Epoch 298/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.44554\n",
      "Epoch 299/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.44554\n",
      "Epoch 300/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.44554\n",
      "Epoch 301/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.44554\n",
      "Epoch 302/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.44554\n",
      "Epoch 303/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.44554\n",
      "Epoch 304/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.44554\n",
      "Epoch 305/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.44554\n",
      "Epoch 306/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.44554\n",
      "Epoch 307/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.44554\n",
      "Epoch 308/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.44554\n",
      "Epoch 309/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.44554\n",
      "Epoch 310/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.44554\n",
      "Epoch 311/2000\n",
      "70221/70221 [==============================] - 10s 143us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.44554\n",
      "Epoch 312/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.44554\n",
      "Epoch 313/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.44554\n",
      "Epoch 314/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.44554\n",
      "Epoch 315/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.44554\n",
      "Epoch 316/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.44554\n",
      "Epoch 317/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.44554\n",
      "Epoch 318/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.44554\n",
      "Epoch 319/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.44554\n",
      "Epoch 320/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.44554\n",
      "Epoch 321/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.44554\n",
      "Epoch 322/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.44554\n",
      "Epoch 323/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.44554\n",
      "Epoch 324/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.44554\n",
      "Epoch 325/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.44554\n",
      "Epoch 326/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.44554\n",
      "Epoch 327/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.44554\n",
      "Epoch 328/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.44554\n",
      "Epoch 329/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.44554\n",
      "Epoch 330/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.44554\n",
      "Epoch 331/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.44554\n",
      "Epoch 332/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.44554\n",
      "Epoch 333/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.44554\n",
      "Epoch 334/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.44554\n",
      "Epoch 335/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.44554\n",
      "Epoch 336/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.44554\n",
      "Epoch 337/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.44554\n",
      "Epoch 338/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.44554\n",
      "Epoch 339/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.44554\n",
      "Epoch 340/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.44554\n",
      "Epoch 341/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.44554\n",
      "Epoch 342/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.44554\n",
      "Epoch 343/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.44554\n",
      "Epoch 344/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.44554\n",
      "Epoch 345/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.44554\n",
      "Epoch 346/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.44554\n",
      "Epoch 347/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.44554\n",
      "Epoch 348/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.44554\n",
      "Epoch 349/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.44554\n",
      "Epoch 350/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.44554\n",
      "Epoch 351/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.44554\n",
      "Epoch 352/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.44554\n",
      "Epoch 353/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.44554\n",
      "Epoch 354/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.44554\n",
      "Epoch 355/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.44554\n",
      "Epoch 356/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.44554\n",
      "Epoch 357/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.44554\n",
      "Epoch 358/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.44554\n",
      "Epoch 359/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.44554\n",
      "Epoch 360/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.44554\n",
      "Epoch 361/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.44554\n",
      "Epoch 362/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.44554\n",
      "Epoch 363/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.44554\n",
      "Epoch 364/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.44554\n",
      "Epoch 365/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.44554\n",
      "Epoch 366/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.44554\n",
      "Epoch 367/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.44554\n",
      "Epoch 368/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.44554\n",
      "Epoch 369/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.44554\n",
      "Epoch 370/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.44554\n",
      "Epoch 371/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.44554\n",
      "Epoch 372/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.44554\n",
      "Epoch 373/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.44554\n",
      "Epoch 374/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.44554\n",
      "Epoch 375/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.44554\n",
      "Epoch 376/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.44554\n",
      "Epoch 377/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.44554\n",
      "Epoch 378/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.44554\n",
      "Epoch 379/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.44554\n",
      "Epoch 380/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.44554\n",
      "Epoch 381/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.44554\n",
      "Epoch 382/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.44554\n",
      "Epoch 383/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.44554\n",
      "Epoch 384/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.44554\n",
      "Epoch 385/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.44554\n",
      "Epoch 386/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.44554\n",
      "Epoch 387/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.44554\n",
      "Epoch 388/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.44554\n",
      "Epoch 389/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.44554\n",
      "Epoch 390/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.44554\n",
      "Epoch 391/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.44554\n",
      "Epoch 392/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.44554\n",
      "Epoch 393/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.44554\n",
      "Epoch 394/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.44554\n",
      "Epoch 395/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.44554\n",
      "Epoch 396/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.44554\n",
      "Epoch 397/2000\n",
      "70221/70221 [==============================] - 10s 141us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.44554\n",
      "Epoch 398/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.44554\n",
      "Epoch 399/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.44554\n",
      "Epoch 400/2000\n",
      "70221/70221 [==============================] - 10s 137us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.44554\n",
      "Epoch 401/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.44554\n",
      "Epoch 402/2000\n",
      "70221/70221 [==============================] - 10s 139us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.44554\n",
      "Epoch 403/2000\n",
      "70221/70221 [==============================] - 10s 140us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.44554\n",
      "Epoch 404/2000\n",
      "70221/70221 [==============================] - 10s 138us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.44554\n",
      "Epoch 405/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.44554\n",
      "Epoch 406/2000\n",
      "70221/70221 [==============================] - 10s 142us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.44554\n",
      "Epoch 407/2000\n",
      "70221/70221 [==============================] - 10s 136us/step - loss: 0.4508 - val_loss: 0.4455\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.44554\n",
      "Epoch 408/2000\n",
      "41750/70221 [================>.............] - ETA: 3s - loss: 0.4508"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-10494f74ae39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, Y_train_scaled, batch_size=batch_size, epochs=nb_epoch,\n\u001b[1;32m      2\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                      callbacks=[tensorboard,model_check,early_stop])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train_scaled, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val_scaled),\n",
    "                     callbacks=[tensorboard,model_check,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "model.save_weights('../redes_CNN_R/defs/CNN_regression_R_{}_{}_{}_{}_{}'.format(nb_epoch,batch_size,experimento,algoritmo,dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(best_model_name)\n",
    "score = best_model.evaluate(X_test, Y_test_scaled, verbose=0)\n",
    "print('Test mse:', score)\n",
    "# print('Test mae:', score[1])\n",
    "Y_test_predicted_scaled=model.predict(X_test)\n",
    "print(Y_test_predicted_scaled[:10].flatten())\n",
    "print(Y_test_scaled[:10])\n",
    "Y_test_predicted = min_max_scaler.inverse_transform(Y_test_predicted_scaled)\n",
    "error_prediction=Y_test-Y_test_predicted.flatten()\n",
    "\n",
    "print(error_prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(error_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(error_prediction, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWHM=result.params['wid'].value*2*sqrt(log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FWHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin0_predicted_scaled=model.predict(X_test_bin0)\n",
    "print(Y_test_bin0_predicted_scaled)\n",
    "Y_test_bin0_predicted = min_max_scaler.inverse_transform(Y_test_bin0_predicted_scaled)\n",
    "error_prediction_bin0=Y_test_bin0-Y_test_bin0_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin0, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin0=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin1_predicted_scaled=model.predict(X_test_bin1)\n",
    "#print(Y_test_bin1_predicted)\n",
    "Y_test_bin1_predicted = min_max_scaler.inverse_transform(Y_test_bin1_predicted_scaled)\n",
    "error_prediction_bin1=Y_test_bin1-Y_test_bin1_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin1, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin1=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin2_predicted_scaled=model.predict(X_test_bin2)\n",
    "#print(Y_test_bin2_predicted)\n",
    "Y_test_bin2_predicted = min_max_scaler.inverse_transform(Y_test_bin2_predicted_scaled)\n",
    "error_prediction_bin2=Y_test_bin2-Y_test_bin2_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin2, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin2=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin3_predicted_scaled=model.predict(X_test_bin3)\n",
    "#print(Y_test_bin3_predicted)\n",
    "Y_test_bin3_predicted = min_max_scaler.inverse_transform(Y_test_bin3_predicted_scaled)\n",
    "error_prediction_bin3=Y_test_bin3-Y_test_bin3_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin3, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin3=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test_bin4_predicted_scaled=model.predict(X_test_bin4)\n",
    "#print(Y_test_bin4_predicted)\n",
    "Y_test_bin4_predicted = min_max_scaler.inverse_transform(Y_test_bin4_predicted_scaled)\n",
    "error_prediction_bin4=Y_test_bin4-Y_test_bin4_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin4, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin4=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin4)\n",
    "print(FWHM_bin3)\n",
    "print(FWHM_bin2)\n",
    "print(FWHM_bin1)\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora los histogramnas 2d que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3] *",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "909px",
    "right": "57px",
    "top": "246px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
