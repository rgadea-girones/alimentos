{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python36.zip', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/lib-dynload', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/extensions', '/home/rgadea3/.ipython', '/home/rgadea/lmfit-py/']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dcf10db4d47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/rgadea/lmfit-py/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from hyperopt import Trials, STATUS_OK, tpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "# from hyperopt import Trials, STATUS_OK, tpe\n",
    "# from hyperas import optim\n",
    "# from hyperas.distributions import choice, uniform\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    datos_matlab = sio.loadmat('../datos_mayo_2018/conjunto_entrenamiento_mayo_2018_ring1y2_filtrado9_run200_pho_super+.mat')\n",
    "    conjunto_datos=datos_matlab.get('photodefbox2_todo_fil2')\n",
    "    numero_muestras=conjunto_datos.shape[0]\n",
    "    #print(numero_muestras)\n",
    "    #print('conjunto_datos shape:', conjunto_datos.shape)\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 16, 30\n",
    "    \n",
    "    tr_size=60\n",
    "    val_size=20\n",
    "    test_size=100-val_size-tr_size\n",
    "\n",
    "    tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "    tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "    tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "    #print(tamanyo_tr)\n",
    "    #print(tamanyo_val)\n",
    "    #print(tamanyo_test)\n",
    "    XY_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "    XY_test_bin0=XY_test[np.where((XY_test[:,1]>=99.3128) * (XY_test[:,1]<105.3128))]\n",
    "    XY_test_bin1=XY_test[np.where((XY_test[:,1]>=105.3128) * (XY_test[:,1]<111.3128))]\n",
    "    XY_test_bin2=XY_test[np.where((XY_test[:,1]>=111.3128) * (XY_test[:,1]<117.3128))]\n",
    "    XY_test_bin3=XY_test[np.where((XY_test[:,1]>=117.3128) * (XY_test[:,1]<123.3128))]\n",
    "    XY_test_bin4=XY_test[np.where((XY_test[:,1]>=123.3128))]\n",
    "\n",
    "    X_train=conjunto_datos[:tamanyo_tr,3:483]\n",
    "    X_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,3:483]\n",
    "    X_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,3:483]\n",
    "\n",
    "    X_test_bin0=XY_test_bin0[:,3:483]\n",
    "    Y_test_bin0=XY_test_bin0[:,1]\n",
    "    #print(X_test_bin0.shape)\n",
    "    X_test_bin1=XY_test_bin1[:,3:483]\n",
    "    Y_test_bin1=XY_test_bin1[:,1]\n",
    "    #print(X_test_bin1.shape)\n",
    "    X_test_bin2=XY_test_bin2[:,3:483]\n",
    "    Y_test_bin2=XY_test_bin2[:,1]\n",
    "    #print(X_test_bin2.shape)\n",
    "    X_test_bin3=XY_test_bin3[:,3:483]\n",
    "    Y_test_bin3=XY_test_bin3[:,1]\n",
    "    #print(X_test_bin3.shape)\n",
    "    X_test_bin4=XY_test_bin4[:,3:483]\n",
    "    Y_test_bin4=XY_test_bin4[:,1]\n",
    "    #print(X_test_bin4.shape)\n",
    "\n",
    "\n",
    "\n",
    "    Y_train=conjunto_datos[:tamanyo_tr,1] #elijo la coordenada radius\n",
    "    Y_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,1] #elijo la corrdenada radius\n",
    "    Y_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,1] #elijo la corrdenada radius\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows,1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows,1)\n",
    "\n",
    "    X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], img_cols, img_rows,1)\n",
    "    X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], img_cols, img_rows,1)\n",
    "    X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], img_cols, img_rows,1)\n",
    "    X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], img_cols, img_rows,1)\n",
    "    X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], img_cols, img_rows,1)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_val= X_val.astype('float32')\n",
    "    X_test = X_test.astype('float32')    \n",
    "\n",
    "    #input_shape = (img_cols, img_rows,1)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  model(X_train, Y_train,X_test, Y_test, X_val_Y_val):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    nb_epoch = 150\n",
    "    # number of convolutional filters to use\n",
    "    nb_filters = 32\n",
    "    # size of pooling area for max pooling\n",
    "    pool_size = (2, 2)\n",
    "    # convolution kernel size\n",
    "    kernel_size=(4,4)\n",
    "    #kernel_size = {{choise([(3,3),(4, 4)])}}\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(8, kernel_size=kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=(30,16,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  #optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['mae','acc'])\n",
    "    model.fit(X_train, Y_train, \n",
    "              batch_size={{choice([250,500,1000])}},\n",
    "              epochs=100,\n",
    "              verbose=0,\n",
    "              validation_data=(X_val, Y_val))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test mse:', score[0])\n",
    "    return {'loss': score[0], 'status': STATUS_OK, 'model': model}           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## optimization of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import _pickle as cPickle\n",
    "#%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, X_val, Y_val = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from math import floor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from lmfit.models import GaussianModel\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy.io as sio\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy import exp, loadtxt, pi, sqrt, log\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from lmfit import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'kernel_size': hp.choise('kernel_size', [(3,3),(4, 4)]),\n",
      "        'Dropout': hp.uniform('Dropout', 0,1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [250,500,1000]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: '''\n",
      "   3: Data providing function:\n",
      "   4: \n",
      "   5: This function is separated from model() so that hyperopt\n",
      "   6: won't reload data for each evaluation run.\n",
      "   7: '''\n",
      "   8: datos_matlab = sio.loadmat('../datos_mayo_2018/conjunto_entrenamiento_mayo_2018_ring1y2_filtrado9_run200_pho_super+.mat')\n",
      "   9: conjunto_datos=datos_matlab.get('photodefbox2_todo_fil2')\n",
      "  10: numero_muestras=conjunto_datos.shape[0]\n",
      "  11: #print(numero_muestras)\n",
      "  12: #print('conjunto_datos shape:', conjunto_datos.shape)\n",
      "  13: # input image dimensions\n",
      "  14: img_rows, img_cols = 16, 30\n",
      "  15: \n",
      "  16: tr_size=60\n",
      "  17: val_size=20\n",
      "  18: test_size=100-val_size-tr_size\n",
      "  19: \n",
      "  20: tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
      "  21: tamanyo_val=floor(val_size*numero_muestras/100)\n",
      "  22: tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
      "  23: #print(tamanyo_tr)\n",
      "  24: #print(tamanyo_val)\n",
      "  25: #print(tamanyo_test)\n",
      "  26: XY_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
      "  27: XY_test_bin0=XY_test[np.where((XY_test[:,1]>=99.3128) * (XY_test[:,1]<105.3128))]\n",
      "  28: XY_test_bin1=XY_test[np.where((XY_test[:,1]>=105.3128) * (XY_test[:,1]<111.3128))]\n",
      "  29: XY_test_bin2=XY_test[np.where((XY_test[:,1]>=111.3128) * (XY_test[:,1]<117.3128))]\n",
      "  30: XY_test_bin3=XY_test[np.where((XY_test[:,1]>=117.3128) * (XY_test[:,1]<123.3128))]\n",
      "  31: XY_test_bin4=XY_test[np.where((XY_test[:,1]>=123.3128))]\n",
      "  32: \n",
      "  33: X_train=conjunto_datos[:tamanyo_tr,3:483]\n",
      "  34: X_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,3:483]\n",
      "  35: X_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,3:483]\n",
      "  36: \n",
      "  37: X_test_bin0=XY_test_bin0[:,3:483]\n",
      "  38: Y_test_bin0=XY_test_bin0[:,1]\n",
      "  39: #print(X_test_bin0.shape)\n",
      "  40: X_test_bin1=XY_test_bin1[:,3:483]\n",
      "  41: Y_test_bin1=XY_test_bin1[:,1]\n",
      "  42: #print(X_test_bin1.shape)\n",
      "  43: X_test_bin2=XY_test_bin2[:,3:483]\n",
      "  44: Y_test_bin2=XY_test_bin2[:,1]\n",
      "  45: #print(X_test_bin2.shape)\n",
      "  46: X_test_bin3=XY_test_bin3[:,3:483]\n",
      "  47: Y_test_bin3=XY_test_bin3[:,1]\n",
      "  48: #print(X_test_bin3.shape)\n",
      "  49: X_test_bin4=XY_test_bin4[:,3:483]\n",
      "  50: Y_test_bin4=XY_test_bin4[:,1]\n",
      "  51: #print(X_test_bin4.shape)\n",
      "  52: \n",
      "  53: \n",
      "  54: \n",
      "  55: Y_train=conjunto_datos[:tamanyo_tr,1] #elijo la coordenada radius\n",
      "  56: Y_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,1] #elijo la corrdenada radius\n",
      "  57: Y_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,1] #elijo la corrdenada radius\n",
      "  58: \n",
      "  59: X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows,1)\n",
      "  60: X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
      "  61: X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows,1)\n",
      "  62: \n",
      "  63: X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], img_cols, img_rows,1)\n",
      "  64: X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], img_cols, img_rows,1)\n",
      "  65: X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], img_cols, img_rows,1)\n",
      "  66: X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], img_cols, img_rows,1)\n",
      "  67: X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], img_cols, img_rows,1)\n",
      "  68: \n",
      "  69: X_train = X_train.astype('float32')\n",
      "  70: X_val= X_val.astype('float32')\n",
      "  71: X_test = X_test.astype('float32')    \n",
      "  72: \n",
      "  73: #input_shape = (img_cols, img_rows,1)\n",
      "  74: \n",
      "  75: \n",
      "  76: \n",
      "  77: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13:     nb_epoch = 150\n",
      "  14:     # number of convolutional filters to use\n",
      "  15:     nb_filters = 32\n",
      "  16:     # size of pooling area for max pooling\n",
      "  17:     pool_size = (2, 2)\n",
      "  18:     # convolution kernel size\n",
      "  19:     kernel_size=(4,4)\n",
      "  20:     #kernel_size = space['kernel_size']\n",
      "  21:     model = Sequential()\n",
      "  22: \n",
      "  23:     model.add(Conv2D(8, kernel_size=kernel_size,\n",
      "  24:                             padding='same',\n",
      "  25:                             input_shape=(30,16,1)))\n",
      "  26:     model.add(BatchNormalization())\n",
      "  27:     model.add(Activation('relu'))\n",
      "  28:     model.add(MaxPooling2D(pool_size=pool_size))\n",
      "  29: \n",
      "  30:     model.add(Conv2D(16, kernel_size, padding='same'))\n",
      "  31:     model.add(BatchNormalization())\n",
      "  32:     model.add(Activation('relu'))\n",
      "  33:     model.add(MaxPooling2D(pool_size=pool_size))\n",
      "  34: \n",
      "  35:     model.add(Conv2D(32, kernel_size, padding='same'))\n",
      "  36:     model.add(BatchNormalization())\n",
      "  37:     model.add(Activation('relu'))\n",
      "  38: \n",
      "  39:     model.add(Conv2D(64, kernel_size, padding='same'))\n",
      "  40:     model.add(BatchNormalization())\n",
      "  41:     model.add(Activation('relu'))\n",
      "  42: \n",
      "  43:     model.add(Dropout(space['Dropout']))\n",
      "  44:     model.add(Flatten())\n",
      "  45:     model.add(Dense(1))\n",
      "  46: \n",
      "  47: \n",
      "  48:     model.compile(loss='mean_squared_error',\n",
      "  49:                   #optimizer=space['optimizer'],\n",
      "  50:                   optimizer='rmsprop',\n",
      "  51:                   metrics=['mae','acc'])\n",
      "  52:     model.fit(X_train, Y_train, \n",
      "  53:               batch_size=space['batch_size'],\n",
      "  54:               epochs=100,\n",
      "  55:               verbose=0,\n",
      "  56:               validation_data=(X_val, Y_val))\n",
      "  57:     score = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  58:     print('Test mse:', score[0])\n",
      "  59:     return {'loss': score[0], 'status': STATUS_OK, 'model': model}           \n",
      "  60: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'hyperopt.hp' has no attribute 'choise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f7437e8cf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                       notebook_name='CNN_for_PETALO_radius_optimization')\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acabe la optimizacion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                      verbose=verbose)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m    113\u001b[0m         return (\n\u001b[1;32m    114\u001b[0m             fmin(keras_fmin_fnct,\n\u001b[0;32m--> 115\u001b[0;31m                  \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                  \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                  \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/volumedisk0/home/rgadea/experimentos/notebooks_new/temp_model.py\u001b[0m in \u001b[0;36mget_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'hyperopt.hp' has no attribute 'choise'"
     ]
    }
   ],
   "source": [
    "# %%cache idea.pkl best_run best_model\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='CNN_for_PETALO_radius_optimization')\n",
    "print('acabe la optimizacion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test mse:', score[0])\n",
    "print('Test mae:', score[1])\n",
    "Y_test_predicted=best_model.predict(X_test)\n",
    "print(Y_test_predicted[:10].flatten())\n",
    "print(Y_test[:10])\n",
    "error_prediction=Y_test-Y_test_predicted.flatten()\n",
    "\n",
    "print(error_prediction[:10])\n",
    "n, bins, patches = plt.hist(error_prediction, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWHM=result.params['wid'].value*2*sqrt(log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FWHM)\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin0_predicted=best_model.predict(X_test_bin0)\n",
    "print(Y_test_bin0_predicted)\n",
    "error_prediction_bin0=Y_test_bin0-Y_test_bin0_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin0, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin0=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin1_predicted=best_model.predict(X_test_bin1)\n",
    "#print(Y_test_bin1_predicted)\n",
    "error_prediction_bin1=Y_test_bin1-Y_test_bin1_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin1, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin1=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin2_predicted=best_model.predict(X_test_bin2)\n",
    "#print(Y_test_bin2_predicted)\n",
    "error_prediction_bin2=Y_test_bin2-Y_test_bin2_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin2, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin2=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin3_predicted=best_model.predict(X_test_bin3)\n",
    "#print(Y_test_bin3_predicted)\n",
    "error_prediction_bin3=Y_test_bin3-Y_test_bin3_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin3, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin3=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_bin4_predicted=best_model.predict(X_test_bin4)\n",
    "#print(Y_test_bin4_predicted)\n",
    "error_prediction_bin4=Y_test_bin4-Y_test_bin4_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin4, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin4=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin4)\n",
    "print(FWHM_bin3)\n",
    "print(FWHM_bin2)\n",
    "print(FWHM_bin1)\n",
    "print(FWHM_bin0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
