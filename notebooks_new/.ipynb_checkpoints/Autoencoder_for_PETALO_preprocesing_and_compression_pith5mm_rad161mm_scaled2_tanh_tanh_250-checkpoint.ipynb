{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AUTOENCODER for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python36.zip', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/lib-dynload', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages', '/home/rgadea3/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/extensions', '/home/rgadea3/.ipython']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en matlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66498, 640)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import hdf5storage\n",
    "datos_matlab = hdf5storage.loadmat('../datos_octubre_2018/conjunto_entrenamiento_octubre_2018_red_pitch5mm_rad161mm_total.mat')\n",
    "conjunto_datos= datos_matlab.get('photodefA')\n",
    "conjunto_datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6320, 3840)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dir_name='../datos_octubre_2018'\n",
    "base_filename='p_OF_5mm_161mm'\n",
    "filename_suffix='.h5'\n",
    "file=os.path.join(dir_name, base_filename+ \"{0:03d}\".format(0) + filename_suffix)\n",
    "conjunto_datos_waves=pd.read_hdf(file,'MC')\n",
    "datos_waves=conjunto_datos_waves.values\n",
    "datos_waves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12641, 3840)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    file=os.path.join(dir_name, base_filename+ \"{0:03d}\".format(i) + filename_suffix)\n",
    "    #print(file)\n",
    "    veamos=pd.read_hdf(file,'MC')\n",
    "    veamos_array=veamos.values\n",
    "    datos_waves=np.concatenate((datos_waves,veamos_array),axis=0)\n",
    "datos_waves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12641, 3840)\n"
     ]
    }
   ],
   "source": [
    "L1A=6;\n",
    "# hay tres L1 con 640 sensores (40*16)\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 16, 40\n",
    "\n",
    "X_trained=datos_waves;\n",
    "x_trained=X_trained;\n",
    "\n",
    "for i in range (X_trained.shape[0]):\n",
    "    idea1=X_trained[i,:].reshape(img_rows,(L1A*img_cols));\n",
    "    ideat=idea1.transpose();\n",
    "    idea2=ideat.reshape(1,(L1A*img_cols)*img_rows);\n",
    "    x_trained[i,:] =idea2;\n",
    "\n",
    "print(x_trained.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_dim_A=img_rows*img_cols\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "\n",
    "conjunto_datos=np.zeros((x_trained.shape[0]*L1A,input_output_dim_A))\n",
    "for i in range(x_trained.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_trained[i,k*input_output_dim_A:k*input_output_dim_A+input_output_dim_A]\n",
    "    conjunto_datos[(i)*L1A :(i+1)*L1A,:] = ideaA    \n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_regularizer = True\n",
    "my_regularizer = None\n",
    "my_epochs = 50\n",
    "features_path = 'simple_autoe_features.pickle'\n",
    "labels_path = 'simple_autoe_labels.pickle'\n",
    "\n",
    "if use_regularizer:\n",
    "    # add a sparsity constraint on the encoded representations\n",
    "    # note use of 10e-5 leads to blurred results\n",
    "    my_regularizer = regularizers.l2(0.001)\n",
    "    # and a larger number of epochs as the added regularization the model\n",
    "    # is less likely to overfit and can be trained longer\n",
    "    my_epochs = 100\n",
    "    features_path = 'sparse_autoe_features.pickle'\n",
    "    labels_path = 'sparse_autoe_labels.pickle'\n",
    "\n",
    "   \n",
    "    \n",
    "encoding_dim = 250  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "\n",
    "input_img = Input(shape=(img_rows*img_cols,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh', use_bias=False,bias_initializer='random_uniform')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(img_cols*img_rows, activation='tanh',use_bias=True,bias_initializer='random_uniform')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "\n",
    "\n",
    "#autoencoder=Sequential([\n",
    "#    Dense(encoding_dim, kernel_regularizer=regularizers.l2(0.001), use_bias=True,bias_initializer='random_uniform',input_shape=(640,)),\n",
    "#    Activation('sigmoid'),\n",
    "#    Dense(img_cols*img_rows, use_bias=True,bias_initializer='random_uniform'),\n",
    "#    Activation('linear'),\n",
    "#])\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75846\n",
      "conjunto_datos shape: (75846, 640)\n",
      "45507\n",
      "15169\n",
      "15170\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "numero_muestras=conjunto_datos.shape[0]\n",
    "print(numero_muestras)\n",
    "print('conjunto_datos shape:', conjunto_datos.shape)\n",
    "\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "X_train=conjunto_datos[:tamanyo_tr,:]\n",
    "X_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,:]\n",
    "X_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_train=conjunto_datos[:tamanyo_tr,1] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,1] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,1] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows,1)\n",
    "\n",
    "\n",
    "input_shape = (img_cols, img_rows,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (45507, 40, 16, 1)\n",
      "45507 train samples\n",
      "15169 validation samples\n",
      "15170 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8976\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30786\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24026\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,X_train.shape[0])\n",
    "    plt.imshow(np.reshape(X_train[idea].transpose(), [16, 40]), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    plt.show()\n",
    "    print(idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar las matrices de datos para la red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45507, 640)\n",
      "(15170, 640)\n",
      "(15170, 640)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "x_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "prueba=x_train[0:15170,:]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(prueba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.98, -1.  , -1.  , -1.  , -0.98, -0.98, -1.  , -0.94, -1.  ,\n",
       "       -0.92, -0.96, -0.98, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -0.96, -0.94, -0.92, -0.98, -1.  , -0.96,\n",
       "       -0.98, -0.96, -0.98, -0.96, -0.98, -1.  , -0.98, -0.98, -0.98,\n",
       "       -0.96, -1.  , -0.92, -0.9 , -0.92, -0.98, -1.  , -1.  , -0.98,\n",
       "       -0.98, -0.96, -0.98, -1.  , -1.  , -0.98, -1.  , -0.98, -0.96,\n",
       "       -0.94, -0.96, -0.98, -1.  , -0.92, -0.92, -0.92, -1.  , -0.98,\n",
       "       -0.98, -0.98, -1.  , -0.98, -1.  , -0.98, -0.94, -0.98, -0.98,\n",
       "       -0.96, -0.98, -0.96, -0.94, -0.98, -1.  , -0.98, -0.98, -1.  ,\n",
       "       -1.  , -1.  , -1.  , -0.98, -0.96, -0.96, -0.98, -0.94, -0.92,\n",
       "       -0.94, -0.94, -0.98, -0.94, -1.  , -0.96, -1.  , -1.  , -0.98,\n",
       "       -1.  , -1.  , -0.96, -0.98, -0.96, -0.98, -0.96, -1.  , -0.94,\n",
       "       -0.96, -0.98, -0.96, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -0.96, -0.94, -0.96, -0.98, -1.  , -0.98, -0.94, -1.  ,\n",
       "       -1.  , -0.98, -1.  , -1.  , -1.  , -0.98, -0.96, -1.  , -1.  ,\n",
       "       -0.98, -0.92, -0.96, -1.  , -0.98, -0.98, -1.  , -1.  , -0.98,\n",
       "       -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  , -0.96, -0.98,\n",
       "       -1.  , -1.  , -0.98, -0.98, -1.  , -1.  , -1.  , -0.98, -0.98,\n",
       "       -1.  , -1.  , -1.  , -0.96, -1.  , -0.98, -0.96, -1.  , -0.98,\n",
       "       -0.96, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98,\n",
       "       -0.98, -1.  , -1.  , -0.98, -0.98, -1.  , -1.  , -0.98, -1.  ,\n",
       "       -1.  , -0.98, -0.98, -1.  , -1.  , -0.98, -1.  , -1.  , -0.98,\n",
       "       -0.98, -0.96, -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -0.98, -1.  , -0.98,\n",
       "       -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -0.98, -0.98, -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -0.98, -0.98, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98,\n",
       "       -1.  , -0.98, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -0.96, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -0.98, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -0.98, -1.  , -1.  , -1.  ,\n",
       "       -1.  , -0.98, -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,\n",
       "       -1.  ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_max_scaler = preprocessing.QuantileTransformer().fit(x_train)\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler().fit(x_train)\n",
    "# min_max_scaler = preprocessing.StandardScaler(with_mean=False).fit(x_train)\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(x_train)\n",
    "#min_max_scaler = preprocessing.RobustScaler().fit(x_train)\n",
    "supermax=100\n",
    "factor_aprendizaje=0.0001\n",
    "print(min_max_scaler)\n",
    "#x_train_scaled = min_max_scaler.transform(x_train)\n",
    "#x_test_scaled = min_max_scaler.transform(x_test)\n",
    "x_train_scaled=(2*x_train/supermax)-1\n",
    "x_test_scaled=(2*x_test/supermax)-1\n",
    "#min_max_scaler.scale_\n",
    "x_train[29413]\n",
    "x_train_scaled[29413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='RMSprop', loss='mse')\n",
    "\n",
    "autoencoder.optimizer.lr=(factor_aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45507 samples, validate on 15170 samples\n",
      "Epoch 1/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 0.0556 - val_loss: 0.0012\n",
      "Epoch 2/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 4/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 5/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 6/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 7/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 8/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 9/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 10/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 12/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 13/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 14/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 15/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 16/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 17/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 18/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 19/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 20/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 21/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 22/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 23/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 24/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 25/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 26/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 27/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 28/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 29/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 30/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 31/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 32/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 33/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 34/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 35/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 36/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 37/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 38/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 39/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 40/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 41/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 42/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 43/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 44/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 45/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 46/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 47/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 48/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 49/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 50/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 51/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 52/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 53/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 54/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 55/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 56/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 57/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 58/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 59/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 60/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 61/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 62/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 63/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 64/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 65/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 66/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 67/10000\n",
      "45507/45507 [==============================] - 1s 25us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 68/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 69/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 70/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 71/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 72/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 73/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 74/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 75/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 76/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 77/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 78/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 79/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 80/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 81/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 82/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 83/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 84/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 85/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 86/10000\n",
      "45507/45507 [==============================] - 1s 24us/step - loss: 0.0010 - val_loss: 9.9642e-04\n",
      "Epoch 87/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 9.9226e-04 - val_loss: 9.9071e-04\n",
      "Epoch 88/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 9.8428e-04 - val_loss: 9.8275e-04\n",
      "Epoch 89/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 9.7454e-04 - val_loss: 9.7801e-04\n",
      "Epoch 90/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 9.6698e-04 - val_loss: 9.6865e-04\n",
      "Epoch 91/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 9.6060e-04 - val_loss: 9.5943e-04\n",
      "Epoch 92/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 9.5101e-04 - val_loss: 9.5236e-04\n",
      "Epoch 93/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 9.4192e-04 - val_loss: 9.4736e-04\n",
      "Epoch 94/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 9.3594e-04 - val_loss: 9.4027e-04\n",
      "Epoch 95/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 9.2721e-04 - val_loss: 9.3170e-04\n",
      "Epoch 96/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 9.2071e-04 - val_loss: 9.2658e-04\n",
      "Epoch 97/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 9.1270e-04 - val_loss: 9.1929e-04\n",
      "Epoch 98/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 9.0567e-04 - val_loss: 9.1136e-04\n",
      "Epoch 99/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 8.9747e-04 - val_loss: 9.0404e-04\n",
      "Epoch 100/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 8.9297e-04 - val_loss: 8.9732e-04\n",
      "Epoch 101/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 8.8673e-04 - val_loss: 8.9090e-04\n",
      "Epoch 102/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 8.7952e-04 - val_loss: 8.8503e-04\n",
      "Epoch 103/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 8.7510e-04 - val_loss: 8.7874e-04\n",
      "Epoch 104/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.6863e-04 - val_loss: 8.7286e-04\n",
      "Epoch 105/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 8.6265e-04 - val_loss: 8.6707e-04\n",
      "Epoch 106/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 8.5692e-04 - val_loss: 8.6017e-04\n",
      "Epoch 107/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 8.5189e-04 - val_loss: 8.5423e-04\n",
      "Epoch 108/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 8.4702e-04 - val_loss: 8.4928e-04\n",
      "Epoch 109/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.4232e-04 - val_loss: 8.4533e-04\n",
      "Epoch 110/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 8.3750e-04 - val_loss: 8.4192e-04\n",
      "Epoch 111/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.3330e-04 - val_loss: 8.3880e-04\n",
      "Epoch 112/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.2901e-04 - val_loss: 8.3551e-04\n",
      "Epoch 113/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 8.2518e-04 - val_loss: 8.3249e-04\n",
      "Epoch 114/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.2093e-04 - val_loss: 8.2974e-04\n",
      "Epoch 115/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.1722e-04 - val_loss: 8.2659e-04\n",
      "Epoch 116/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 8.1345e-04 - val_loss: 8.2231e-04\n",
      "Epoch 117/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 8.0994e-04 - val_loss: 8.1773e-04\n",
      "Epoch 118/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 8.0645e-04 - val_loss: 8.1611e-04\n",
      "Epoch 119/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 8.0354e-04 - val_loss: 8.1482e-04\n",
      "Epoch 120/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 8.0030e-04 - val_loss: 8.1335e-04\n",
      "Epoch 121/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.9769e-04 - val_loss: 8.1148e-04\n",
      "Epoch 122/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.9510e-04 - val_loss: 8.0941e-04\n",
      "Epoch 123/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.9238e-04 - val_loss: 8.0653e-04\n",
      "Epoch 124/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 7.8974e-04 - val_loss: 8.0273e-04\n",
      "Epoch 125/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 7.8725e-04 - val_loss: 7.9789e-04\n",
      "Epoch 126/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.8480e-04 - val_loss: 7.9462e-04\n",
      "Epoch 127/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.8237e-04 - val_loss: 7.9280e-04\n",
      "Epoch 128/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.8007e-04 - val_loss: 7.9178e-04\n",
      "Epoch 129/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.7774e-04 - val_loss: 7.9068e-04\n",
      "Epoch 130/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.7533e-04 - val_loss: 7.8910e-04\n",
      "Epoch 131/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.7288e-04 - val_loss: 7.8733e-04\n",
      "Epoch 132/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 7.7049e-04 - val_loss: 7.8569e-04\n",
      "Epoch 133/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.6821e-04 - val_loss: 7.8406e-04\n",
      "Epoch 134/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.6607e-04 - val_loss: 7.8244e-04\n",
      "Epoch 135/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.6405e-04 - val_loss: 7.8079e-04\n",
      "Epoch 136/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.6213e-04 - val_loss: 7.7909e-04\n",
      "Epoch 137/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.6061e-04 - val_loss: 7.7714e-04\n",
      "Epoch 138/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.5866e-04 - val_loss: 7.7521e-04\n",
      "Epoch 139/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.5717e-04 - val_loss: 7.7327e-04\n",
      "Epoch 140/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.5564e-04 - val_loss: 7.7128e-04\n",
      "Epoch 141/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.5412e-04 - val_loss: 7.6924e-04\n",
      "Epoch 142/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.5241e-04 - val_loss: 7.6724e-04\n",
      "Epoch 143/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 7.5078e-04 - val_loss: 7.6541e-04\n",
      "Epoch 144/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 7.4891e-04 - val_loss: 7.6402e-04\n",
      "Epoch 145/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 1s 33us/step - loss: 7.4729e-04 - val_loss: 7.6255e-04\n",
      "Epoch 146/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.4591e-04 - val_loss: 7.6074e-04\n",
      "Epoch 147/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.4458e-04 - val_loss: 7.5890e-04\n",
      "Epoch 148/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 7.4293e-04 - val_loss: 7.5712e-04\n",
      "Epoch 149/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.4143e-04 - val_loss: 7.5555e-04\n",
      "Epoch 150/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.4041e-04 - val_loss: 7.5422e-04\n",
      "Epoch 151/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.3932e-04 - val_loss: 7.5295e-04\n",
      "Epoch 152/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.3800e-04 - val_loss: 7.5168e-04\n",
      "Epoch 153/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.3670e-04 - val_loss: 7.5047e-04\n",
      "Epoch 154/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.3541e-04 - val_loss: 7.4929e-04\n",
      "Epoch 155/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.3413e-04 - val_loss: 7.4810e-04\n",
      "Epoch 156/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.3288e-04 - val_loss: 7.4690e-04\n",
      "Epoch 157/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.3167e-04 - val_loss: 7.4570e-04\n",
      "Epoch 158/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.3048e-04 - val_loss: 7.4451e-04\n",
      "Epoch 159/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.2934e-04 - val_loss: 7.4333e-04\n",
      "Epoch 160/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.2823e-04 - val_loss: 7.4216e-04\n",
      "Epoch 161/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.2714e-04 - val_loss: 7.4098e-04\n",
      "Epoch 162/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 7.2607e-04 - val_loss: 7.3976e-04\n",
      "Epoch 163/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 7.2503e-04 - val_loss: 7.3848e-04\n",
      "Epoch 164/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.2401e-04 - val_loss: 7.3716e-04\n",
      "Epoch 165/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.2301e-04 - val_loss: 7.3589e-04\n",
      "Epoch 166/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.2204e-04 - val_loss: 7.3477e-04\n",
      "Epoch 167/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 7.2107e-04 - val_loss: 7.3377e-04\n",
      "Epoch 168/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.2012e-04 - val_loss: 7.3274e-04\n",
      "Epoch 169/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1929e-04 - val_loss: 7.3159e-04\n",
      "Epoch 170/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.1845e-04 - val_loss: 7.3037e-04\n",
      "Epoch 171/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1758e-04 - val_loss: 7.2914e-04\n",
      "Epoch 172/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1675e-04 - val_loss: 7.2796e-04\n",
      "Epoch 173/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.1596e-04 - val_loss: 7.2688e-04\n",
      "Epoch 174/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1516e-04 - val_loss: 7.2592e-04\n",
      "Epoch 175/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.1436e-04 - val_loss: 7.2508e-04\n",
      "Epoch 176/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1358e-04 - val_loss: 7.2436e-04\n",
      "Epoch 177/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1283e-04 - val_loss: 7.2376e-04\n",
      "Epoch 178/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1211e-04 - val_loss: 7.2322e-04\n",
      "Epoch 179/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.1143e-04 - val_loss: 7.2271e-04\n",
      "Epoch 180/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.1077e-04 - val_loss: 7.2218e-04\n",
      "Epoch 181/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 7.1014e-04 - val_loss: 7.2160e-04\n",
      "Epoch 182/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 7.0952e-04 - val_loss: 7.2097e-04\n",
      "Epoch 183/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.0890e-04 - val_loss: 7.2033e-04\n",
      "Epoch 184/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.0825e-04 - val_loss: 7.1968e-04\n",
      "Epoch 185/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.0758e-04 - val_loss: 7.1903e-04\n",
      "Epoch 186/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.0689e-04 - val_loss: 7.1837e-04\n",
      "Epoch 187/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.0618e-04 - val_loss: 7.1771e-04\n",
      "Epoch 188/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.0549e-04 - val_loss: 7.1704e-04\n",
      "Epoch 189/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.0506e-04 - val_loss: 7.1635e-04\n",
      "Epoch 190/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.0475e-04 - val_loss: 7.1555e-04\n",
      "Epoch 191/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.0388e-04 - val_loss: 7.1480e-04\n",
      "Epoch 192/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 7.0284e-04 - val_loss: 7.1425e-04\n",
      "Epoch 193/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 7.0218e-04 - val_loss: 7.1394e-04\n",
      "Epoch 194/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 7.0161e-04 - val_loss: 7.1361e-04\n",
      "Epoch 195/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 7.0107e-04 - val_loss: 7.1332e-04\n",
      "Epoch 196/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 7.0051e-04 - val_loss: 7.1309e-04\n",
      "Epoch 197/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9994e-04 - val_loss: 7.1293e-04\n",
      "Epoch 198/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9937e-04 - val_loss: 7.1284e-04\n",
      "Epoch 199/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.9879e-04 - val_loss: 7.1281e-04\n",
      "Epoch 200/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 6.9819e-04 - val_loss: 7.1284e-04\n",
      "Epoch 201/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.9758e-04 - val_loss: 7.1294e-04\n",
      "Epoch 202/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.9692e-04 - val_loss: 7.1305e-04\n",
      "Epoch 203/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9631e-04 - val_loss: 7.1317e-04\n",
      "Epoch 204/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.9585e-04 - val_loss: 7.1297e-04\n",
      "Epoch 205/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.9565e-04 - val_loss: 7.1294e-04\n",
      "Epoch 206/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9513e-04 - val_loss: 7.1317e-04\n",
      "Epoch 207/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.9452e-04 - val_loss: 7.1334e-04\n",
      "Epoch 208/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9393e-04 - val_loss: 7.1327e-04\n",
      "Epoch 209/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9343e-04 - val_loss: 7.1326e-04\n",
      "Epoch 210/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9294e-04 - val_loss: 7.1332e-04\n",
      "Epoch 211/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9248e-04 - val_loss: 7.1340e-04\n",
      "Epoch 212/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.9204e-04 - val_loss: 7.1353e-04\n",
      "Epoch 213/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.9180e-04 - val_loss: 7.1392e-04\n",
      "Epoch 214/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.9143e-04 - val_loss: 7.1417e-04\n",
      "Epoch 215/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9105e-04 - val_loss: 7.1439e-04\n",
      "Epoch 216/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9062e-04 - val_loss: 7.1455e-04\n",
      "Epoch 217/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.9019e-04 - val_loss: 7.1472e-04\n",
      "Epoch 218/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.8979e-04 - val_loss: 7.1492e-04\n",
      "Epoch 219/10000\n",
      "45507/45507 [==============================] - 1s 24us/step - loss: 6.8943e-04 - val_loss: 7.1515e-04\n",
      "Epoch 220/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.8908e-04 - val_loss: 7.1560e-04\n",
      "Epoch 221/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8907e-04 - val_loss: 7.1596e-04\n",
      "Epoch 222/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8863e-04 - val_loss: 7.1570e-04\n",
      "Epoch 223/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8821e-04 - val_loss: 7.1571e-04\n",
      "Epoch 224/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8775e-04 - val_loss: 7.1571e-04\n",
      "Epoch 225/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 6.8731e-04 - val_loss: 7.1547e-04\n",
      "Epoch 226/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8688e-04 - val_loss: 7.1494e-04\n",
      "Epoch 227/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.8645e-04 - val_loss: 7.1408e-04\n",
      "Epoch 228/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 6.8600e-04 - val_loss: 7.1309e-04\n",
      "Epoch 229/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8556e-04 - val_loss: 7.1301e-04\n",
      "Epoch 230/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8574e-04 - val_loss: 7.1281e-04\n",
      "Epoch 231/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 6.8500e-04 - val_loss: 7.1309e-04\n",
      "Epoch 232/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.8447e-04 - val_loss: 7.1306e-04\n",
      "Epoch 233/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8415e-04 - val_loss: 7.1343e-04\n",
      "Epoch 234/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8383e-04 - val_loss: 7.1389e-04\n",
      "Epoch 235/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8353e-04 - val_loss: 7.1455e-04\n",
      "Epoch 236/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8328e-04 - val_loss: 7.1551e-04\n",
      "Epoch 237/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.8304e-04 - val_loss: 7.1651e-04\n",
      "Epoch 238/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 6.8279e-04 - val_loss: 7.1744e-04\n",
      "Epoch 239/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.8254e-04 - val_loss: 7.1832e-04\n",
      "Epoch 240/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8229e-04 - val_loss: 7.1911e-04\n",
      "Epoch 241/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.8204e-04 - val_loss: 7.1977e-04\n",
      "Epoch 242/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.8175e-04 - val_loss: 7.2024e-04\n",
      "Epoch 243/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8139e-04 - val_loss: 7.2062e-04\n",
      "Epoch 244/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.8094e-04 - val_loss: 7.2024e-04\n",
      "Epoch 245/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8064e-04 - val_loss: 7.2161e-04\n",
      "Epoch 246/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.8033e-04 - val_loss: 7.2402e-04\n",
      "Epoch 247/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.7989e-04 - val_loss: 7.2394e-04\n",
      "Epoch 248/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7954e-04 - val_loss: 7.2280e-04\n",
      "Epoch 249/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7909e-04 - val_loss: 7.2463e-04\n",
      "Epoch 250/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.7890e-04 - val_loss: 7.2414e-04\n",
      "Epoch 251/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.7860e-04 - val_loss: 7.2415e-04\n",
      "Epoch 252/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7832e-04 - val_loss: 7.2439e-04\n",
      "Epoch 253/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.7809e-04 - val_loss: 7.2461e-04\n",
      "Epoch 254/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 6.7786e-04 - val_loss: 7.2472e-04\n",
      "Epoch 255/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7762e-04 - val_loss: 7.2471e-04\n",
      "Epoch 256/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.7739e-04 - val_loss: 7.2458e-04\n",
      "Epoch 257/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 6.7720e-04 - val_loss: 7.2277e-04\n",
      "Epoch 258/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7710e-04 - val_loss: 7.2137e-04\n",
      "Epoch 259/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7675e-04 - val_loss: 7.2188e-04\n",
      "Epoch 260/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7643e-04 - val_loss: 7.2396e-04\n",
      "Epoch 261/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7625e-04 - val_loss: 7.2472e-04\n",
      "Epoch 262/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7607e-04 - val_loss: 7.2547e-04\n",
      "Epoch 263/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7587e-04 - val_loss: 7.2638e-04\n",
      "Epoch 264/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7515e-04 - val_loss: 7.2850e-04\n",
      "Epoch 265/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7615e-04 - val_loss: 7.2318e-04\n",
      "Epoch 266/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7563e-04 - val_loss: 7.2301e-04\n",
      "Epoch 267/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.7536e-04 - val_loss: 7.2396e-04\n",
      "Epoch 268/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7463e-04 - val_loss: 7.2671e-04\n",
      "Epoch 269/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7473e-04 - val_loss: 7.2354e-04\n",
      "Epoch 270/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 6.7375e-04 - val_loss: 7.2133e-04\n",
      "Epoch 271/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7452e-04 - val_loss: 7.4525e-04\n",
      "Epoch 272/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7279e-04 - val_loss: 7.2337e-04\n",
      "Epoch 273/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7315e-04 - val_loss: 7.3239e-04\n",
      "Epoch 274/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7310e-04 - val_loss: 7.2946e-04\n",
      "Epoch 275/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.7271e-04 - val_loss: 7.2945e-04\n",
      "Epoch 276/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 6.7238e-04 - val_loss: 7.2964e-04\n",
      "Epoch 277/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7202e-04 - val_loss: 7.3044e-04\n",
      "Epoch 278/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7162e-04 - val_loss: 7.3150e-04\n",
      "Epoch 279/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7108e-04 - val_loss: 7.3287e-04\n",
      "Epoch 280/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7135e-04 - val_loss: 7.2979e-04\n",
      "Epoch 281/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7157e-04 - val_loss: 7.3303e-04\n",
      "Epoch 282/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7143e-04 - val_loss: 7.3029e-04\n",
      "Epoch 283/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7083e-04 - val_loss: 7.2561e-04\n",
      "Epoch 284/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7025e-04 - val_loss: 7.3438e-04\n",
      "Epoch 285/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7104e-04 - val_loss: 7.3265e-04\n",
      "Epoch 286/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.7020e-04 - val_loss: 7.3840e-04\n",
      "Epoch 287/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6983e-04 - val_loss: 7.3544e-04\n",
      "Epoch 288/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.7012e-04 - val_loss: 7.2609e-04\n",
      "Epoch 289/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.7122e-04 - val_loss: 7.2504e-04\n",
      "Epoch 290/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.7036e-04 - val_loss: 7.2507e-04\n",
      "Epoch 291/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6980e-04 - val_loss: 7.2119e-04\n",
      "Epoch 292/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.6990e-04 - val_loss: 7.2922e-04\n",
      "Epoch 293/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6969e-04 - val_loss: 7.2881e-04\n",
      "Epoch 294/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.6932e-04 - val_loss: 7.3016e-04\n",
      "Epoch 295/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.6907e-04 - val_loss: 7.2967e-04\n",
      "Epoch 296/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.6878e-04 - val_loss: 7.2556e-04\n",
      "Epoch 297/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6828e-04 - val_loss: 7.2435e-04\n",
      "Epoch 298/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6874e-04 - val_loss: 7.0586e-04\n",
      "Epoch 299/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6824e-04 - val_loss: 7.1433e-04\n",
      "Epoch 300/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6833e-04 - val_loss: 7.0508e-04\n",
      "Epoch 301/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6763e-04 - val_loss: 7.1674e-04\n",
      "Epoch 302/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 6.6780e-04 - val_loss: 7.3803e-04\n",
      "Epoch 303/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6690e-04 - val_loss: 7.2940e-04\n",
      "Epoch 304/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6759e-04 - val_loss: 7.1894e-04\n",
      "Epoch 305/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.6710e-04 - val_loss: 7.0286e-04\n",
      "Epoch 306/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6808e-04 - val_loss: 7.2916e-04\n",
      "Epoch 307/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6660e-04 - val_loss: 7.1942e-04\n",
      "Epoch 308/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.6704e-04 - val_loss: 7.3387e-04\n",
      "Epoch 309/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6712e-04 - val_loss: 7.3648e-04\n",
      "Epoch 310/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 6.6753e-04 - val_loss: 7.0650e-04\n",
      "Epoch 311/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6636e-04 - val_loss: 7.3559e-04\n",
      "Epoch 312/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6628e-04 - val_loss: 7.2151e-04\n",
      "Epoch 313/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6645e-04 - val_loss: 7.1281e-04\n",
      "Epoch 314/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.6598e-04 - val_loss: 7.1258e-04\n",
      "Epoch 315/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6620e-04 - val_loss: 7.3314e-04\n",
      "Epoch 316/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6624e-04 - val_loss: 7.0113e-04\n",
      "Epoch 317/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6569e-04 - val_loss: 7.1346e-04\n",
      "Epoch 318/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6590e-04 - val_loss: 7.1794e-04\n",
      "Epoch 319/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6578e-04 - val_loss: 7.4541e-04\n",
      "Epoch 320/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6666e-04 - val_loss: 7.2737e-04\n",
      "Epoch 321/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6605e-04 - val_loss: 7.3459e-04\n",
      "Epoch 322/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6556e-04 - val_loss: 7.1927e-04\n",
      "Epoch 323/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6602e-04 - val_loss: 7.1550e-04\n",
      "Epoch 324/10000\n",
      "45507/45507 [==============================] - 1s 33us/step - loss: 6.6550e-04 - val_loss: 7.2806e-04\n",
      "Epoch 325/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6395e-04 - val_loss: 6.9771e-04\n",
      "Epoch 326/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6459e-04 - val_loss: 7.1891e-04\n",
      "Epoch 327/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6494e-04 - val_loss: 7.2782e-04\n",
      "Epoch 328/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6543e-04 - val_loss: 7.2138e-04\n",
      "Epoch 329/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6390e-04 - val_loss: 7.3125e-04\n",
      "Epoch 330/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6505e-04 - val_loss: 7.1942e-04\n",
      "Epoch 331/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6343e-04 - val_loss: 7.1188e-04\n",
      "Epoch 332/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 6.6400e-04 - val_loss: 7.1364e-04\n",
      "Epoch 333/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 6.6417e-04 - val_loss: 7.2341e-04\n",
      "Epoch 334/10000\n",
      "45507/45507 [==============================] - 1s 28us/step - loss: 6.6401e-04 - val_loss: 7.3041e-04\n",
      "Epoch 335/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6328e-04 - val_loss: 7.0058e-04\n",
      "Epoch 336/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6358e-04 - val_loss: 7.1387e-04\n",
      "Epoch 337/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6271e-04 - val_loss: 7.2226e-04\n",
      "Epoch 338/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6369e-04 - val_loss: 7.1550e-04\n",
      "Epoch 339/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6330e-04 - val_loss: 7.4271e-04\n",
      "Epoch 340/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6345e-04 - val_loss: 7.1878e-04\n",
      "Epoch 341/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6300e-04 - val_loss: 7.2164e-04\n",
      "Epoch 342/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6218e-04 - val_loss: 7.4753e-04\n",
      "Epoch 343/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6354e-04 - val_loss: 7.0505e-04\n",
      "Epoch 344/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6235e-04 - val_loss: 7.4751e-04\n",
      "Epoch 345/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6308e-04 - val_loss: 7.2203e-04\n",
      "Epoch 346/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6202e-04 - val_loss: 7.2394e-04\n",
      "Epoch 347/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 6.6215e-04 - val_loss: 7.3737e-04\n",
      "Epoch 348/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6245e-04 - val_loss: 7.3509e-04\n",
      "Epoch 349/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6117e-04 - val_loss: 7.3023e-04\n",
      "Epoch 350/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6051e-04 - val_loss: 7.3076e-04\n",
      "Epoch 351/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 6.6239e-04 - val_loss: 7.3773e-04\n",
      "Epoch 352/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 1s 29us/step - loss: 6.6144e-04 - val_loss: 7.3984e-04\n",
      "Epoch 353/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6165e-04 - val_loss: 7.3968e-04\n",
      "Epoch 354/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6217e-04 - val_loss: 7.2719e-04\n",
      "Epoch 355/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6136e-04 - val_loss: 7.4464e-04\n",
      "Epoch 356/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6162e-04 - val_loss: 7.2854e-04\n",
      "Epoch 357/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6140e-04 - val_loss: 7.4044e-04\n",
      "Epoch 358/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6121e-04 - val_loss: 7.3416e-04\n",
      "Epoch 359/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6154e-04 - val_loss: 7.4286e-04\n",
      "Epoch 360/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6207e-04 - val_loss: 7.3857e-04\n",
      "Epoch 361/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6089e-04 - val_loss: 7.3950e-04\n",
      "Epoch 362/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.6114e-04 - val_loss: 7.3654e-04\n",
      "Epoch 363/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6087e-04 - val_loss: 7.3681e-04\n",
      "Epoch 364/10000\n",
      "45507/45507 [==============================] - 2s 33us/step - loss: 6.6034e-04 - val_loss: 7.4516e-04\n",
      "Epoch 365/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.6088e-04 - val_loss: 7.3861e-04\n",
      "Epoch 366/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6073e-04 - val_loss: 7.1685e-04\n",
      "Epoch 367/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5962e-04 - val_loss: 7.3975e-04\n",
      "Epoch 368/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5976e-04 - val_loss: 7.3436e-04\n",
      "Epoch 369/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.6059e-04 - val_loss: 7.0884e-04\n",
      "Epoch 370/10000\n",
      "45507/45507 [==============================] - 1s 30us/step - loss: 6.5945e-04 - val_loss: 7.1191e-04\n",
      "Epoch 371/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6084e-04 - val_loss: 7.2388e-04\n",
      "Epoch 372/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.6016e-04 - val_loss: 7.1458e-04\n",
      "Epoch 373/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5968e-04 - val_loss: 7.3611e-04\n",
      "Epoch 374/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5900e-04 - val_loss: 7.3748e-04\n",
      "Epoch 375/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5895e-04 - val_loss: 7.1120e-04\n",
      "Epoch 376/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5966e-04 - val_loss: 6.9691e-04\n",
      "Epoch 377/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5877e-04 - val_loss: 7.0264e-04\n",
      "Epoch 378/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5902e-04 - val_loss: 7.2758e-04\n",
      "Epoch 379/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5877e-04 - val_loss: 7.1424e-04\n",
      "Epoch 380/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5871e-04 - val_loss: 7.3699e-04\n",
      "Epoch 381/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5937e-04 - val_loss: 7.1646e-04\n",
      "Epoch 382/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5856e-04 - val_loss: 7.0304e-04\n",
      "Epoch 383/10000\n",
      "45507/45507 [==============================] - 1s 31us/step - loss: 6.5815e-04 - val_loss: 7.0640e-04\n",
      "Epoch 384/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5838e-04 - val_loss: 7.0398e-04\n",
      "Epoch 385/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5822e-04 - val_loss: 7.1741e-04\n",
      "Epoch 386/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5826e-04 - val_loss: 7.1185e-04\n",
      "Epoch 387/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5764e-04 - val_loss: 7.2218e-04\n",
      "Epoch 388/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.5856e-04 - val_loss: 7.4513e-04\n",
      "Epoch 389/10000\n",
      "45507/45507 [==============================] - 1s 27us/step - loss: 6.5885e-04 - val_loss: 7.1682e-04\n",
      "Epoch 390/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5903e-04 - val_loss: 7.0383e-04\n",
      "Epoch 391/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5843e-04 - val_loss: 7.2041e-04\n",
      "Epoch 392/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5756e-04 - val_loss: 7.2135e-04\n",
      "Epoch 393/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5819e-04 - val_loss: 6.9925e-04\n",
      "Epoch 394/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5866e-04 - val_loss: 7.0815e-04\n",
      "Epoch 395/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5702e-04 - val_loss: 7.0119e-04\n",
      "Epoch 396/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5742e-04 - val_loss: 7.1284e-04\n",
      "Epoch 397/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5688e-04 - val_loss: 7.3300e-04\n",
      "Epoch 398/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5733e-04 - val_loss: 6.9242e-04\n",
      "Epoch 399/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5723e-04 - val_loss: 7.1742e-04\n",
      "Epoch 400/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5741e-04 - val_loss: 7.3249e-04\n",
      "Epoch 401/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5744e-04 - val_loss: 7.0207e-04\n",
      "Epoch 402/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5752e-04 - val_loss: 6.9436e-04\n",
      "Epoch 403/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5679e-04 - val_loss: 7.0826e-04\n",
      "Epoch 404/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5654e-04 - val_loss: 7.5129e-04\n",
      "Epoch 405/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5675e-04 - val_loss: 7.3510e-04\n",
      "Epoch 406/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5835e-04 - val_loss: 6.9534e-04\n",
      "Epoch 407/10000\n",
      "45507/45507 [==============================] - 1s 29us/step - loss: 6.5661e-04 - val_loss: 7.0847e-04\n",
      "Epoch 408/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5681e-04 - val_loss: 7.2993e-04\n",
      "Epoch 409/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5663e-04 - val_loss: 6.9866e-04\n",
      "Epoch 410/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5650e-04 - val_loss: 7.2089e-04\n",
      "Epoch 411/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5621e-04 - val_loss: 7.0933e-04\n",
      "Epoch 412/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5646e-04 - val_loss: 7.2407e-04\n",
      "Epoch 413/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5652e-04 - val_loss: 6.9402e-04\n",
      "Epoch 414/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5637e-04 - val_loss: 7.3014e-04\n",
      "Epoch 415/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5682e-04 - val_loss: 6.9070e-04\n",
      "Epoch 416/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5638e-04 - val_loss: 6.8954e-04\n",
      "Epoch 417/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5513e-04 - val_loss: 6.9093e-04\n",
      "Epoch 418/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5585e-04 - val_loss: 7.0775e-04\n",
      "Epoch 419/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5581e-04 - val_loss: 6.9657e-04\n",
      "Epoch 420/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.5573e-04 - val_loss: 7.1388e-04\n",
      "Epoch 421/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5557e-04 - val_loss: 7.0242e-04\n",
      "Epoch 422/10000\n",
      "45507/45507 [==============================] - 1s 32us/step - loss: 6.5577e-04 - val_loss: 7.2638e-04\n",
      "Epoch 423/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5498e-04 - val_loss: 7.0003e-04\n",
      "Epoch 424/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5434e-04 - val_loss: 6.9789e-04\n",
      "Epoch 425/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5457e-04 - val_loss: 6.9868e-04\n",
      "Epoch 426/10000\n",
      "45507/45507 [==============================] - 1s 25us/step - loss: 6.5467e-04 - val_loss: 7.0428e-04\n",
      "Epoch 427/10000\n",
      "45507/45507 [==============================] - 1s 26us/step - loss: 6.5551e-04 - val_loss: 7.2939e-04\n",
      "Epoch 428/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5555e-04 - val_loss: 7.0819e-04\n",
      "Epoch 429/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5494e-04 - val_loss: 7.0667e-04\n",
      "Epoch 430/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5486e-04 - val_loss: 7.0009e-04\n",
      "Epoch 431/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5491e-04 - val_loss: 7.0762e-04\n",
      "Epoch 432/10000\n",
      "45507/45507 [==============================] - 2s 35us/step - loss: 6.5419e-04 - val_loss: 7.0930e-04\n",
      "Epoch 433/10000\n",
      "45507/45507 [==============================] - 2s 34us/step - loss: 6.5502e-04 - val_loss: 7.1245e-04\n",
      "Epoch 434/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5427e-04 - val_loss: 7.3763e-04\n",
      "Epoch 435/10000\n",
      "45507/45507 [==============================] - 2s 37us/step - loss: 6.5516e-04 - val_loss: 6.9884e-04\n",
      "Epoch 436/10000\n",
      "45507/45507 [==============================] - 2s 36us/step - loss: 6.5520e-04 - val_loss: 7.1017e-04\n",
      "Epoch 437/10000\n",
      " 8600/45507 [====>.........................] - ETA: 1s - loss: 6.8621e-04"
     ]
    }
   ],
   "source": [
    "algoritmo='RMSprop'\n",
    "experimento=\"scaled_{}_encoder_without_bias_tanh_tanh_lr_{}\".format(supermax,factor_aprendizaje)\n",
    "tensorboard=TensorBoard(log_dir=\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/{}{}{}{}\".format(encoding_dim,algoritmo,experimento,datetime.now()))\n",
    "#modelCheckpoint=ModelCheckpoint(\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None)\n",
    "autoencoder.fit(x_train_scaled, x_train_scaled,\n",
    "                epochs=10000,\n",
    "                batch_size=200,\n",
    "                shuffle=False,\n",
    "                callbacks=[tensorboard, early_stop],\n",
    "                validation_data=(x_test_scaled, x_test_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(x=x_test_scaled,y=x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights('../redes_compresoras/compresor_python_{}{}{}{}'.format(encoding_dim,algoritmo,experimento,datetime.now()))\n",
    "#np.savez('../redes_compresoras/maxmin_python_ver_rms_prop_scaled_min_max_ver2', min_max_scaler.data_max_, min_max_scaler.data_min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scores = encoder.predict(x_test_scaled).ravel()\n",
    "#regularized_scores = encoded_regularized.predict(x_test).ravel()\n",
    "sns.distplot(standard_scores, hist=True, label='standard model')\n",
    "#sns.distplot(regularized_scores, hist=False, label='regularized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some images\n",
    "# note that we take them from the *test* set\n",
    "# encoded_imgs = encoder.predict(x_test_min_max)\n",
    "# decoded_imgs_scaled = decoder.predict(encoded_imgs)\n",
    "#decoded_imgs_scaled = autoencoder.predict(x_test_min_max)\n",
    "decoded_imgs_scaled = autoencoder.predict(x_test_scaled)\n",
    "decoded_imgs = supermax*(decoded_imgs_scaled+1)/2\n",
    "#decoded_imgs = min_max_scaler.inverse_transform(decoded_imgs_scaled)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_test.shape[0])\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[idea].reshape(40, 16).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[idea].reshape(40, 16).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print(idea)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = '../datos_octubre_2018/p_OF_5mm_161mm003.h5'\n",
    "conjunto_datos_test=pd.read_hdf(filename,'MC');\n",
    "conjunto_datos_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1A=6;\n",
    "# hay tres L1 con 640 sensores (40*16)\n",
    "L1B=0;\n",
    "# hay dos L1 con 640 sensores (40*16)\n",
    "X_trained=conjunto_datos_test.values;\n",
    "x_trained=X_trained;\n",
    "\n",
    "for i in range (X_trained.shape[0]):\n",
    "    idea1=X_trained[i,:].reshape(img_rows,(L1A*img_cols));\n",
    "    ideat=idea1.transpose();\n",
    "    idea2=ideat.reshape(1,(L1A*img_cols)*img_rows);\n",
    "    x_trained[i,:] =idea2;\n",
    "x_tested = x_trained;\n",
    "print(x_trained.shape)\n",
    "print(x_tested.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a procesar y comprimir con la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora los particionamos y pasamos por las redes de compresión. Hay una red la A que se utiliza 5 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "\n",
    "cara_externa=x_tested[:,0: L1A*input_output_dim_A] \n",
    "cara_externa_reconstruida=np.zeros((x_tested.shape[0],L1A*input_output_dim_A))\n",
    "for i in range(x_tested.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_tested[i,k*input_output_dim_A:k*input_output_dim_A+input_output_dim_A]\n",
    "    #ideaA_scaled=min_max_scaler.transform(ideaA)\n",
    "    ideaA_scaled=(2*ideaA/(supermax)) -1\n",
    "    salida_reconstructed_1_scaled = autoencoder.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled+1)/2\n",
    "    #salida_reconstructed_1 = min_max_scaler.inverse_transform(salida_reconstructed_1_scaled)     \n",
    "    #salida_reconstructed_1 = ideaA\n",
    "    \n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "    #entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "    #encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "    #decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "    #print(decoded_imgs_A.shape)\n",
    "    #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "    #salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    " \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "\n",
    "    #print(hola.shape)\n",
    "    salida_total=hola1\n",
    "    #salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos todos los sensores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 1  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_tested.shape[0])\n",
    "    idea=1890\n",
    "    idea= 4299\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_cols, img_rows).transpose(), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_cols, img_rows).transpose(), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos ahora L1 a L1, teniendo en cuenta que hay de dos tipos:\n",
    "L1A (con 36 columnas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = L1A  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_cols, img_rows).transpose()[:,i*img_cols:(i+1)*img_cols] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_cols, img_rows).transpose()[:,i*img_cols:(i+1)*img_cols] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "print(cara_externa[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:])\n",
    "print(np.sum(cara_externa[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cara_externa_reconstruida[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:].astype(int))\n",
    "print(np.sum(cara_externa_reconstruida[idea].reshape(L1A*img_cols,img_rows)[i*img_cols:(i+1)*img_cols,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idea)\n",
    "np.sum(cara_externa_reconstruida,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamos_energia=(np.sum(cara_externa_reconstruida, axis=1))-(np.sum(cara_externa, axis=1))\n",
    "n, bins, patches = plt.hist(veamos_energia, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=200, cen=0, wid=100)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "1249px",
    "right": "57px",
    "top": "240px",
    "width": "390px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
