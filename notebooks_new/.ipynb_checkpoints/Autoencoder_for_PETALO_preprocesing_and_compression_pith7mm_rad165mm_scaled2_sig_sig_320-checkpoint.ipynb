{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AUTOENCODER for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en matlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import hdf5storage\n",
    "datos_matlab = hdf5storage.loadmat('../datos_junio_2019/conjunto_entrenamiento_junio_2019_pitch7mm_rad165mm_29_total.mat')\n",
    "conjunto_datos= datos_matlab.get('photodefbox2_todo')\n",
    "conjunto_datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "# hay dos L1 con 640 sensores (40*16)\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "numero_muestras=conjunto_datos.shape[0]\n",
    "print(numero_muestras)\n",
    "print('conjunto_datos shape:', conjunto_datos.shape)\n",
    "\n",
    "tr_size=10\n",
    "val_size=80\n",
    "test_size=100-val_size-tr_size\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "X_train=conjunto_datos[:tamanyo_tr,:]\n",
    "X_val=conjunto_datos[tamanyo_tr:tamanyo_tr+tamanyo_val,:]\n",
    "X_test=conjunto_datos[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "x_train = X_train\n",
    "x_test = X_test\n",
    "\n",
    "datos_trA=np.zeros((X_train.shape[0]*L1A,input_output_dim_A))\n",
    "for i in range(x_train.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_train[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    datos_trA[(i)*L1A :(i+1)*L1A,:] = ideaA  \n",
    " \n",
    "x_trainA = datos_trA;\n",
    "\n",
    "datos_testA=np.zeros((X_test.shape[0]*L1A,input_output_dim_A))\n",
    "for i in range(x_test.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_test[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    datos_testA[(i)*L1A :(i+1)*L1A,:] = ideaA  \n",
    " \n",
    "x_testA = datos_testA;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x_trainA.shape)\n",
    "print(x_testA.shape)\n",
    "\n",
    "\n",
    "datos_trB=np.zeros((X_train.shape[0]*L1B,input_output_dim_B))\n",
    "for i in range(x_train.shape[0]):\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:]=x_train[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+(k+1)*input_output_dim_B]\n",
    "    datos_trB[(i)*L1B :(i+1)*L1B,:] = ideaB  \n",
    " \n",
    "x_trainB = datos_trB;\n",
    "\n",
    "datos_testB=np.zeros((X_test.shape[0]*L1B,input_output_dim_B))\n",
    "for i in range(x_test.shape[0]):\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:]=x_test[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+(k+1)*input_output_dim_B]\n",
    "    datos_testB[(i)*L1B :(i+1)*L1B,:] = ideaB  \n",
    " \n",
    "x_testB = datos_testB;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x_trainB.shape)\n",
    "print(x_testB.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_regularizer = True\n",
    "my_regularizer = None\n",
    "my_epochs = 50\n",
    "hidden_size=320\n",
    "features_path = 'simple_autoe_features.pickle'\n",
    "labels_path = 'simple_autoe_labels.pickle'\n",
    "\n",
    "if use_regularizer:\n",
    "    # add a sparsity constraint on the encoded representations\n",
    "    # note use of 10e-5 leads to blurred results\n",
    "    my_regularizer = regularizers.l2(0.001)\n",
    "    # and a larger number of epochs as the added regularization the model\n",
    "    # is less likely to overfit and can be trained longer\n",
    "    my_epochs = 100\n",
    "    features_path = 'sparse_autoe_features.pickle'\n",
    "    labels_path = 'sparse_autoe_labels.pickle'\n",
    "\n",
    "   \n",
    "    \n",
    "encoding_dimA = hidden_size  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "\n",
    "input_imgA = Input(shape=(img_rows*img_colsA,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encodedA = Dense(encoding_dimA, activation='sigmoid', use_bias=False,bias_initializer='random_uniform')(input_imgA)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decodedA = Dense(img_colsA*img_rows, activation='linear',use_bias=True,bias_initializer='random_uniform')(encodedA)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "\n",
    "\n",
    "#autoencoder=Sequential([\n",
    "#    Dense(encoding_dim, kernel_regularizer=regularizers.l2(0.001), use_bias=True,bias_initializer='random_uniform',input_shape=(640,)),\n",
    "#    Activation('sigmoid'),\n",
    "#    Dense(img_cols*img_rows, use_bias=True,bias_initializer='random_uniform'),\n",
    "#    Activation('linear'),\n",
    "#])\n",
    "\n",
    "autoencoderA = Model(input_imgA, decodedA)\n",
    "\n",
    "encoding_dimB = hidden_size  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "\n",
    "input_imgB = Input(shape=(img_rows*img_colsB,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encodedB = Dense(encoding_dimB, activation='sigmoid', use_bias=False,bias_initializer='random_uniform')(input_imgB)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decodedB = Dense(img_colsB*img_rows, activation='linear',use_bias=True,bias_initializer='random_uniform')(encodedB)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "\n",
    "\n",
    "#autoencoder=Sequential([\n",
    "#    Dense(encoding_dim, kernel_regularizer=regularizers.l2(0.001), use_bias=True,bias_initializer='random_uniform',input_shape=(640,)),\n",
    "#    Activation('sigmoid'),\n",
    "#    Dense(img_cols*img_rows, use_bias=True,bias_initializer='random_uniform'),\n",
    "#    Activation('linear'),\n",
    "#])\n",
    "\n",
    "autoencoderB = Model(input_imgB, decodedB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainA = x_trainA.reshape(x_trainA.shape[0], img_colsA, img_rows,1)\n",
    "# X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
    "x_testA = x_testA.reshape(x_testA.shape[0], img_colsA, img_rows,1)\n",
    "\n",
    "\n",
    "input_shapeA = (img_colsA, img_rows,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainA = x_trainA.astype('float32')\n",
    "# X_val= X_val.astype('float32')\n",
    "x_testA = x_testA.astype('float32')\n",
    "\n",
    "print('X_train shape:', x_trainA.shape)\n",
    "# print(X_train.shape[0], 'train samples')\n",
    "# print(X_val.shape[0], 'validation samples')\n",
    "\n",
    "X_train=x_trainA\n",
    "X_test=x_testA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,X_train.shape[0])\n",
    "    plt.imshow(np.reshape(X_train[idea].transpose(), [img_rows, img_colsA]), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    plt.show()\n",
    "    print(idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar las matrices de datos para la red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "x_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = preprocessing.QuantileTransformer().fit(x_train)\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler().fit(x_train)\n",
    "# min_max_scaler = preprocessing.StandardScaler(with_mean=False).fit(x_train)\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(x_train)\n",
    "#min_max_scaler = preprocessing.RobustScaler().fit(x_train)\n",
    "supermax=1\n",
    "factor_aprendizaje=0.0001\n",
    "print(min_max_scaler)\n",
    "#x_train_scaled = min_max_scaler.transform(x_train)\n",
    "#x_test_scaled = min_max_scaler.transform(x_test)\n",
    "x_train_scaled=(x_train/supermax)\n",
    "x_test_scaled=(x_test/supermax)\n",
    "#min_max_scaler.scale_\n",
    "# print(x_train[20413])\n",
    "# print(x_train_scaled[20413])\n",
    "# print(x_test[20413])\n",
    "# print(x_test_scaled[20413])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our metrics, for example energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as KK\n",
    "import keras.callbacks as KKcall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(KKcall.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.asarray(self.model.predict(X_val))\n",
    "\n",
    "        y_val = np.sum((y_val+1)*supermax/2, axis=1)\n",
    "        y_predict = np.sum((y_predict+1)*supermax/2, axis=1)\n",
    "\n",
    "        self._data.append({\n",
    "            'val_energy': np.mean(y_predict-y_val),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "def energy_error(y_true, y_pred):\n",
    "    veamos_energia=(KK.sum(y_pred, axis=1)-KK.sum(y_true,axis=1))\n",
    "    return KK.mean(veamos_energia,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoderA = Model(input_imgA, encodedA)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_inputA = Input(shape=(encoding_dimA,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layerA = autoencoderA.layers[-1]\n",
    "# create the decoder model\n",
    "decoderA = Model(encoded_inputA, decoder_layerA(encoded_inputA))\n",
    "\n",
    "autoencoderA.compile(optimizer='RMSprop', loss='mse', metrics=[energy_error])\n",
    "\n",
    "autoencoderA.optimizer.lr=(factor_aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algoritmo='RMSprop'\n",
    "experimento=\"scaledA_{}_encoder_without_bias_sig_sig_lr_{}\".format(supermax,factor_aprendizaje)\n",
    "tensorboard=TensorBoard(log_dir=\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/defs/{}{}{}{}\".format(encoding_dimA,algoritmo,experimento,datetime.now()))\n",
    "#modelCheckpoint=ModelCheckpoint(\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None)\n",
    "results=autoencoderA.fit(x_train_scaled, x_train_scaled,\n",
    "                epochs=100,\n",
    "                batch_size=200,\n",
    "                shuffle=False,\n",
    "                callbacks=[tensorboard, early_stop, metrics],\n",
    "                validation_data=(x_test_scaled, x_test_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for categorical accuracy\n",
    "plt.plot(results.history['energy_error'])\n",
    "plt.plot(results.history['val_energy_error'])\n",
    "plt.title('Energy error')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# autoencoder.evaluate(x=x_test_scaled,y=x_test_scaled)\n",
    "# D=metrics.get_data()\n",
    "\n",
    "\n",
    "# energies=pd.DataFrame(D).values.reshape(len(D))\n",
    "# valores=len(energies)\n",
    "# plt.plot(range(valores),energies) \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderA.save_weights('../redes_compresoras/defs/compresor_python_{}{}{}{}'.format(encoding_dimA,algoritmo,experimento,datetime.now()))\n",
    "#np.savez('../redes_compresoras/maxmin_python_ver_rms_prop_scaled_min_max_ver2', min_max_scaler.data_max_, min_max_scaler.data_min_)\n",
    "#autoencoder.load_weights('../redes_compresoras/defs/compresor_python_320RMSpropscaled_100_encoder_without_bias_sig_sig_lr_0.00012018-11-03 09:43:55.047213')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scores = encoder.predict(x_test_scaled).ravel()\n",
    "# #regularized_scores = encoded_regularized.predict(x_test).ravel()\n",
    "# sns.distplot(standard_scores, hist=True, label='standard model')\n",
    "# #sns.distplot(regularized_scores, hist=False, label='regularized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some images\n",
    "# note that we take them from the *test* set\n",
    "# encoded_imgs = encoder.predict(x_test_min_max)\n",
    "# decoded_imgs_scaled = decoder.predict(encoded_imgs)\n",
    "#decoded_imgs_scaled = autoencoder.predict(x_test_min_max)\n",
    "decoded_imgs_scaled = autoencoderA.predict(x_test_scaled)\n",
    "decoded_imgs = supermax*(decoded_imgs_scaled)\n",
    "#decoded_imgs = min_max_scaler.inverse_transform(decoded_imgs_scaled)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_test.shape[0])\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[idea].reshape(img_colsA,img_rows ).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[idea].reshape(img_colsA, img_rows).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print(idea)\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainB = x_trainB.reshape(x_trainB.shape[0], img_colsB, img_rows,1)\n",
    "# X_val = X_val.reshape(X_val.shape[0], img_cols, img_rows,1)\n",
    "x_testB = x_testB.reshape(x_testB.shape[0], img_colsB, img_rows,1)\n",
    "\n",
    "\n",
    "input_shapeB = (img_colsB, img_rows,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainB = x_trainB.astype('float32')\n",
    "# X_val= X_val.astype('float32')\n",
    "x_testB = x_testB.astype('float32')\n",
    "\n",
    "print('X_train shape:', x_trainB.shape)\n",
    "# print(X_train.shape[0], 'train samples')\n",
    "# print(X_val.shape[0], 'validation samples')\n",
    "\n",
    "X_train=x_trainB\n",
    "X_test=x_testB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,X_train.shape[0])\n",
    "    plt.imshow(np.reshape(X_train[idea].transpose(), [img_rows, img_colsB]), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    plt.show()\n",
    "    print(idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar las matrices de datos para la red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "x_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = preprocessing.QuantileTransformer().fit(x_train)\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler().fit(x_train)\n",
    "# min_max_scaler = preprocessing.StandardScaler(with_mean=False).fit(x_train)\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(x_train)\n",
    "#min_max_scaler = preprocessing.RobustScaler().fit(x_train)\n",
    "supermax=1\n",
    "factor_aprendizaje=0.0001\n",
    "print(min_max_scaler)\n",
    "# x_train_scaled = min_max_scaler.transform(x_train)\n",
    "# x_test_scaled = min_max_scaler.transform(x_test)\n",
    "x_train_scaled=(x_train/supermax)\n",
    "x_test_scaled=(x_test/supermax)\n",
    "#min_max_scaler.scale_\n",
    "# print(x_train[2413])\n",
    "# print(x_train_scaled[2413])\n",
    "# print(x_test[2413])\n",
    "# print(x_test_scaled[2413])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our metrics, for example energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as KK\n",
    "import keras.callbacks as KKcall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(KKcall.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.asarray(self.model.predict(X_val))\n",
    "\n",
    "        y_val = np.sum((y_val+1)*supermax/2, axis=1)\n",
    "        y_predict = np.sum((y_predict+1)*supermax/2, axis=1)\n",
    "\n",
    "        self._data.append({\n",
    "            'val_energy': np.mean(y_predict-y_val),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "def energy_error(y_true, y_pred):\n",
    "    veamos_energia=(KK.sum(y_pred, axis=1)-KK.sum(y_true,axis=1))\n",
    "    return KK.mean(veamos_energia,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoderB = Model(input_imgB, encodedB)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_inputB = Input(shape=(encoding_dimB,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layerB = autoencoderB.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_inputB, decoder_layerB(encoded_inputB))\n",
    "\n",
    "autoencoderB.compile(optimizer='RMSprop', loss='mse', metrics=[energy_error])\n",
    "\n",
    "autoencoderB.optimizer.lr=(factor_aprendizaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algoritmo='RMSprop'\n",
    "experimento=\"scaledB_{}_encoder_without_bias_sig_sig_lr_{}\".format(supermax,factor_aprendizaje)\n",
    "tensorboard=TensorBoard(log_dir=\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/defs/{}{}{}{}\".format(encoding_dimB,algoritmo,experimento,datetime.now()))\n",
    "#modelCheckpoint=ModelCheckpoint(\"/home/rgadea3/EXPERIMENTOS/nuevas_investigaciones_2018/experimentos/logs/\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=500, verbose=2, mode='auto', baseline=None)\n",
    "results=autoencoderB.fit(x_train_scaled, x_train_scaled,\n",
    "                epochs=500,\n",
    "                batch_size=200,\n",
    "                shuffle=False,\n",
    "                callbacks=[tensorboard, early_stop, metrics],\n",
    "                validation_data=(x_test_scaled, x_test_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for categorical accuracy\n",
    "plt.plot(results.history['energy_error'])\n",
    "plt.plot(results.history['val_energy_error'])\n",
    "plt.title('Energy error')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# autoencoderB.evaluate(x=x_test_scaled,y=x_test_scaled)\n",
    "# D=metrics.get_data()\n",
    "\n",
    "\n",
    "# energies=pd.DataFrame(D).values.reshape(len(D))\n",
    "# valores=len(energies)\n",
    "# plt.plot(range(valores),energies) \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderB.save_weights('../redes_compresoras/defs/compresor_python_{}{}{}{}'.format(encoding_dimB,algoritmo,experimento,datetime.now()))\n",
    "#np.savez('../redes_compresoras/maxmin_python_ver_rms_prop_scaled_min_max_ver2', min_max_scaler.data_max_, min_max_scaler.data_min_)\n",
    "#autoencoder.load_weights('../redes_compresoras/defs/compresor_python_320RMSpropscaled_100_encoder_without_bias_sig_sig_lr_0.00012018-11-03 09:43:55.047213')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scores = encoder.predict(x_test_scaled).ravel()\n",
    "# #regularized_scores = encoded_regularized.predict(x_test).ravel()\n",
    "# sns.distplot(standard_scores, hist=True, label='standard model')\n",
    "# #sns.distplot(regularized_scores, hist=False, label='regularized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some images\n",
    "# note that we take them from the *test* set\n",
    "# encoded_imgs = encoder.predict(x_test_min_max)\n",
    "# decoded_imgs_scaled = decoder.predict(encoded_imgs)\n",
    "# decoded_imgs_scaled = autoencoderB.predict(x_test_min_max)\n",
    "decoded_imgs_scaledB = autoencoderB.predict(x_test_scaled)\n",
    "decoded_imgs = supermax*(decoded_imgs_scaledB)\n",
    "# decoded_imgs = min_max_scaler.inverse_transform(decoded_imgs_scaled)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_test.shape[0])\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[idea].reshape(img_colsB,img_rows ).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[idea].reshape(img_colsB, img_rows).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "# print(idea)\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a procesar y comprimir con la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora los particionamos y pasamos por las redes de compresión. Hay una red la A que se utiliza 5 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total=conjunto_datos[0:numero_muestras,:]\n",
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "cara_externa=x_total[:,6: 6+L1A*input_output_dim_A+L1B*input_output_dim_B] \n",
    "cara_externa_reconstruida=np.zeros((x_total.shape[0],L1A*input_output_dim_A+L1B*input_output_dim_B))\n",
    "for i in range(x_total.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_total[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "#     entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "#     encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "#     decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "#     #print(decoded_imgs_A.shape)\n",
    "#     #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "#     salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    "\n",
    "\n",
    "    ideaA_scaled=(ideaA/(supermax))\n",
    "    salida_reconstructed_1_scaled = autoencoderA.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled)\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:input_output_dim_B]=x_total[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+k*input_output_dim_B+input_output_dim_B]\n",
    "    #entrada_imgs_B=(ideaB-min_B.transpose())/(max_B.transpose()-min_B.transpose())\n",
    "#     entrada_imgs_B=(ideaB)  #quito el escalado\n",
    "#     encoded_imgs_B = sigmoid(np.dot(entrada_imgs_B, Encoder_weights_B) + Encoder_biases_B)\n",
    "#     decoded_imgs_B= (np.dot(encoded_imgs_B, Decoder_weights_B) + Decoder_biases_B)\n",
    "#     salida_reconstructed_2 = decoded_imgs_B #quito el escalado inverso  \n",
    "    ideaB_scaled=(ideaB/(supermax))\n",
    "    salida_reconstructed_2_scaled = autoencoderB.predict(ideaB_scaled)    \n",
    "    salida_reconstructed_2 = supermax*(salida_reconstructed_2_scaled)\n",
    "    #salida_reconstructed_2 = decoded_imgs_B*(max_B.transpose()-min_B.transpose())+min_B.transpose()\n",
    "  \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "    hola2=np.reshape(salida_reconstructed_2,(L1B*input_output_dim_B))\n",
    "    #print(hola.shape)\n",
    "    salida_total=np.concatenate((hola1,hola2))\n",
    "    salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "    \n",
    "mse = (np.square(cara_externa - cara_externa_reconstruida)).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora ciertos estudios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodefbox2_todo_fil1= datos_matlab.get('photodefbox2_todo_fil1')\n",
    "numero_muestras=photodefbox2_todo_fil1.shape[0]\n",
    "x_total=photodefbox2_todo_fil1[0:numero_muestras,:]\n",
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "cara_externa=x_total[:,6: 6+L1A*input_output_dim_A+L1B*input_output_dim_B] \n",
    "cara_externa_reconstruida=np.zeros((x_total.shape[0],L1A*input_output_dim_A+L1B*input_output_dim_B))\n",
    "for i in range(x_total.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_total[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "#     entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "#     encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "#     decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "#     #print(decoded_imgs_A.shape)\n",
    "#     #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "#     salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    "\n",
    "\n",
    "    ideaA_scaled=(ideaA/(supermax))\n",
    "    salida_reconstructed_1_scaled = autoencoderA.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled)\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:input_output_dim_B]=x_total[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+k*input_output_dim_B+input_output_dim_B]\n",
    "    #entrada_imgs_B=(ideaB-min_B.transpose())/(max_B.transpose()-min_B.transpose())\n",
    "#     entrada_imgs_B=(ideaB)  #quito el escalado\n",
    "#     encoded_imgs_B = sigmoid(np.dot(entrada_imgs_B, Encoder_weights_B) + Encoder_biases_B)\n",
    "#     decoded_imgs_B= (np.dot(encoded_imgs_B, Decoder_weights_B) + Decoder_biases_B)\n",
    "#     salida_reconstructed_2 = decoded_imgs_B #quito el escalado inverso  \n",
    "    ideaB_scaled=(ideaB/(supermax))\n",
    "    salida_reconstructed_2_scaled = autoencoderB.predict(ideaB_scaled)    \n",
    "    salida_reconstructed_2 = supermax*(salida_reconstructed_2_scaled)\n",
    "    #salida_reconstructed_2 = decoded_imgs_B*(max_B.transpose()-min_B.transpose())+min_B.transpose()\n",
    "  \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "    hola2=np.reshape(salida_reconstructed_2,(L1B*input_output_dim_B))\n",
    "    #print(hola.shape)\n",
    "    salida_total=np.concatenate((hola1,hola2))\n",
    "    salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "    \n",
    "mse = (np.square(cara_externa - cara_externa_reconstruida)).mean(axis=1)\n",
    "mse.shape[0]\n",
    "mse_idea=np.reshape(mse,(mse.shape[0],1))\n",
    "photodefbox2_todo_fil1_e=np.concatenate((photodefbox2_todo_fil1,mse_idea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodefbox2_todo_fil2= datos_matlab.get('photodefbox2_todo_fil2')\n",
    "numero_muestras=photodefbox2_todo_fil2.shape[0]\n",
    "x_total=photodefbox2_todo_fil2[0:numero_muestras,:]\n",
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "cara_externa=x_total[:,6: 6+L1A*input_output_dim_A+L1B*input_output_dim_B] \n",
    "cara_externa_reconstruida=np.zeros((x_total.shape[0],L1A*input_output_dim_A+L1B*input_output_dim_B))\n",
    "for i in range(x_total.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_total[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "#     entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "#     encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "#     decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "#     #print(decoded_imgs_A.shape)\n",
    "#     #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "#     salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    "\n",
    "\n",
    "    ideaA_scaled=(ideaA/(supermax))\n",
    "    salida_reconstructed_1_scaled = autoencoderA.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled)\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:input_output_dim_B]=x_total[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+k*input_output_dim_B+input_output_dim_B]\n",
    "    #entrada_imgs_B=(ideaB-min_B.transpose())/(max_B.transpose()-min_B.transpose())\n",
    "#     entrada_imgs_B=(ideaB)  #quito el escalado\n",
    "#     encoded_imgs_B = sigmoid(np.dot(entrada_imgs_B, Encoder_weights_B) + Encoder_biases_B)\n",
    "#     decoded_imgs_B= (np.dot(encoded_imgs_B, Decoder_weights_B) + Decoder_biases_B)\n",
    "#     salida_reconstructed_2 = decoded_imgs_B #quito el escalado inverso  \n",
    "    ideaB_scaled=(ideaB/(supermax))\n",
    "    salida_reconstructed_2_scaled = autoencoderB.predict(ideaB_scaled)    \n",
    "    salida_reconstructed_2 = supermax*(salida_reconstructed_2_scaled)\n",
    "    #salida_reconstructed_2 = decoded_imgs_B*(max_B.transpose()-min_B.transpose())+min_B.transpose()\n",
    "  \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "    hola2=np.reshape(salida_reconstructed_2,(L1B*input_output_dim_B))\n",
    "    #print(hola.shape)\n",
    "    salida_total=np.concatenate((hola1,hola2))\n",
    "    salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "    \n",
    "mse = (np.square(cara_externa - cara_externa_reconstruida)).mean(axis=1)\n",
    "mse.shape[0]\n",
    "mse_idea=np.reshape(mse,(mse.shape[0],1))\n",
    "photodefbox2_todo_fil2_e=np.concatenate((photodefbox2_todo_fil2,mse_idea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodefbox2_todo_fil3= datos_matlab.get('photodefbox2_todo_fil3')\n",
    "numero_muestras=photodefbox2_todo_fil3.shape[0]\n",
    "x_total=photodefbox2_todo_fil3[0:numero_muestras,:]\n",
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "cara_externa=x_total[:,6: 6+L1A*input_output_dim_A+L1B*input_output_dim_B] \n",
    "cara_externa_reconstruida=np.zeros((x_total.shape[0],L1A*input_output_dim_A+L1B*input_output_dim_B))\n",
    "for i in range(x_total.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_total[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "#     entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "#     encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "#     decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "#     #print(decoded_imgs_A.shape)\n",
    "#     #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "#     salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    "\n",
    "\n",
    "    ideaA_scaled=(ideaA/(supermax))\n",
    "    salida_reconstructed_1_scaled = autoencoderA.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled)\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:input_output_dim_B]=x_total[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+k*input_output_dim_B+input_output_dim_B]\n",
    "    #entrada_imgs_B=(ideaB-min_B.transpose())/(max_B.transpose()-min_B.transpose())\n",
    "#     entrada_imgs_B=(ideaB)  #quito el escalado\n",
    "#     encoded_imgs_B = sigmoid(np.dot(entrada_imgs_B, Encoder_weights_B) + Encoder_biases_B)\n",
    "#     decoded_imgs_B= (np.dot(encoded_imgs_B, Decoder_weights_B) + Decoder_biases_B)\n",
    "#     salida_reconstructed_2 = decoded_imgs_B #quito el escalado inverso  \n",
    "    ideaB_scaled=(ideaB/(supermax))\n",
    "    salida_reconstructed_2_scaled = autoencoderB.predict(ideaB_scaled)    \n",
    "    salida_reconstructed_2 = supermax*(salida_reconstructed_2_scaled)\n",
    "    #salida_reconstructed_2 = decoded_imgs_B*(max_B.transpose()-min_B.transpose())+min_B.transpose()\n",
    "  \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "    hola2=np.reshape(salida_reconstructed_2,(L1B*input_output_dim_B))\n",
    "    #print(hola.shape)\n",
    "    salida_total=np.concatenate((hola1,hola2))\n",
    "    salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "    \n",
    "mse = (np.square(cara_externa - cara_externa_reconstruida)).mean(axis=1)\n",
    "mse.shape[0]\n",
    "mse_idea=np.reshape(mse,(mse.shape[0],1))\n",
    "photodefbox2_todo_fil3_e=np.concatenate((photodefbox2_todo_fil3,mse_idea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodefbox2_todo_fil4= datos_matlab.get('photodefbox2_todo_fil4')\n",
    "numero_muestras=photodefbox2_todo_fil4.shape[0]\n",
    "x_total=photodefbox2_todo_fil4[0:numero_muestras,:]\n",
    "L1A=5;\n",
    "# hay tres L1 con 576 sensores (36*16)\n",
    "L1B=1;\n",
    "img_rows=20\n",
    "img_colsA=30\n",
    "img_colsB=25\n",
    "img_cols=30\n",
    "input_output_dim_A=img_rows*img_colsA\n",
    "input_output_dim_B=img_rows*img_colsB\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "ideaA=np.zeros((L1A,input_output_dim_A))\n",
    "ideaB=np.zeros((L1B,input_output_dim_B)) \n",
    "cara_externa=x_total[:,6: 6+L1A*input_output_dim_A+L1B*input_output_dim_B] \n",
    "cara_externa_reconstruida=np.zeros((x_total.shape[0],L1A*input_output_dim_A+L1B*input_output_dim_B))\n",
    "for i in range(x_total.shape[0]):\n",
    "    for k in range(L1A):\n",
    "        ideaA[k,:]=x_total[i,6+k*input_output_dim_A:6+k*input_output_dim_A+input_output_dim_A]\n",
    "    #entrada_imgs_A=(ideaA-min_A.transpose())/(max_A.transpose()-min_A.transpose())\n",
    "#     entrada_imgs_A=(ideaA) #he quitado el escalado\n",
    "#     encoded_imgs_A = sigmoid(np.dot(entrada_imgs_A, Encoder_weights_A) + Encoder_biases_A)\n",
    "#     decoded_imgs_A= (np.dot(encoded_imgs_A, Decoder_weights_A) + Decoder_biases_A)\n",
    "#     #print(decoded_imgs_A.shape)\n",
    "#     #salida_reconstructed_1 = decoded_imgs_A*(max_A.transpose()-min_A.transpose())+min_A.transpose();\n",
    "#     salida_reconstructed_1 = decoded_imgs_A #quito el escalado inverso    \n",
    "\n",
    "\n",
    "    ideaA_scaled=(ideaA/(supermax))\n",
    "    salida_reconstructed_1_scaled = autoencoderA.predict(ideaA_scaled)    \n",
    "    salida_reconstructed_1 = supermax*(salida_reconstructed_1_scaled)\n",
    "    for k in range(L1B):\n",
    "        ideaB[k,:input_output_dim_B]=x_total[i,6+L1A*input_output_dim_A+k*input_output_dim_B:6+L1A*input_output_dim_A+k*input_output_dim_B+input_output_dim_B]\n",
    "    #entrada_imgs_B=(ideaB-min_B.transpose())/(max_B.transpose()-min_B.transpose())\n",
    "#     entrada_imgs_B=(ideaB)  #quito el escalado\n",
    "#     encoded_imgs_B = sigmoid(np.dot(entrada_imgs_B, Encoder_weights_B) + Encoder_biases_B)\n",
    "#     decoded_imgs_B= (np.dot(encoded_imgs_B, Decoder_weights_B) + Decoder_biases_B)\n",
    "#     salida_reconstructed_2 = decoded_imgs_B #quito el escalado inverso  \n",
    "    ideaB_scaled=(ideaB/(supermax))\n",
    "    salida_reconstructed_2_scaled = autoencoderB.predict(ideaB_scaled)    \n",
    "    salida_reconstructed_2 = supermax*(salida_reconstructed_2_scaled)\n",
    "    #salida_reconstructed_2 = decoded_imgs_B*(max_B.transpose()-min_B.transpose())+min_B.transpose()\n",
    "  \n",
    "    hola1=np.reshape(salida_reconstructed_1,(L1A*input_output_dim_A))\n",
    "    hola2=np.reshape(salida_reconstructed_2,(L1B*input_output_dim_B))\n",
    "    #print(hola.shape)\n",
    "    salida_total=np.concatenate((hola1,hola2))\n",
    "    salida_total[salida_total<0]=0\n",
    "    #print(salida_total.shape)\n",
    "    cara_externa_reconstruida[i]=salida_total\n",
    "    \n",
    "mse = (np.square(cara_externa - cara_externa_reconstruida)).mean(axis=1)\n",
    "mse.shape[0]\n",
    "mse_idea=np.reshape(mse,(mse.shape[0],1))\n",
    "photodefbox2_todo_fil4_e=np.concatenate((photodefbox2_todo_fil4,mse_idea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamosd1 =np.concatenate((photodefbox2_todo_fil1_e[:,-9],photodefbox2_todo_fil2_e[:,-9]),axis=0);\n",
    "# veamosd2 =[photodefbox2_todo_fil3_e(:,end-3);photodefbox2_todo_fil4_e(:,end-3)];\n",
    "veamosen1 =np.concatenate((photodefbox2_todo_fil1_e[:,-11],photodefbox2_todo_fil2_e[:,-11]),axis=0);\n",
    "veamoser1=np.concatenate((photodefbox2_todo_fil1_e[:,-1],photodefbox2_todo_fil2_e[:,-1]),axis=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora los histogramnas 2d que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist2d(veamosen1, veamosd1, bins=100)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 8))\n",
    "h1=plt.hist(veamoser1, bins=100, range=(0,10))\n",
    "fig.colorbar(h1[3])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 8))\n",
    "h2=plt.hist2d(veamoser1,veamosd1, bins=100, range=[(0,1),(0,1.2)])\n",
    "fig.colorbar(h2[3])\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 8))\n",
    "h3=plt.hist2d(veamoser1,veamosen1, bins=100, range=[(0,1),(0,0.5)])\n",
    "fig.colorbar(h3[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos todos los sensores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 1  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    idea=np.random.randint(1,x_test.shape[0])\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose(),vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose(), vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos ahora L1 a L1, teniendo en cuenta que hay de dos tipos:\n",
    "L1A (con 36 columnas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = L1A  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose()[:,i*img_colsA:(i+1)*img_colsA] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose()[:,i*img_colsA:(i+1)*img_colsA] ,vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = L1B  # how many digits we will display\n",
    "plt.figure(figsize=(40, 10))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(cara_externa[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose()[:,L1A*img_colsA+i*img_colsB:L1A*img_colsA+(i+1)*img_colsB],vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1+n)\n",
    "    plt.imshow(cara_externa_reconstruida[idea].reshape(L1A*img_colsA+L1B*img_colsB, img_rows).transpose()[:,L1A*img_colsA+i*img_colsB:L1A*img_colsA+(i+1)*img_colsB],vmin=0, vmax=30)\n",
    "    plt.viridis()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "print(idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idea)\n",
    "np.sum(cara_externa_reconstruida,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamos_energia=(np.sum(cara_externa_reconstruida, axis=1))-(np.sum(cara_externa, axis=1))\n",
    "n, bins, patches = plt.hist(veamos_energia, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=200, cen=0, wid=100)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "1079px",
    "right": "57px",
    "top": "233px",
    "width": "760px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
