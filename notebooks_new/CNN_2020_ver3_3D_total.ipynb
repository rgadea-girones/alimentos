{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rgadea/anaconda3/envs/tensorflow3/lib/python36.zip', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/lib-dynload', '', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/IPython/extensions', '/volumedisk0/home/rgadea/.ipython', '/home/rgadea/lmfit-py/', '/home/rgadea/experimentos/viherbos/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "# from sklearn import preprocessing\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "sys.path.append(\"/home/rgadea/experimentos/viherbos/\")\n",
    "\n",
    "print(sys.path)\n",
    "import json \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,Conv3D, MaxPooling3D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en pyhton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto_datos_entradas A shape: (56839, 20, 175)\n",
      "conjunto_datos_entradas B shape: (56839, 20, 175)\n",
      "conjunto_datos_salidas shape: (56839, 3)\n"
     ]
    }
   ],
   "source": [
    "filtro=2\n",
    "if filtro==1:\n",
    "    npzfile = np.load('../conjuntos_datos_nuevos_2020/20_12_2019_comptom_filt.npz')\n",
    "    npzfile.files\n",
    "    conjunto_datos_entradasA=npzfile['arr_0']\n",
    "    conjunto_datos_entradasB=npzfile['arr_1']\n",
    "    conjunto_datos_salidas=npzfile['arr_2']\n",
    "else:\n",
    "    if filtro==2:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/20_12_2019_comptom_filt2.npz')\n",
    "        npzfile.files\n",
    "        conjunto_datos_entradasA=npzfile['arr_0']\n",
    "        conjunto_datos_entradasB=npzfile['arr_1']\n",
    "        conjunto_datos_salidas=npzfile['arr_2']\n",
    "    else:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/11_12_2019.npz')\n",
    "        npzfile.files\n",
    "        entradas_sensorsA1=npzfile['arr_0']\n",
    "        entradas_sensorsB1=npzfile['arr_1']\n",
    "        coordenadas1=npzfile['arr_2']\n",
    "        entradas_sensorsA2=npzfile['arr_3']\n",
    "        entradas_sensorsB2=npzfile['arr_4']\n",
    "        coordenadas2=npzfile['arr_5']\n",
    "        conjunto_datos_entradasA=np.concatenate((entradas_sensorsA1,entradas_sensorsA2),axis=0)\n",
    "        conjunto_datos_entradasB=np.concatenate((entradas_sensorsB1,entradas_sensorsB2),axis=0)\n",
    "        conjunto_datos_salidas=np.concatenate((coordenadas1,coordenadas2),axis=0)\n",
    "\n",
    "\n",
    "print('conjunto_datos_entradas A shape:', conjunto_datos_entradasA.shape)\n",
    "print('conjunto_datos_entradas B shape:', conjunto_datos_entradasB.shape)\n",
    "print('conjunto_datos_salidas shape:', conjunto_datos_salidas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166.55446522  57.22428188   4.93574905]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175.53782283 351.13553053 -33.42412949]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166.06154545 224.71903329   9.34265423]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176.98455872 300.9877457  -24.89082336]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189.57981212 309.04799896  65.43500519]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187.68502256 310.66502523 -23.34671021]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167.1012482   16.4165904   65.37042999]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABICAYAAADvR65LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAIaUlEQVR4nO3dbYxUZxnG8f/F8uZiCSClroUINNCEL1pEC/UlthRLSQOa9ANIIsY2jSYatfEFQmKin2w1pjExrURRokiLiC0hNaSljX4xW9oqFNoubC2VpbRA1KI1qV24/XCeSYdhFnZ3Zs8Le/2SzZzznGnPlXv23DzzzNldRQRmZlY9Y4oOYGZmw+MGbmZWUW7gZmYV5QZuZlZRbuBmZhXlBm5mVlEtNXBJyyX1SOqVtL5doczM7NI03PvAJXUAh4FlQB+wD1gTEc+3L56ZmQ2klRn4R4DeiPhbRPwPeBBY1Z5YZmZ2KWNb+G+vBo7V7fcB1zc+SdJdwF0AHXR8qJPJLZzSzGz0+Tf/PB0RVzaOt9LA1WTsgvWYiNgEbAKYrGlxvZa2cEoblVT3reZf/WCj0OOx45Vm460sofQBs+r2ZwKvtvD/MzOzIWhlBr4PmCdpDnAcWA18ti2pzOp0zL8GgJg4jjh0JNvu7y8yklkpDLuBR0S/pC8De4AOYHNEHGpbMjMzu6hWZuBExKPAo23KYnaeMZ2dABy+M312o2D+97Kxs2fOFBWrPWrr+l7Ttxb4JzHNzCqqpRm42UjSpEkA9K69H4AbD62CMc1ufqogz7ytDTwDNzOrKM/ArbTOpXXuG+7+IgBTu09w9j9vFhlpeCTPuG1EeAZuZlZRnoFbacVbbwFwxUPdAPRHwJiOIiMNj2ffNkI8AzczqyjPwK386mew584Wl8OsZDwDNzOrKDdwK68xHQOveUvn/5ZCs1Hokg1c0mZJJyUdrBubJukxSUfS49SRjWlmZo0GMwP/JbC8YWw9sDci5gF7076VxeUyOz13duA17wjf3WGj3iUbeET8CfhHw/AqYEva3gJ8us25rBVubmajwnDXwK+KiBMA6XFG+yKZmdlgjPhthPV/E3MinSN9OjOzUWO4M/DXJXUBpMeTAz0xIjZFxKKIWDSOCcM8nZmZNRpuA98FrEvb64BH2hPHrEWXywe4ZoMwmNsItwF/Bq6V1CfpDuD7wDJJR4Blad/MzHJ0yTXwiFgzwKGlbc5i1jrffWOjiH8S08ysotzAzcwqyg3czKyi3MDNzCrKDdzMrKLcwC8HvvfZbFRyAzczqyj/SbXLge99NhuVPAM3M6soN3Azs4pyAzczqyg3cDOzinIDNzOrKEWOdzBIOgW8CZzO7aSDNx3nGqqyZnOuoSlrLihvtrxzvT8irmwczLWBA0h6OiIW5XrSQXCuoStrNucamrLmgvJmK0suL6GYmVWUG7iZWUUV0cA3FXDOwXCuoStrNucamrLmgvJmK0Wu3NfAzcysPbyEYmZWUW7gZmYVlVsDl7RcUo+kXknr8zpvkxyzJD0p6QVJhyR9NY1Pk/SYpCPpcWpB+Tok/UXS7rQ/R1J3yvWQpPEF5ZoiaYekF1PtlpShZpK+nl7Hg5K2SZpYVM0kbZZ0UtLBurGmNVLmx+l6OCBpYc65fpBeywOSfi9pSt2xDSlXj6Rb8sxVd+wbkkLS9LSfW70ulk3SV1JdDkm6t248l5pdICJG/AvoAF4C5gLjgf3AgjzO3SRLF7AwbV8BHAYWAPcC69P4euCegvLdDfwG2J32twOr0/YDwJcKyrUFuDNtjwemFF0z4GrgZeBddbX6fFE1Az4BLAQO1o01rRGwAvgDIGAx0J1zrk8BY9P2PXW5FqTrcwIwJ123HXnlSuOzgD3AK8D0vOt1kZrdCDwOTEj7M/Ku2QU5czkJLAH21O1vADbkce5BZHsEWAb0AF1prAvoKSDLTGAvcBOwO32znq670M6rY465JqdGqYbxQmuWGvgxYBrZ77bfDdxSZM2A2Q0XfdMaAT8F1jR7Xh65Go59Btiats+7NlMjXZJnLmAH8AHgaF0Dz7VeA7yW24Gbmzwv15rVf+W1hFK70Gr60lihJM0GrgO6gasi4gRAepxRQKT7gG8B59L+e4B/RUR/2i+qbnOBU8Av0vLOzyRNouCaRcRx4IfA34ETwBvAM5SjZjUD1ahM18QXyGa3UHAuSSuB4xGxv+FQGeo1H/h4Wp77o6QPF50trwbe7A82Fnr/oqR3A78DvhYRZ4rMkvLcBpyMiGfqh5s8tYi6jSV7O3l/RFxH9vtsCvscoyatJ68ie9v6PmAScGuTp5bxXtlSvLaSNgL9wNbaUJOn5ZJLUiewEfhOs8NNxvKu11hgKtkSzjeB7ZJEgdnyauB9ZOtaNTOBV3M69wUkjSNr3lsjYmcafl1SVzreBZzMOdZHgZWSjgIPki2j3AdMkVT703dF1a0P6IuI7rS/g6yhF12zm4GXI+JURLwN7ARuoBw1qxmoRoVfE5LWAbcBayO99y841zVk/xjvT9fBTOBZSe8tOFdNH7AzMk+RvVOeXmS2vBr4PmBeujtgPLAa2JXTuc+T/sX8OfBCRPyo7tAuYF3aXke2Np6biNgQETMjYjZZfZ6IiLXAk8DtReVK2V4Djkm6Ng0tBZ6n4JqRLZ0sltSZXtdarsJrVmegGu0CPpfurlgMvFFbasmDpOXAt4GVEfHfhryrJU2QNAeYBzyVR6aIeC4iZkTE7HQd9JHdcPAaBdcreZhsYoWk+WQf5p+mwJqN+CJ73cL+CrI7Pl4CNuZ13iY5Pkb29uYA8Nf0tYJsvXkvcCQ9Tisw4yd55y6UuemboRf4LekT8AIyfRB4OtXtYbK3koXXDPgu8CJwEPgV2Z0AhdQM2Ea2Fv82WfO5Y6Aakb3t/km6Hp4DFuWcq5ds3bZ2DTxQ9/yNKVcPcGueuRqOH+WdDzFzq9dFajYe+HX6XnsWuCnvmjV++Ufpzcwqyj+JaWZWUW7gZmYV5QZuZlZRbuBmZhXlBm5mVlFu4GZmFeUGbmZWUf8HNOJ1PF4FhJsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189.72755177 150.81150772 -45.65587616]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173.2550703  290.98782997 -26.70606804]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168.20875463 254.70556476  26.74402428]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    idea=np.random.randint(1,conjunto_datos_entradasB.shape[0])\n",
    "    plt.imshow(conjunto_datos_entradasB[idea], cmap='viridis')\n",
    "    plt.show()\n",
    "    print(conjunto_datos_salidas[idea,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 250\n",
    "nb_classes = 10\n",
    "nb_epoch = 2000\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 20, 41\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (1,2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (2, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector A shape: (56839, 20, 41)\n",
      "conjunto_datos_nuevo A: (56839, 820)\n",
      "sector B shape: (56839, 20, 41)\n",
      "conjunto_datos_nuevo B: (56839, 820)\n"
     ]
    }
   ],
   "source": [
    "muestras=conjunto_datos_entradasA.shape[0]\n",
    "# veamos=idea.reshape(idea.shape[0],175, 20)\n",
    "\n",
    "\n",
    "veamos2=np.zeros([muestras,20,175])\n",
    "veamos2_3=np.zeros([muestras,20,525])\n",
    "sector2A=np.zeros([muestras,20,img_cols])\n",
    "sector2B=np.zeros([muestras,20,img_cols])\n",
    "veamos3=np.zeros([muestras,175])\n",
    "# for i in range(idea.shape[0]):\n",
    "for i in range(muestras):\n",
    "    veamos2[i]=conjunto_datos_entradasA[i]\n",
    "    veamos3[i]=np.sum(veamos2[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    veamos2_3[i]=np.concatenate((veamos2[i],veamos2[i],veamos2[i]),axis=1) \n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    sector2A[i]=veamos2_3[i,:,indice_inferior:indice_superior]\n",
    "for i in range(muestras):\n",
    "    veamos2[i]=conjunto_datos_entradasB[i]\n",
    "    veamos3[i]=np.sum(veamos2[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    veamos2_3[i]=np.concatenate((veamos2[i],veamos2[i],veamos2[i]),axis=1) \n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    sector2B[i]=veamos2_3[i,:,indice_inferior:indice_superior]    \n",
    "\n",
    "print('sector A shape:', sector2A.shape)\n",
    "conjunto_datos_nuevoA=sector2A.reshape(sector2A.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo A:', conjunto_datos_nuevoA.shape)\n",
    "\n",
    "print('sector B shape:', sector2B.shape)\n",
    "conjunto_datos_nuevoB=sector2B.reshape(sector2B.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo B:', conjunto_datos_nuevoB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5  # how many digits we will display\n",
    "\n",
    "fig = plt.figure(figsize=(8,20))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ideas=np.random.randint(1,muestras)\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+1)\n",
    "    plt.imshow(sector2A[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+2)\n",
    "    plt.imshow(sector2B[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAADCCAYAAABKUHl7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVzUlEQVR4nO3df5BdZX3H8fcnmx9LlmASEn6Y8CNopKKFgDTq0FoQxRCpaAdrGKelFifiyIxOpRXrDFo7ndF21P7AyqBS0CL+RjMalQzYQWcUCBhCUgIkkMiSlAUiBBIS2OTbP+6JXTd3n+fuPffuPcn5vGZ29t7znHvOd5+997t3z32e56uIwMzMDn2Teh2AmZlNDCd8M7OacMI3M6sJJ3wzs5pwwjczq4nJvQ6gmamaFv0M9DqMrtKk9N/a2LdvgiIxy/PzNa/XfbSbnbwQe5Tap1TCl7QE+FegD/hSRHxqVPs04CvAa4CngHdFxObccfsZ4LU6t0xolTfpsOnJ9n27dk1QJGZ5fr7m9bqP7ohbs/u0fUlHUh/weeB84BTgYkmnjNrtUuA3EfFy4HPAp9s9n5mZlVPmGv5iYGNEPBwRLwBfBy4ctc+FwA3F7W8D50pK/sthZmbdUSbhzwMeHXF/sNjWdJ+IGAaeAY4scU4zM2tTmWv4zd6pj16noZV9GjtKy4HlAP2kr4WZmdn4lXmHPwgcN+L+fGDrWPtImgy8BNje7GARcW1EnBkRZ05hWomwzMysmTIJ/y5goaQFkqYCy4AVo/ZZAVxS3L4IuC28WpuZWU+0fUknIoYlXQ78hMawzOsiYr2kTwKrI2IF8GXgq5I20nhnv6wTQR8KPIzNDiZ+vubl+qhv7tzsMfY+8USnwmmq1Dj8iFgJrBy17aoRt3cD7yxzDjMz6wwvrWBmVhNO+GZmNeGEb2ZWE074ZmY14YRvZlYTTvhmZjVRyfXwDwWTpns52VwfQD36wfIOhddL7meInTsnKJKx+R2+mVlNOOGbmdWEE76ZWU044ZuZ1YQTvplZTZSpaXucpJ9Kul/SekkfbLLP2ZKekbSm+Lqq2bHMzKz7ygzLHAY+HBH3SJoB3C1pVUT8z6j9fhYRF5Q4j5mZdUDb7/AjYltE3FPcfha4nwNr2pqZWUV0ZOKVpBOB04E7mjS/XtK9NMofXhER68c4xiFV0zY3UeRQmGhSBWX7sRO/h7LHyBXGKFsUo5UJcDnd7seD4fne7d/TRCid8CUdDnwH+FBE7BjVfA9wQkQ8J2kp8D1gYbPjRMS1wLUAR2i2yyCamXVYqVE6kqbQSPY3RsR3R7dHxI6IeK64vRKYImlOmXOamVl7yozSEY2atfdHxGfH2OeYYj8kLS7O91S75zQzs/aVuaRzFvDnwH2S1hTb/g44HiAirgEuAt4vaRh4HlgWEb5cY2bWA20n/Ij4OaDMPlcDV7d7DjMz6xzPtDUzqwknfDOzmnABlDH0epz8RBQPORTGsJc9fs7kE47L7jO85dFS5yhrIn4PvVaFYjoHwzj7HL/DNzOrCSd8M7OacMI3M6sJJ3wzs5pwwjczqwknfDOzmnDCNzOrCY/DH0Ovx7gfDDrxM5Q9hgYGSj0+N7Z6Ekdmj5Ebqx+7dpeKQae/Kh3AA48kmydiDLtfL3lVmEtQ+h2+pM2S7itq1q5u0i5J/yZpo6S1ks4oe04zMxu/Tr3DPycinhyj7XwaRU8WAq8FvlB8NzOzCTQR1/AvBL4SDb8EZko6dgLOa2ZmI3Qi4Qdwi6S7i7q0o80DRi42MkiTYueSlktaLWn1i+zpQFhmZjZSJy7pnBURWyUdBayStCEibh/R3mzN/AOKoLimrZlZd5V+hx8RW4vvQ8DNwOJRuwwCI4cxzAe2lj2vmZmNT9ki5gOSZuy/DZwHrBu12wrgL4rROq8DnomIbWXOa2Zm41f2ks7RwM1FnfLJwNci4seSLoPf1rVdCSwFNgK7gPeUPOeE6PX64K2cf9Lc9Bjxsuu0l+2DsmPkATS9P9meG+O+449PSrYfsX5m+vhPPp1sBxh+xQEfSY3L5DnpGNj9QrJ5z+tfmWzv3ziUjWHflt7WHcg9V/paeC5Vfb36KswlKJXwI+Jh4LQm268ZcTuAD5Q5j5mZleelFczMasIJ38ysJpzwzcxqwgnfzKwmnPDNzGrCCd/MrCa8Hv4Yej1mtpXza1f5ce5ldGIeQG4t+azMGPYZK+9Ltu87eUGy/cVTj8+GMHxYX7J9z8z0+6qZu15Mtk/KjMOftnVHsr0Vud9D2TkdOVUYQ9/tNfk7MbendAylIzAzs4OCE76ZWU044ZuZ1YQTvplZTTjhm5nVRNsJX9LJReHy/V87JH1o1D5nS3pmxD5XlQ/ZzMza0fawzIh4AFgEIKkPeIxGAZTRfhYRF7R7HjMz64xOXdI5F9gUEVs6dDwzM+uwTk28WgbcNEbb6yXdS6Os4RURsb7ZTkUB9OUA/fS2+Egryk7S6MQkjLKTVXIxDp/7mmT75FvvTrb3vXJhPoYtjyXb957+imT7jgXpAimzM+ff9K4jku39Q81KMo/PzuP2JdsHtqV/hj2zDk+2H77puXQA/VPT7cCkwXSRlLIT5PY98VT6/B2Y9NTtokV9c+eWenwrr9du/wyl3+FLmgq8DfhWk+Z7gBMi4jTg34HvjXWciLg2Is6MiDOnMK1sWGZmNkonLumcD9wTEY+PboiIHRHxXHF7JTBF0pwOnNPMzMapEwn/Ysa4nCPpGBUFbyUtLs6X/t/OzMy6otQ1fEnTgTcD7xuxbWQB84uA90saBp4HlhU1bs3MbIKVLWK+Czhy1LaRBcyvBq4ucw4zM+sMz7Q1M6sJJ3wzs5pwAZQe6USBlbJjdnMxTFv762R7ZM6vXbvzQQyki7js7c8UF3lJ+j3L4PnpsdMDmboeA3/yv+kdgKE1Ryfb+4fSMT5xWnoYct+e9Pn7dqf7cOfR6T4EGJiXPsbAHY9kj5GSK5aTG6ffiq4XLZqAokjKvB76Eu3ank/nfodvZlYTTvhmZjXhhG9mVhNO+GZmNeGEb2ZWE074ZmY14YRvZlYTHoc/hm6vS92J83d7bHPZ9fZj9oL8TrPT69H37d5bKobcWvSXvXlVsv1vZm/KnmPJ1Lcm25+88fhk+97+9Jr7uzPLsB+RKTs0554d6R2Avu3pfWLOzGR7bs5Fdj38zHOZFp6KuXH4ufXsc8/3smv2t7KefpnXXMRwdp+W3uFLuk7SkKR1I7bNlrRK0kPF91ljPPaSYp+HJF3ScvRmZtZRrV7SuR5YMmrblcCtEbEQuLW4/zskzQY+DrwWWAx8fKw/DGZm1l0tJfyIuB3YPmrzhcANxe0bgLc3eehbgFURsT0ifgOs4sA/HGZmNgHKfGh7dERsAyi+H9Vkn3nAyNVKBottB5C0XNJqSatfJLN4iJmZjVu3R+k0+zSqaQEU17Q1M+uuMgn/cUnHAhTfm5W9HwRGlrufD2wtcU4zM2tTmYS/Atg/6uYS4PtN9vkJcJ6kWcWHtecV28zMbIK1NA5f0k3A2cAcSYM0Rt58CvimpEuBXwPvLPY9E7gsIt4bEdsl/QNwV3GoT0bE6A9/Kyk3prbb4/RbWtu73DD57M+QW5s7a/cL5R7fyikyQ5sXnpZZ8D7jZbe9J7vPZYtuT7Z/rf+EZHvuZ9jbny4DnVvvfvpt+bXsI/O7Ts8UgOEt6X7OjUHPjdNv6bmYec2UnVdyKGgp4UfExWM0ndtk39XAe0fcvw64rq3ozMysY7y0gplZTTjhm5nVhBO+mVlNOOGbmdWEE76ZWU044ZuZ1YTXw29TS+PkE8qurQ35scmxc+e4Yhqv3PFjfrPllX7XvulTku17+9NjzKfklnr/cHod96+e85Zk+6Yr/iNzAliyIb0e/tOvSq9TPmVH+mc84qH0+VtZ776syKx3nxtnn3uuZOe9JFsnRtnXfCvzAMqu2Z9ThX40M7MJ4IRvZlYTTvhmZjXhhG9mVhPZhD9GPdt/lrRB0lpJN0tq+smYpM2S7pO0RtLqTgZuZmbj08o7/Os5sCzhKuDVEXEq8CDw0cTjz4mIRRFxZnshmplZJ2QTfrN6thFxS0TsH2v2SxqFTczMrMI6cQ3/r4AfjdEWwC2S7pa0vAPnMjOzNpWaeCXpY8AwcOMYu5wVEVslHQWskrSh+I+h2bGWA8sB+ulucZFOKDvRJDdpqpW/xGUns+RMnntkqfPrgXzhjUknL0i2TxlMF8bYu/i4ZHtuYtezL09Pilrwvfz7lBkb0y+jmX+crvmze2h2sn32hvTvMfczTj5hXrIdQJmJVTG9v9TjlXn8JNLPtVyBFJiASUslJ0u2UjQpF2PqGHo+nzXafocv6RLgAuDdETFWYfKtxfch4GZg8VjHcxFzM7PuaivhS1oCfAR4W0Q0/bMmaUDSjP23adSzXddsXzMz675WhmXeBPwCOFnSYFHD9mpgBo3LNGskXVPs+1JJK4uHHg38XNK9wJ3ADyPix135KczMLCt7DX+MerZfHmPfrcDS4vbDwGmlojMzs47xTFszs5pwwjczqwknfDOzmnABlINYbtxvblxybmx0WWph/Pek7eniHbnCG8fcuSfZniugkhtDP+vB9Dh9gCd/P92+8+F0EZa5G/cl2/fMmppsH9j0m3QAHRBbHkvvULIYT25eSktzSkrOO6kDv8M3M6sJJ3wzs5pwwjczqwknfDOzmnDCNzOrCSd8M7OacMI3M6sJj8NvU27d6m6vzd2K3NjnXPukzHr4WU8+nd1lX258dmYs/55Z6afw4ZueS7Yfc8feZPukXS8m2wFmzJyRbO/bk35fNe3p9Fj/wx5L91HO8Oz0GHeAKZn5DmXlnkvDWx5NP76FteTLyo31L7vefdn6FLljRKTnc0D7Rcw/IemxYqXMNZKWjvHYJZIekLRR0pXZaMzMrGvaLWIO8LmiOPmiiFg5ulFSH/B54HzgFOBiSaeUCdbMzNrXVhHzFi0GNkbEwxHxAvB14MI2jmNmZh1Q5kPbyyWtLS75zGrSPg8YeWFusNjWlKTlklZLWv0i6fVRzMxs/NpN+F8AXgYsArYBn2myj5psa1r7FlzT1sys29pK+BHxeETsjcbHwl+keXHyQeC4EffnA1vbOZ+ZmZXXbhHzY0fcfQfNi5PfBSyUtEDSVGAZsKKd85mZWXnZcfhFEfOzgTmSBoGPA2dLWkTjEs1m4H3Fvi8FvhQRSyNiWNLlwE+APuC6iFjflZ+igiZinH1Z2XHHW8qNG+7ErL7cOuzTWxhjnjL5wfTx95x6fKnjA8z/Uea5kJmvkKtbkKsZoPsfSp8fiMy8kZzc870K4+gPhfOn+lHP519xXStiXtxfCRwwZNPMzCael1YwM6sJJ3wzs5pwwjczqwknfDOzmnDCNzOrCSd8M7Oa8Hr4FdXKuOWy63d3Ioay58/VDcit2T9l8Kl0DE+k25VZp71/41CyHWDa2vQ4+H3zj0rHULJuQdl12qH74+hzMZZ9HrSi2+vVd2KuQZnXdEfWwzczs0ODE76ZWU044ZuZ1YQTvplZTbSyeNp1wAXAUES8utj2DeDkYpeZwNMRsajJYzcDzwJ7geGIOLNDcZuZ2Ti1MkrneuBq4Cv7N0TEu/bflvQZ4JnE48+JiCfbDdDMzDqjldUyb5d0YrM2SQL+DHhjZ8MyM7NOK3sN/4+AxyNirAW3A7hF0t2SlqcO5Jq2ZmbdVXbi1cXATYn2syJiq6SjgFWSNkTE7c12jIhrgWsBjtDsMWvf1kWviylMVAxlJ/xki38MpAuk5B7fiQk/euCRUo8v+3voxO+x25OSyk4ua+UcZSeoTcTvoczkrVYKoLT9Dl/SZOBPgW+MtU9REIWIGAJupnntWzMzmwBlLum8CdgQEYPNGiUNSJqx/zZwHs1r35qZ2QTIJvyipu0vgJMlDUq6tGhaxqjLOZJeKml/ScOjgZ9Luhe4E/hhRPy4c6Gbmdl4tFvTloj4yybbflvTNiIeBk4rGZ+ZmXWIZ9qamdWEE76ZWU044ZuZ1YQLoFRUJwqglH18FcbpZ2Ms+fhO6Pb474PBwfBc6fbjc3JFXiA/LyXFBVDMzOy3nPDNzGrCCd/MrCac8M3MasIJ38ysJpzwzcxqwgnfzKwmFFG9peclPQFsGbFpDlDlMolVjw8cY6c4xs6oeoxVjw8OjPGEiEgO9q9kwh9N0uoqF0CvenzgGDvFMXZG1WOsenzQXoy+pGNmVhNO+GZmNXGwJPxrex1ARtXjA8fYKY6xM6oeY9XjgzZiPCiu4ZuZWXkHyzt8MzMryQnfzKwmKp3wJS2R9ICkjZKu7HU8zUjaLOk+SWskre51PACSrpM0JGndiG2zJa2S9FDxfVYFY/yEpMeKvlwjaWkP4ztO0k8l3S9pvaQPFtsr04+JGKvUj/2S7pR0bxHj3xfbF0i6o+jHb0iaWsEYr5f0yIh+XNSrGIt4+iT9StIPivvj78OIqOQX0AdsAk4CpgL3Aqf0Oq4mcW4G5vQ6jlExvQE4A1g3Yts/AVcWt68EPl3BGD8BXNHr/itiORY4o7g9A3gQOKVK/ZiIsUr9KODw4vYU4A7gdcA3gWXF9muA91cwxuuBi3rdhyPi/Gvga8APivvj7sMqv8NfDGyMiIcj4gXg68CFPY7poBARtwPbR22+ELihuH0D8PYJDWqUMWKsjIjYFhH3FLefBe4H5lGhfkzEWBnR8Fxxd0rxFcAbgW8X23vdj2PFWBmS5gNvBb5U3Bdt9GGVE/484NER9wep2JO5EMAtku6WtLzXwSQcHRHboJEogKN6HM9YLpe0trjk09PLTvtJOhE4ncY7v0r246gYoUL9WFyKWAMMAato/Of+dEQMF7v0/LU9OsaI2N+P/1j04+ckTethiP8C/C2wv47hkbTRh1VO+GqyrVJ/dQtnRcQZwPnAByS9odcBHcS+ALwMWARsAz7T23BA0uHAd4APRcSOXsfTTJMYK9WPEbE3IhYB82n85/7KZrtNbFSjTj4qRkmvBj4K/B7wB8Bs4CO9iE3SBcBQRNw9cnOTXbN9WOWEPwgcN+L+fGBrj2IZU0RsLb4PATfTeEJX0eOSjgUovg/1OJ4DRMTjxQtvH/BFetyXkqbQSKQ3RsR3i82V6sdmMVatH/eLiKeB/6ZxfXympMlFU2Ve2yNiXFJcMouI2AP8J73rx7OAt0naTOPS9htpvOMfdx9WOeHfBSwsPomeCiwDVvQ4pt8haUDSjP23gfOAdelH9cwK4JLi9iXA93sYS1P7E2nhHfSwL4trpF8G7o+Iz45oqkw/jhVjxfpxrqSZxe3DgDfR+Kzhp8BFxW697sdmMW4Y8YddNK6P96QfI+KjETE/Ik6kkQdvi4h3004f9vqT58yn0ktpjDzYBHys1/E0ie8kGqOH7gXWVyVG4CYa/8q/SOM/pUtpXPO7FXio+D67gjF+FbgPWEsjsR7bw/j+kMa/yGuBNcXX0ir1YyLGKvXjqcCviljWAVcV208C7gQ2At8CplUwxtuKflwH/BfFSJ5efgFn8/+jdMbdh15awcysJqp8ScfMzDrICd/MrCac8M3MasIJ38ysJpzwzcxqwgnfzKwmnPDNzGri/wDKht7PSTn9ywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,sector2B.shape[0])\n",
    "    plt.imshow(sector2B[idea], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34103\n",
      "11367\n",
      "11369\n",
      "(3816, 1640)\n",
      "(2430, 1640)\n",
      "(1973, 1640)\n",
      "(1608, 1640)\n",
      "(1542, 1640)\n"
     ]
    }
   ],
   "source": [
    "numero_muestras=muestras\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "conjunto_datos_nuevo2=np.concatenate((conjunto_datos_salidas,conjunto_datos_nuevoB, conjunto_datos_nuevoA), axis=1)\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "XY_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "XY_test_bin0=XY_test[np.where((XY_test[:,0]>=164.9999) * (XY_test[:,0]<171.000))]\n",
    "XY_test_bin1=XY_test[np.where((XY_test[:,0]>=171.000) * (XY_test[:,0]<177.000))]\n",
    "XY_test_bin2=XY_test[np.where((XY_test[:,0]>=177.000) * (XY_test[:,0]<183.0000))]\n",
    "XY_test_bin3=XY_test[np.where((XY_test[:,0]>=183.000) * (XY_test[:,0]<189.0000))]\n",
    "XY_test_bin4=XY_test[np.where((XY_test[:,0]>=189.0000))]\n",
    "\n",
    "X_train=conjunto_datos_nuevo2[:tamanyo_tr,3:]\n",
    "X_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,3:]\n",
    "X_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,3:]\n",
    "\n",
    "X_test_bin0=XY_test_bin0[:,3:]\n",
    "Y_test_bin0=XY_test_bin0[:,0]\n",
    "print(X_test_bin0.shape)\n",
    "X_test_bin1=XY_test_bin1[:,3:]\n",
    "Y_test_bin1=XY_test_bin1[:,0]\n",
    "print(X_test_bin1.shape)\n",
    "X_test_bin2=XY_test_bin2[:,3:]\n",
    "Y_test_bin2=XY_test_bin2[:,0]\n",
    "print(X_test_bin2.shape)\n",
    "X_test_bin3=XY_test_bin3[:,3:]\n",
    "Y_test_bin3=XY_test_bin3[:,0]\n",
    "print(X_test_bin3.shape)\n",
    "X_test_bin4=XY_test_bin4[:,3:]\n",
    "Y_test_bin4=XY_test_bin4[:,0]\n",
    "print(X_test_bin4.shape)\n",
    "\n",
    "\n",
    "\n",
    "Y_train=conjunto_datos_nuevo2[:tamanyo_tr,0] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,0] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,0] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],2, img_rows, img_cols,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 2,img_rows, img_cols,1)\n",
    "\n",
    "X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], 2, img_rows, img_cols,1)\n",
    "\n",
    "input_shape = (2, img_rows, img_cols,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (34103, 2, 20, 41, 1)\n",
      "34103 train samples\n",
      "11367 validation samples\n",
      "11369 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 20 random training images using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert class vectors to binary class matrices\n",
    "# #Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# #Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "# for i in range(1,5):\n",
    "#     idea=np.random.randint(1,X_train.shape[0])\n",
    "#     plt.imshow(np.reshape(X_train[idea], [img_rows, img_cols]), cmap='viridis')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=kernel_size,\n",
    "                        padding='same',\n",
    "                        data_format='channels_last',\n",
    "                        input_shape=(2,img_rows,img_cols,1)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "# model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('elu'))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "experimento=\"CNN_kernel_{}x{}x{}_con_batchnormalization_sector_{}x{}x{}_elu\".format(kernel_size[0],kernel_size[1],kernel_size[2],img_rows,img_cols,1)\n",
    "algoritmo='Nadam'\n",
    "optimizador=Nadam(beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"../logs/defs/{}{}{}\".format(experimento,algoritmo,dt))\n",
    "best_model_name='../redes_CNN_R/models_best/CNN_regression_R_{}_{}_{}_{}_{}.h5'.format(nb_epoch,batch_size,experimento,algoritmo,dt)\n",
    "model_check=ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=600, verbose=1, mode='auto', baseline=None)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizador)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 2, 20, 41, 8)      264       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 20, 41, 8)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 10, 20, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 2, 10, 20, 32)     8224      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 10, 20, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 10, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 5, 10, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 5, 10, 32)      32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 5, 10, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 5, 10, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 5, 10, 64)      65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 5, 10, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 5, 10, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 2, 5, 10, 128)     262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 5, 10, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 5, 10, 128)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 12801     \n",
      "=================================================================\n",
      "Total params: 382,985\n",
      "Trainable params: 382,473\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 34103 samples, validate on 11367 samples\n",
      "Epoch 1/2000\n",
      "34103/34103 [==============================] - 7s 202us/step - loss: 880.2336 - val_loss: 51.8870\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 51.88705, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 2/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 18.9171 - val_loss: 31.1192\n",
      "\n",
      "Epoch 00002: val_loss improved from 51.88705 to 31.11922, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 3/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 16.5609 - val_loss: 24.8341\n",
      "\n",
      "Epoch 00003: val_loss improved from 31.11922 to 24.83412, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 4/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 16.0036 - val_loss: 19.2015\n",
      "\n",
      "Epoch 00004: val_loss improved from 24.83412 to 19.20148, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 5/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 16.4291 - val_loss: 74.1421\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 19.20148\n",
      "Epoch 6/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 310.2046 - val_loss: 41145.7798\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 19.20148\n",
      "Epoch 7/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 42.6983 - val_loss: 501.1510\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 19.20148\n",
      "Epoch 8/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 31.9987 - val_loss: 25.1585\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 19.20148\n",
      "Epoch 9/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 33.2507 - val_loss: 36.3730\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 19.20148\n",
      "Epoch 10/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 26.4493 - val_loss: 18.4015\n",
      "\n",
      "Epoch 00010: val_loss improved from 19.20148 to 18.40145, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 11/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 34.1243 - val_loss: 24.3767\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 18.40145\n",
      "Epoch 12/2000\n",
      "34103/34103 [==============================] - 4s 124us/step - loss: 25.9124 - val_loss: 54.3533\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 18.40145\n",
      "Epoch 13/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 33.0791 - val_loss: 69.2363\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 18.40145\n",
      "Epoch 14/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 28.9363 - val_loss: 16.1869\n",
      "\n",
      "Epoch 00014: val_loss improved from 18.40145 to 16.18687, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 15/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 23.7209 - val_loss: 83.0394\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 16.18687\n",
      "Epoch 16/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 31.1095 - val_loss: 48.3780\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 16.18687\n",
      "Epoch 17/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 25.6158 - val_loss: 43.2086\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 16.18687\n",
      "Epoch 18/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 26.4216 - val_loss: 95.6270\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 16.18687\n",
      "Epoch 19/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 25.1173 - val_loss: 12.0203\n",
      "\n",
      "Epoch 00019: val_loss improved from 16.18687 to 12.02027, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 20/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 20.5036 - val_loss: 34.7651\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 12.02027\n",
      "Epoch 21/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 23.5986 - val_loss: 13.3073\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 12.02027\n",
      "Epoch 22/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 23.9081 - val_loss: 22.6024\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 12.02027\n",
      "Epoch 23/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 21.4532 - val_loss: 20.3656\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 12.02027\n",
      "Epoch 24/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 21.9408 - val_loss: 10.8809\n",
      "\n",
      "Epoch 00024: val_loss improved from 12.02027 to 10.88094, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 25/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 19.7845 - val_loss: 10.8972\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 10.88094\n",
      "Epoch 26/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 14.9800 - val_loss: 48.4520\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 10.88094\n",
      "Epoch 27/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 22.6179 - val_loss: 10.0376\n",
      "\n",
      "Epoch 00027: val_loss improved from 10.88094 to 10.03762, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 28/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 20.3199 - val_loss: 10.4147\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 10.03762\n",
      "Epoch 29/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 16.6445 - val_loss: 12.7229\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 10.03762\n",
      "Epoch 30/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 17.2199 - val_loss: 15.9904\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 10.03762\n",
      "Epoch 31/2000\n",
      "34103/34103 [==============================] - 4s 124us/step - loss: 25.3558 - val_loss: 12.7613\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 10.03762\n",
      "Epoch 32/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 16.1753 - val_loss: 16.1341\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 10.03762\n",
      "Epoch 33/2000\n",
      "34103/34103 [==============================] - 4s 124us/step - loss: 18.6517 - val_loss: 12.0229\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 10.03762\n",
      "Epoch 34/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 17.9594 - val_loss: 11.9653\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 10.03762\n",
      "Epoch 35/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 15.6886 - val_loss: 10.6507\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 10.03762\n",
      "Epoch 36/2000\n",
      "34103/34103 [==============================] - 4s 125us/step - loss: 16.0216 - val_loss: 12.0253\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 10.03762\n",
      "Epoch 37/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 17.5327 - val_loss: 49.5646\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 10.03762\n",
      "Epoch 38/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 17.2818 - val_loss: 9.4335\n",
      "\n",
      "Epoch 00038: val_loss improved from 10.03762 to 9.43350, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 39/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 16.0005 - val_loss: 16.1037\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 9.43350\n",
      "Epoch 40/2000\n",
      "34103/34103 [==============================] - 4s 125us/step - loss: 15.3816 - val_loss: 44.4216\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 9.43350\n",
      "Epoch 41/2000\n",
      "34103/34103 [==============================] - 4s 125us/step - loss: 15.7260 - val_loss: 9.3263\n",
      "\n",
      "Epoch 00041: val_loss improved from 9.43350 to 9.32631, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 42/2000\n",
      "34103/34103 [==============================] - 4s 124us/step - loss: 15.7188 - val_loss: 18.4560\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 9.32631\n",
      "Epoch 43/2000\n",
      "34103/34103 [==============================] - 4s 125us/step - loss: 14.4769 - val_loss: 10.8656\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 9.32631\n",
      "Epoch 44/2000\n",
      "34103/34103 [==============================] - 4s 124us/step - loss: 14.0202 - val_loss: 34.8839\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 9.32631\n",
      "Epoch 45/2000\n",
      "34103/34103 [==============================] - 4s 122us/step - loss: 16.9453 - val_loss: 49.9243\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 9.32631\n",
      "Epoch 46/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 15.2096 - val_loss: 31.1944\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 9.32631\n",
      "Epoch 47/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 14.6402 - val_loss: 10.7535\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 9.32631\n",
      "Epoch 48/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 14.7982 - val_loss: 11.5202\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 9.32631\n",
      "Epoch 49/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 13.1532 - val_loss: 17.3005\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 9.32631\n",
      "Epoch 50/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 16.3894 - val_loss: 24.9236\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 9.32631\n",
      "Epoch 51/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 14.6607 - val_loss: 13.1765\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 9.32631\n",
      "Epoch 52/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 12.2006 - val_loss: 25.0689\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 9.32631\n",
      "Epoch 53/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 12.2160 - val_loss: 13.7137\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 9.32631\n",
      "Epoch 54/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 13.5076 - val_loss: 21.4612\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 9.32631\n",
      "Epoch 55/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 14.2658 - val_loss: 32.1304\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 9.32631\n",
      "Epoch 56/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 12.5703 - val_loss: 10.4090\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 9.32631\n",
      "Epoch 57/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 13.7452 - val_loss: 12.9707\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 9.32631\n",
      "Epoch 58/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 13.8453 - val_loss: 12.2578\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 9.32631\n",
      "Epoch 59/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 11.4320 - val_loss: 9.4220\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 9.32631\n",
      "Epoch 60/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 12.0155 - val_loss: 10.2834\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 9.32631\n",
      "Epoch 61/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 12.0893 - val_loss: 10.2969\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 9.32631\n",
      "Epoch 62/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 14.4362 - val_loss: 11.5515\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 9.32631\n",
      "Epoch 63/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 11.5400 - val_loss: 10.1981\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 9.32631\n",
      "Epoch 64/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 12.3022 - val_loss: 10.0951\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 9.32631\n",
      "Epoch 65/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 11.2267 - val_loss: 9.4339\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 9.32631\n",
      "Epoch 66/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 11.0388 - val_loss: 10.5368\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 9.32631\n",
      "Epoch 67/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 12.7851 - val_loss: 10.9657\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 9.32631\n",
      "Epoch 68/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 10.9880 - val_loss: 10.2746\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 9.32631\n",
      "Epoch 69/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 11.4378 - val_loss: 17.1303\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 9.32631\n",
      "Epoch 70/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 11.8459 - val_loss: 8.4070\n",
      "\n",
      "Epoch 00070: val_loss improved from 9.32631 to 8.40699, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 71/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 11.1145 - val_loss: 15.5670\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 8.40699\n",
      "Epoch 72/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 11.2442 - val_loss: 8.4115\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 8.40699\n",
      "Epoch 73/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 10.8741 - val_loss: 11.7094\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 8.40699\n",
      "Epoch 74/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 12.2950 - val_loss: 11.9514\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 8.40699\n",
      "Epoch 75/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 10.6242 - val_loss: 8.1975\n",
      "\n",
      "Epoch 00075: val_loss improved from 8.40699 to 8.19755, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 76/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 11.7756 - val_loss: 9.5845\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 8.19755\n",
      "Epoch 77/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 9.9323 - val_loss: 8.9765\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 8.19755\n",
      "Epoch 78/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 10.1054 - val_loss: 12.4771\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 8.19755\n",
      "Epoch 79/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 11.4267 - val_loss: 42.0406\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 8.19755\n",
      "Epoch 80/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 10.6451 - val_loss: 9.0060\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 8.19755\n",
      "Epoch 81/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 9.8374 - val_loss: 8.3367\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 8.19755\n",
      "Epoch 82/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 9.3209 - val_loss: 11.5480\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 8.19755\n",
      "Epoch 83/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 10.3912 - val_loss: 8.2237\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 8.19755\n",
      "Epoch 84/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 9.9992 - val_loss: 8.8450\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 8.19755\n",
      "Epoch 85/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 10.5990 - val_loss: 12.4723\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 8.19755\n",
      "Epoch 86/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 9.5575 - val_loss: 44.5289\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 8.19755\n",
      "Epoch 87/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 9.2677 - val_loss: 8.1118\n",
      "\n",
      "Epoch 00087: val_loss improved from 8.19755 to 8.11178, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 88/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 10.5685 - val_loss: 14.7586\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 8.11178\n",
      "Epoch 89/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 9.3779 - val_loss: 14.4475\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 8.11178\n",
      "Epoch 90/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 9.7226 - val_loss: 14.9931\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 8.11178\n",
      "Epoch 91/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 10.7971 - val_loss: 9.1139\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 8.11178\n",
      "Epoch 92/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 9.5106 - val_loss: 20.8365\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 8.11178\n",
      "Epoch 93/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 9.4380 - val_loss: 21.5648\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 8.11178\n",
      "Epoch 94/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 9.0181 - val_loss: 15.3195\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 8.11178\n",
      "Epoch 95/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 8.9241 - val_loss: 10.1677\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 8.11178\n",
      "Epoch 96/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 9.1611 - val_loss: 9.2630\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 8.11178\n",
      "Epoch 97/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 9.1440 - val_loss: 15.1355\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 8.11178\n",
      "Epoch 98/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 8.7739 - val_loss: 9.2314\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 8.11178\n",
      "Epoch 99/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 9.1880 - val_loss: 17.2784\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 8.11178\n",
      "Epoch 100/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 9.1823 - val_loss: 7.9478\n",
      "\n",
      "Epoch 00100: val_loss improved from 8.11178 to 7.94781, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 101/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 9.1706 - val_loss: 8.4677\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 7.94781\n",
      "Epoch 102/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 8.7498 - val_loss: 10.9277\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 7.94781\n",
      "Epoch 103/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 8.4074 - val_loss: 30.4534\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 7.94781\n",
      "Epoch 104/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 9.8268 - val_loss: 15.4575\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 7.94781\n",
      "Epoch 105/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 8.7717 - val_loss: 36.0222\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 7.94781\n",
      "Epoch 106/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 8.5042 - val_loss: 7.8665\n",
      "\n",
      "Epoch 00106: val_loss improved from 7.94781 to 7.86645, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_250_CNN_kernel_2x4x4_con_batchnormalization_sector_20x41x1_elu_Nadam_2019-12-21 13:14:00.h5\n",
      "Epoch 107/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 7.9918 - val_loss: 8.2840\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 7.86645\n",
      "Epoch 108/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 8.7349 - val_loss: 39.4479\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 7.86645\n",
      "Epoch 109/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 8.5889 - val_loss: 16.4795\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 7.86645\n",
      "Epoch 110/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 7.9483 - val_loss: 13.0394\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 7.86645\n",
      "Epoch 111/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 8.2651 - val_loss: 17.4269\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 7.86645\n",
      "Epoch 112/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 7.7497 - val_loss: 10.1479\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 7.86645\n",
      "Epoch 113/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 8.6746 - val_loss: 13.8651\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 7.86645\n",
      "Epoch 114/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 8.0709 - val_loss: 8.8795\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 7.86645\n",
      "Epoch 115/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 8.5799 - val_loss: 20.9650\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 7.86645\n",
      "Epoch 116/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 8.2842 - val_loss: 8.7795\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 7.86645\n",
      "Epoch 117/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 7.8021 - val_loss: 8.5472\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 7.86645\n",
      "Epoch 118/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 7.9005 - val_loss: 8.7839\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 7.86645\n",
      "Epoch 119/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 7.8066 - val_loss: 8.2948\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 7.86645\n",
      "Epoch 120/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 7.5145 - val_loss: 9.7787\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 7.86645\n",
      "Epoch 121/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 7.9967 - val_loss: 31.6826\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 7.86645\n",
      "Epoch 122/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 7.6285 - val_loss: 19.0500\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 7.86645\n",
      "Epoch 123/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 7.6512 - val_loss: 11.5966\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 7.86645\n",
      "Epoch 124/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 7.2558 - val_loss: 9.6686\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 7.86645\n",
      "Epoch 125/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 8.0443 - val_loss: 8.9824\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 7.86645\n",
      "Epoch 126/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 7.4554 - val_loss: 8.9806\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 7.86645\n",
      "Epoch 127/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.3834 - val_loss: 12.0035\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 7.86645\n",
      "Epoch 128/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.5174 - val_loss: 12.8105\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 7.86645\n",
      "Epoch 129/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 7.4272 - val_loss: 8.7431\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 7.86645\n",
      "Epoch 130/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 7.5422 - val_loss: 9.2955\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 7.86645\n",
      "Epoch 131/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.0583 - val_loss: 10.2337\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 7.86645\n",
      "Epoch 132/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.4074 - val_loss: 12.0383\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 7.86645\n",
      "Epoch 133/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.0472 - val_loss: 8.7286\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 7.86645\n",
      "Epoch 134/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 6.9351 - val_loss: 8.1620\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 7.86645\n",
      "Epoch 135/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 6.6825 - val_loss: 19.0591\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 7.86645\n",
      "Epoch 136/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.3552 - val_loss: 9.6747\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 7.86645\n",
      "Epoch 137/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 6.7929 - val_loss: 8.4303\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 7.86645\n",
      "Epoch 138/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 7.0092 - val_loss: 12.1570\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 7.86645\n",
      "Epoch 139/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.8005 - val_loss: 18.8972\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 7.86645\n",
      "Epoch 140/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.7805 - val_loss: 8.9933\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 7.86645\n",
      "Epoch 141/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 6.5556 - val_loss: 11.4238\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 7.86645\n",
      "Epoch 142/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 6.7016 - val_loss: 26.1667\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 7.86645\n",
      "Epoch 143/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 6.6552 - val_loss: 9.4673\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 7.86645\n",
      "Epoch 144/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 6.5835 - val_loss: 9.0782\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 7.86645\n",
      "Epoch 145/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 6.9837 - val_loss: 23.9978\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 7.86645\n",
      "Epoch 146/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.5156 - val_loss: 15.4387\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 7.86645\n",
      "Epoch 147/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 6.9235 - val_loss: 10.6901\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 7.86645\n",
      "Epoch 148/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 6.3281 - val_loss: 9.6486\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 7.86645\n",
      "Epoch 149/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 6.8802 - val_loss: 12.1543\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 7.86645\n",
      "Epoch 150/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.4065 - val_loss: 8.6044\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 7.86645\n",
      "Epoch 151/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 6.4294 - val_loss: 11.2805\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 7.86645\n",
      "Epoch 152/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 6.4249 - val_loss: 17.3312\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 7.86645\n",
      "Epoch 153/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.2608 - val_loss: 17.7608\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 7.86645\n",
      "Epoch 154/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 6.5801 - val_loss: 14.4339\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 7.86645\n",
      "Epoch 155/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.0658 - val_loss: 13.5790\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 7.86645\n",
      "Epoch 156/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 7.0789 - val_loss: 23.0651\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 7.86645\n",
      "Epoch 157/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 5.9135 - val_loss: 13.6426\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 7.86645\n",
      "Epoch 158/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 6.4719 - val_loss: 9.8419\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 7.86645\n",
      "Epoch 159/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 6.4189 - val_loss: 8.5464\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 7.86645\n",
      "Epoch 160/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.9130 - val_loss: 9.2345\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 7.86645\n",
      "Epoch 161/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.1093 - val_loss: 10.3161\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 7.86645\n",
      "Epoch 162/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.1979 - val_loss: 12.6991\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 7.86645\n",
      "Epoch 163/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 6.0043 - val_loss: 8.6984\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 7.86645\n",
      "Epoch 164/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 6.1470 - val_loss: 25.9750\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 7.86645\n",
      "Epoch 165/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 6.1517 - val_loss: 8.8792\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 7.86645\n",
      "Epoch 166/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 6.0923 - val_loss: 22.0246\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 7.86645\n",
      "Epoch 167/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 6.1603 - val_loss: 10.9592\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 7.86645\n",
      "Epoch 168/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 6.5541 - val_loss: 9.5184\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 7.86645\n",
      "Epoch 169/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.6401 - val_loss: 11.4950\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 7.86645\n",
      "Epoch 170/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 6.0125 - val_loss: 14.5734\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 7.86645\n",
      "Epoch 171/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.6897 - val_loss: 10.0789\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 7.86645\n",
      "Epoch 172/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.7474 - val_loss: 13.0031\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 7.86645\n",
      "Epoch 173/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 6.0540 - val_loss: 11.1332\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 7.86645\n",
      "Epoch 174/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 5.7415 - val_loss: 11.2105\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 7.86645\n",
      "Epoch 175/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 5.6045 - val_loss: 8.5149\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 7.86645\n",
      "Epoch 176/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 5.8033 - val_loss: 9.2389\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 7.86645\n",
      "Epoch 177/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 5.8354 - val_loss: 8.5067\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 7.86645\n",
      "Epoch 178/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 5.4708 - val_loss: 9.5878\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 7.86645\n",
      "Epoch 179/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 5.5455 - val_loss: 11.2178\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 7.86645\n",
      "Epoch 180/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 6.3444 - val_loss: 9.6467\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 7.86645\n",
      "Epoch 181/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.3452 - val_loss: 19.1847\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 7.86645\n",
      "Epoch 182/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.4179 - val_loss: 9.5795\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 7.86645\n",
      "Epoch 183/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.4026 - val_loss: 9.1178\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 7.86645\n",
      "Epoch 184/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.5609 - val_loss: 12.1381\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 7.86645\n",
      "Epoch 185/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.6181 - val_loss: 10.2650\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 7.86645\n",
      "Epoch 186/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.7013 - val_loss: 12.8554\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 7.86645\n",
      "Epoch 187/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.1266 - val_loss: 11.4084\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 7.86645\n",
      "Epoch 188/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.7528 - val_loss: 10.6845\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 7.86645\n",
      "Epoch 189/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.4628 - val_loss: 10.0804\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 7.86645\n",
      "Epoch 190/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.4907 - val_loss: 11.3460\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 7.86645\n",
      "Epoch 191/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.5149 - val_loss: 15.5327\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 7.86645\n",
      "Epoch 192/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.2814 - val_loss: 18.4735\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 7.86645\n",
      "Epoch 193/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.4627 - val_loss: 9.9339\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 7.86645\n",
      "Epoch 194/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.3031 - val_loss: 10.5743\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 7.86645\n",
      "Epoch 195/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.1185 - val_loss: 17.4847\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 7.86645\n",
      "Epoch 196/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.7964 - val_loss: 8.6902\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 7.86645\n",
      "Epoch 197/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 5.4336 - val_loss: 9.0948\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 7.86645\n",
      "Epoch 198/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.5541 - val_loss: 12.3272\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 7.86645\n",
      "Epoch 199/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.8773 - val_loss: 10.0151\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 7.86645\n",
      "Epoch 200/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.3484 - val_loss: 10.0212\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 7.86645\n",
      "Epoch 201/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.3580 - val_loss: 9.2884\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 7.86645\n",
      "Epoch 202/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 5.1802 - val_loss: 16.6390\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 7.86645\n",
      "Epoch 203/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.0850 - val_loss: 10.0751\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 7.86645\n",
      "Epoch 204/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.8006 - val_loss: 10.1275\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 7.86645\n",
      "Epoch 205/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.4508 - val_loss: 11.3952\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 7.86645\n",
      "Epoch 206/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.8014 - val_loss: 10.2230\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 7.86645\n",
      "Epoch 207/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.0800 - val_loss: 11.9011\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 7.86645\n",
      "Epoch 208/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.1119 - val_loss: 18.7449\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 7.86645\n",
      "Epoch 209/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 5.1902 - val_loss: 10.4462\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 7.86645\n",
      "Epoch 210/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.0666 - val_loss: 20.2769\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 7.86645\n",
      "Epoch 211/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.0574 - val_loss: 10.8998\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 7.86645\n",
      "Epoch 212/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 5.0129 - val_loss: 11.3969\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 7.86645\n",
      "Epoch 213/2000\n",
      "34103/34103 [==============================] - 4s 110us/step - loss: 4.6980 - val_loss: 13.1461\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 7.86645\n",
      "Epoch 214/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.1186 - val_loss: 11.4349\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 7.86645\n",
      "Epoch 215/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.5875 - val_loss: 9.1352\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 7.86645\n",
      "Epoch 216/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.5228 - val_loss: 9.4810\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 7.86645\n",
      "Epoch 217/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.9084 - val_loss: 15.9083\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 7.86645\n",
      "Epoch 218/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 5.4336 - val_loss: 10.9352\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 7.86645\n",
      "Epoch 219/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.7784 - val_loss: 9.2694\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 7.86645\n",
      "Epoch 220/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.5640 - val_loss: 9.1094\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 7.86645\n",
      "Epoch 221/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 4.4306 - val_loss: 10.0394\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 7.86645\n",
      "Epoch 222/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.6090 - val_loss: 10.8114\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 7.86645\n",
      "Epoch 223/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.6545 - val_loss: 13.9044\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 7.86645\n",
      "Epoch 224/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.3463 - val_loss: 9.8609\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 7.86645\n",
      "Epoch 225/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 4.5583 - val_loss: 10.3975\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 7.86645\n",
      "Epoch 226/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.6931 - val_loss: 11.2991\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 7.86645\n",
      "Epoch 227/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.5822 - val_loss: 9.2826\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 7.86645\n",
      "Epoch 228/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.4653 - val_loss: 10.2983\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 7.86645\n",
      "Epoch 229/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 4.8087 - val_loss: 10.8807\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 7.86645\n",
      "Epoch 230/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 5.5909 - val_loss: 14.1844\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 7.86645\n",
      "Epoch 231/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.3469 - val_loss: 9.8376\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 7.86645\n",
      "Epoch 232/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.5723 - val_loss: 15.2426\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 7.86645\n",
      "Epoch 233/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.4578 - val_loss: 9.5247\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 7.86645\n",
      "Epoch 234/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.5436 - val_loss: 12.6887\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 7.86645\n",
      "Epoch 235/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.3087 - val_loss: 10.9458\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 7.86645\n",
      "Epoch 236/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.2164 - val_loss: 15.4143\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 7.86645\n",
      "Epoch 237/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.8776 - val_loss: 13.2199\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 7.86645\n",
      "Epoch 238/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.2625 - val_loss: 10.0668\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 7.86645\n",
      "Epoch 239/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 4.4600 - val_loss: 19.1700\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 7.86645\n",
      "Epoch 240/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.2205 - val_loss: 9.9863\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 7.86645\n",
      "Epoch 241/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.9994 - val_loss: 10.7006\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 7.86645\n",
      "Epoch 242/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 4.3195 - val_loss: 11.3791\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 7.86645\n",
      "Epoch 243/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.2834 - val_loss: 10.1246\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 7.86645\n",
      "Epoch 244/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 4.2251 - val_loss: 10.8406\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 7.86645\n",
      "Epoch 245/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 4.2209 - val_loss: 10.3261\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 7.86645\n",
      "Epoch 246/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.3113 - val_loss: 10.5787\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 7.86645\n",
      "Epoch 247/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.9854 - val_loss: 8.9623\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 7.86645\n",
      "Epoch 248/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.2504 - val_loss: 10.6799\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 7.86645\n",
      "Epoch 249/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 4.6933 - val_loss: 10.1984\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 7.86645\n",
      "Epoch 250/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.2783 - val_loss: 9.7989\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 7.86645\n",
      "Epoch 251/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.0186 - val_loss: 12.8063\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 7.86645\n",
      "Epoch 252/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.4655 - val_loss: 9.2450\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 7.86645\n",
      "Epoch 253/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.3174 - val_loss: 17.2066\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 7.86645\n",
      "Epoch 254/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.1648 - val_loss: 10.9789\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 7.86645\n",
      "Epoch 255/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.0211 - val_loss: 12.0956\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 7.86645\n",
      "Epoch 256/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.2714 - val_loss: 9.4690\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 7.86645\n",
      "Epoch 257/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.5222 - val_loss: 21.3634\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 7.86645\n",
      "Epoch 258/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.1760 - val_loss: 9.5982\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 7.86645\n",
      "Epoch 259/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.9803 - val_loss: 9.1768\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 7.86645\n",
      "Epoch 260/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.0169 - val_loss: 12.7065\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 7.86645\n",
      "Epoch 261/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.0402 - val_loss: 18.5520\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 7.86645\n",
      "Epoch 262/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 4.1858 - val_loss: 9.7557\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 7.86645\n",
      "Epoch 263/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.0046 - val_loss: 9.7207\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 7.86645\n",
      "Epoch 264/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.9992 - val_loss: 11.6141\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 7.86645\n",
      "Epoch 265/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.8464 - val_loss: 9.9749\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 7.86645\n",
      "Epoch 266/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.7715 - val_loss: 10.4394\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 7.86645\n",
      "Epoch 267/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.0450 - val_loss: 10.1546\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 7.86645\n",
      "Epoch 268/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.8982 - val_loss: 11.0821\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 7.86645\n",
      "Epoch 269/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.2522 - val_loss: 10.6784\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 7.86645\n",
      "Epoch 270/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.7183 - val_loss: 8.7544\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 7.86645\n",
      "Epoch 271/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 4.1586 - val_loss: 9.5804\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 7.86645\n",
      "Epoch 272/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.1324 - val_loss: 10.4072\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 7.86645\n",
      "Epoch 273/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 3.7523 - val_loss: 9.6659\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 7.86645\n",
      "Epoch 274/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.0103 - val_loss: 9.7862\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 7.86645\n",
      "Epoch 275/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.1320 - val_loss: 13.1635\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 7.86645\n",
      "Epoch 276/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.5402 - val_loss: 14.3141\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 7.86645\n",
      "Epoch 277/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 4.3327 - val_loss: 10.0734\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 7.86645\n",
      "Epoch 278/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.8456 - val_loss: 11.5818\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 7.86645\n",
      "Epoch 279/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.5844 - val_loss: 9.2778\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 7.86645\n",
      "Epoch 280/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.9346 - val_loss: 11.2928\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 7.86645\n",
      "Epoch 281/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.6805 - val_loss: 10.0023\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 7.86645\n",
      "Epoch 282/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.7945 - val_loss: 27.9779\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 7.86645\n",
      "Epoch 283/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.2281 - val_loss: 11.3056\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 7.86645\n",
      "Epoch 284/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.7840 - val_loss: 9.9664\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 7.86645\n",
      "Epoch 285/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 4.0114 - val_loss: 11.2856\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 7.86645\n",
      "Epoch 286/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.6862 - val_loss: 9.9111\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 7.86645\n",
      "Epoch 287/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 3.6297 - val_loss: 10.1626\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 7.86645\n",
      "Epoch 288/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.9889 - val_loss: 11.0271\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 7.86645\n",
      "Epoch 289/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 4.0302 - val_loss: 10.2980\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 7.86645\n",
      "Epoch 290/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3354 - val_loss: 15.4838\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 7.86645\n",
      "Epoch 291/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.8428 - val_loss: 10.5588\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 7.86645\n",
      "Epoch 292/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.5111 - val_loss: 15.4745\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 7.86645\n",
      "Epoch 293/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.9168 - val_loss: 10.7388\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 7.86645\n",
      "Epoch 294/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.4754 - val_loss: 9.5253\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 7.86645\n",
      "Epoch 295/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.5100 - val_loss: 9.9646\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 7.86645\n",
      "Epoch 296/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.8592 - val_loss: 16.3197\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 7.86645\n",
      "Epoch 297/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.5009 - val_loss: 10.1571\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 7.86645\n",
      "Epoch 298/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.9061 - val_loss: 10.5629\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 7.86645\n",
      "Epoch 299/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.9430 - val_loss: 10.0040\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 7.86645\n",
      "Epoch 300/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.6364 - val_loss: 10.3034\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 7.86645\n",
      "Epoch 301/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.6104 - val_loss: 12.3217\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 7.86645\n",
      "Epoch 302/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.3432 - val_loss: 10.9378\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 7.86645\n",
      "Epoch 303/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.8987 - val_loss: 14.3275\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 7.86645\n",
      "Epoch 304/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.4742 - val_loss: 9.0974\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 7.86645\n",
      "Epoch 305/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.8467 - val_loss: 14.0101\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 7.86645\n",
      "Epoch 306/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.3810 - val_loss: 13.2392\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 7.86645\n",
      "Epoch 307/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.5407 - val_loss: 10.0341\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 7.86645\n",
      "Epoch 308/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.4563 - val_loss: 10.8238\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 7.86645\n",
      "Epoch 309/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.5499 - val_loss: 16.5624\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 7.86645\n",
      "Epoch 310/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 3.5264 - val_loss: 11.9658\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 7.86645\n",
      "Epoch 311/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.4298 - val_loss: 8.8978\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 7.86645\n",
      "Epoch 312/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3706 - val_loss: 9.4355\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 7.86645\n",
      "Epoch 313/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 3.5543 - val_loss: 16.3781\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 7.86645\n",
      "Epoch 314/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 3.4078 - val_loss: 9.5316\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 7.86645\n",
      "Epoch 315/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.6223 - val_loss: 9.7501\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 7.86645\n",
      "Epoch 316/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.0137 - val_loss: 13.3131\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 7.86645\n",
      "Epoch 317/2000\n",
      "34103/34103 [==============================] - 4s 121us/step - loss: 3.7296 - val_loss: 10.0846\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 7.86645\n",
      "Epoch 318/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.1015 - val_loss: 21.8370\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 7.86645\n",
      "Epoch 319/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 4.1157 - val_loss: 9.9747\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 7.86645\n",
      "Epoch 320/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.3749 - val_loss: 12.3359\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 7.86645\n",
      "Epoch 321/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 3.2185 - val_loss: 10.0327\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 7.86645\n",
      "Epoch 322/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.2820 - val_loss: 9.5988\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 7.86645\n",
      "Epoch 323/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.3683 - val_loss: 9.6706\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 7.86645\n",
      "Epoch 324/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.2210 - val_loss: 10.5861\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 7.86645\n",
      "Epoch 325/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.4711 - val_loss: 10.4289\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 7.86645\n",
      "Epoch 326/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.5777 - val_loss: 9.9100\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 7.86645\n",
      "Epoch 327/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.1350 - val_loss: 9.2769\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 7.86645\n",
      "Epoch 328/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 3.7546 - val_loss: 10.6952\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 7.86645\n",
      "Epoch 329/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.3894 - val_loss: 9.3943\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 7.86645\n",
      "Epoch 330/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.0874 - val_loss: 12.1590\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 7.86645\n",
      "Epoch 331/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1400 - val_loss: 11.1805\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 7.86645\n",
      "Epoch 332/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3499 - val_loss: 11.0771\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 7.86645\n",
      "Epoch 333/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.4158 - val_loss: 10.1345\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 7.86645\n",
      "Epoch 334/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.1854 - val_loss: 10.8504\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 7.86645\n",
      "Epoch 335/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.3855 - val_loss: 11.2570\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 7.86645\n",
      "Epoch 336/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.4892 - val_loss: 9.8664\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 7.86645\n",
      "Epoch 337/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.4910 - val_loss: 10.6851\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 7.86645\n",
      "Epoch 338/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9676 - val_loss: 10.3042\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 7.86645\n",
      "Epoch 339/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 3.1193 - val_loss: 12.6428\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 7.86645\n",
      "Epoch 340/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.1100 - val_loss: 10.5879\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 7.86645\n",
      "Epoch 341/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.2228 - val_loss: 10.3790\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 7.86645\n",
      "Epoch 342/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.6744 - val_loss: 15.0813\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 7.86645\n",
      "Epoch 343/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.0655 - val_loss: 11.6028\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 7.86645\n",
      "Epoch 344/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1112 - val_loss: 18.1953\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 7.86645\n",
      "Epoch 345/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3538 - val_loss: 22.3872\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 7.86645\n",
      "Epoch 346/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1022 - val_loss: 21.0503\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 7.86645\n",
      "Epoch 347/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3422 - val_loss: 9.5671\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 7.86645\n",
      "Epoch 348/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 3.0863 - val_loss: 9.9148\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 7.86645\n",
      "Epoch 349/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.9424 - val_loss: 14.2696\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 7.86645\n",
      "Epoch 350/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.2728 - val_loss: 9.9468\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 7.86645\n",
      "Epoch 351/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.7529 - val_loss: 11.3940\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 7.86645\n",
      "Epoch 352/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0758 - val_loss: 11.8634\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 7.86645\n",
      "Epoch 353/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9951 - val_loss: 13.0660\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 7.86645\n",
      "Epoch 354/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.3320 - val_loss: 11.7892\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 7.86645\n",
      "Epoch 355/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.9992 - val_loss: 10.5480\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 7.86645\n",
      "Epoch 356/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.8660 - val_loss: 10.9052\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 7.86645\n",
      "Epoch 357/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.6614 - val_loss: 9.7243\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 7.86645\n",
      "Epoch 358/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.9176 - val_loss: 10.0559\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 7.86645\n",
      "Epoch 359/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.9180 - val_loss: 10.4423\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 7.86645\n",
      "Epoch 360/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1668 - val_loss: 12.3426\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 7.86645\n",
      "Epoch 361/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0063 - val_loss: 11.3777\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 7.86645\n",
      "Epoch 362/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0193 - val_loss: 9.6920\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 7.86645\n",
      "Epoch 363/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.8505 - val_loss: 10.1056\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 7.86645\n",
      "Epoch 364/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 3.1644 - val_loss: 9.7118\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 7.86645\n",
      "Epoch 365/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.8740 - val_loss: 10.1162\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 7.86645\n",
      "Epoch 366/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.3294 - val_loss: 13.2071\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 7.86645\n",
      "Epoch 367/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1268 - val_loss: 12.4759\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 7.86645\n",
      "Epoch 368/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0766 - val_loss: 13.3511\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 7.86645\n",
      "Epoch 369/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0270 - val_loss: 13.0505\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 7.86645\n",
      "Epoch 370/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0557 - val_loss: 11.8212\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 7.86645\n",
      "Epoch 371/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.5909 - val_loss: 15.5853\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 7.86645\n",
      "Epoch 372/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.9780 - val_loss: 11.2506\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 7.86645\n",
      "Epoch 373/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.8135 - val_loss: 12.3370\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 7.86645\n",
      "Epoch 374/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9769 - val_loss: 10.7495\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 7.86645\n",
      "Epoch 375/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9313 - val_loss: 10.6886\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 7.86645\n",
      "Epoch 376/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 3.3004 - val_loss: 10.1681\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 7.86645\n",
      "Epoch 377/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.8000 - val_loss: 11.4568\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 7.86645\n",
      "Epoch 378/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.8842 - val_loss: 12.4621\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 7.86645\n",
      "Epoch 379/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7876 - val_loss: 10.0024\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 7.86645\n",
      "Epoch 380/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.7828 - val_loss: 10.4295\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 7.86645\n",
      "Epoch 381/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 3.2369 - val_loss: 18.0244\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 7.86645\n",
      "Epoch 382/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 3.0343 - val_loss: 9.5405\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 7.86645\n",
      "Epoch 383/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.8469 - val_loss: 9.5687\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 7.86645\n",
      "Epoch 384/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9212 - val_loss: 10.0095\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 7.86645\n",
      "Epoch 385/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.7300 - val_loss: 12.5715\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 7.86645\n",
      "Epoch 386/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1348 - val_loss: 9.2054\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 7.86645\n",
      "Epoch 387/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9016 - val_loss: 10.2252\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 7.86645\n",
      "Epoch 388/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.7080 - val_loss: 9.2946\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 7.86645\n",
      "Epoch 389/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.8305 - val_loss: 10.1698\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 7.86645\n",
      "Epoch 390/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.6077 - val_loss: 10.7406\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 7.86645\n",
      "Epoch 391/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.8752 - val_loss: 10.7573\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 7.86645\n",
      "Epoch 392/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.8526 - val_loss: 13.6799\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 7.86645\n",
      "Epoch 393/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.8813 - val_loss: 10.7169\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 7.86645\n",
      "Epoch 394/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.6330 - val_loss: 9.6978\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 7.86645\n",
      "Epoch 395/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.1168 - val_loss: 10.1401\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 7.86645\n",
      "Epoch 396/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.8290 - val_loss: 11.6503\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 7.86645\n",
      "Epoch 397/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.5528 - val_loss: 12.6042\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 7.86645\n",
      "Epoch 398/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.7455 - val_loss: 11.9645\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 7.86645\n",
      "Epoch 399/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.6808 - val_loss: 10.7846\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 7.86645\n",
      "Epoch 400/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7389 - val_loss: 13.1558\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 7.86645\n",
      "Epoch 401/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.9250 - val_loss: 11.0177\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 7.86645\n",
      "Epoch 402/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.5575 - val_loss: 10.3522\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 7.86645\n",
      "Epoch 403/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.9635 - val_loss: 11.1854\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 7.86645\n",
      "Epoch 404/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.8317 - val_loss: 9.6605\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 7.86645\n",
      "Epoch 405/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.6138 - val_loss: 18.0695\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 7.86645\n",
      "Epoch 406/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.7527 - val_loss: 10.0640\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 7.86645\n",
      "Epoch 407/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.7685 - val_loss: 9.6214\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 7.86645\n",
      "Epoch 408/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 3.0263 - val_loss: 12.4413\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 7.86645\n",
      "Epoch 409/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 2.5308 - val_loss: 12.2918\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 7.86645\n",
      "Epoch 410/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.6198 - val_loss: 9.7552\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 7.86645\n",
      "Epoch 411/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7463 - val_loss: 11.8739\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 7.86645\n",
      "Epoch 412/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.6859 - val_loss: 9.7430\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 7.86645\n",
      "Epoch 413/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.4988 - val_loss: 12.0214\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 7.86645\n",
      "Epoch 414/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7482 - val_loss: 12.2969\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 7.86645\n",
      "Epoch 415/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7276 - val_loss: 9.9371\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 7.86645\n",
      "Epoch 416/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.8316 - val_loss: 10.2690\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 7.86645\n",
      "Epoch 417/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4382 - val_loss: 10.0629\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 7.86645\n",
      "Epoch 418/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.4249 - val_loss: 12.2434\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 7.86645\n",
      "Epoch 419/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.9521 - val_loss: 9.6003\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 7.86645\n",
      "Epoch 420/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.4933 - val_loss: 11.6297\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 7.86645\n",
      "Epoch 421/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.5941 - val_loss: 12.1045\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 7.86645\n",
      "Epoch 422/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.7361 - val_loss: 17.5482\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 7.86645\n",
      "Epoch 423/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.8054 - val_loss: 27.5328\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 7.86645\n",
      "Epoch 424/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.3941 - val_loss: 9.8078\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 7.86645\n",
      "Epoch 425/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 2.6783 - val_loss: 13.8484\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 7.86645\n",
      "Epoch 426/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.5604 - val_loss: 10.0305\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 7.86645\n",
      "Epoch 427/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.7839 - val_loss: 9.9565\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 7.86645\n",
      "Epoch 428/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.6356 - val_loss: 10.2202\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 7.86645\n",
      "Epoch 429/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 2.3361 - val_loss: 10.9261\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 7.86645\n",
      "Epoch 430/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.7930 - val_loss: 10.6203\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 7.86645\n",
      "Epoch 431/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.9918 - val_loss: 9.9839\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 7.86645\n",
      "Epoch 432/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.5807 - val_loss: 10.0984\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 7.86645\n",
      "Epoch 433/2000\n",
      "34103/34103 [==============================] - 4s 120us/step - loss: 2.4825 - val_loss: 10.9858\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 7.86645\n",
      "Epoch 434/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.6207 - val_loss: 13.9954\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 7.86645\n",
      "Epoch 435/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4672 - val_loss: 13.2126\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 7.86645\n",
      "Epoch 436/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2169 - val_loss: 9.9992\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 7.86645\n",
      "Epoch 437/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.6657 - val_loss: 10.8906\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 7.86645\n",
      "Epoch 438/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.6101 - val_loss: 13.1328\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 7.86645\n",
      "Epoch 439/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.6122 - val_loss: 10.0757\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 7.86645\n",
      "Epoch 440/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.7994 - val_loss: 9.9024\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 7.86645\n",
      "Epoch 441/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3896 - val_loss: 9.6177\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 7.86645\n",
      "Epoch 442/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3051 - val_loss: 10.4132\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 7.86645\n",
      "Epoch 443/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.8418 - val_loss: 11.9296\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 7.86645\n",
      "Epoch 444/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.6791 - val_loss: 20.8463\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 7.86645\n",
      "Epoch 445/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.2943 - val_loss: 10.1274\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 7.86645\n",
      "Epoch 446/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.5907 - val_loss: 10.4458\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 7.86645\n",
      "Epoch 447/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.5329 - val_loss: 10.1825\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 7.86645\n",
      "Epoch 448/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.6630 - val_loss: 12.1684\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 7.86645\n",
      "Epoch 449/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.4360 - val_loss: 11.5524\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 7.86645\n",
      "Epoch 450/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3698 - val_loss: 10.2626\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 7.86645\n",
      "Epoch 451/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4964 - val_loss: 10.6486\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 7.86645\n",
      "Epoch 452/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.5474 - val_loss: 10.0597\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 7.86645\n",
      "Epoch 453/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.4330 - val_loss: 10.1997\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 7.86645\n",
      "Epoch 454/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.6105 - val_loss: 13.6196\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 7.86645\n",
      "Epoch 455/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.4037 - val_loss: 9.9773\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 7.86645\n",
      "Epoch 456/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3943 - val_loss: 10.4063\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 7.86645\n",
      "Epoch 457/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3162 - val_loss: 11.0667\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 7.86645\n",
      "Epoch 458/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2239 - val_loss: 11.3745\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 7.86645\n",
      "Epoch 459/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.6135 - val_loss: 10.2637\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 7.86645\n",
      "Epoch 460/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2903 - val_loss: 13.9974\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 7.86645\n",
      "Epoch 461/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.8036 - val_loss: 12.2949\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 7.86645\n",
      "Epoch 462/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.3776 - val_loss: 14.7723\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 7.86645\n",
      "Epoch 463/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.5728 - val_loss: 11.2511\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 7.86645\n",
      "Epoch 464/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.4436 - val_loss: 10.7347\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 7.86645\n",
      "Epoch 465/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1989 - val_loss: 13.3591\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 7.86645\n",
      "Epoch 466/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.1937 - val_loss: 14.3946\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 7.86645\n",
      "Epoch 467/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4024 - val_loss: 10.5121\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 7.86645\n",
      "Epoch 468/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.4196 - val_loss: 11.3708\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 7.86645\n",
      "Epoch 469/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.8460 - val_loss: 12.3924\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 7.86645\n",
      "Epoch 470/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.5095 - val_loss: 10.0001\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 7.86645\n",
      "Epoch 471/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2015 - val_loss: 10.6315\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 7.86645\n",
      "Epoch 472/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.5186 - val_loss: 12.9109\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 7.86645\n",
      "Epoch 473/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2510 - val_loss: 11.7275\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 7.86645\n",
      "Epoch 474/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.4574 - val_loss: 9.9943\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 7.86645\n",
      "Epoch 475/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.3920 - val_loss: 9.8412\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 7.86645\n",
      "Epoch 476/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.1517 - val_loss: 10.1676\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 7.86645\n",
      "Epoch 477/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.3492 - val_loss: 10.4350\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 7.86645\n",
      "Epoch 478/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3625 - val_loss: 13.3047\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 7.86645\n",
      "Epoch 479/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.4634 - val_loss: 10.2197\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 7.86645\n",
      "Epoch 480/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2232 - val_loss: 11.7772\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 7.86645\n",
      "Epoch 481/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1910 - val_loss: 10.2330\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 7.86645\n",
      "Epoch 482/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.4972 - val_loss: 11.4203\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 7.86645\n",
      "Epoch 483/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4408 - val_loss: 10.1030\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 7.86645\n",
      "Epoch 484/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.3102 - val_loss: 12.8959\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 7.86645\n",
      "Epoch 485/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.2743 - val_loss: 11.0796\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 7.86645\n",
      "Epoch 486/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.5070 - val_loss: 12.5074\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 7.86645\n",
      "Epoch 487/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.5794 - val_loss: 11.2180\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 7.86645\n",
      "Epoch 488/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.1075 - val_loss: 10.4199\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 7.86645\n",
      "Epoch 489/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2379 - val_loss: 9.8819\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 7.86645\n",
      "Epoch 490/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3448 - val_loss: 11.7100\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 7.86645\n",
      "Epoch 491/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3178 - val_loss: 11.5677\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 7.86645\n",
      "Epoch 492/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2345 - val_loss: 10.1371\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 7.86645\n",
      "Epoch 493/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.4052 - val_loss: 10.5947\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 7.86645\n",
      "Epoch 494/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.1752 - val_loss: 10.7663\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 7.86645\n",
      "Epoch 495/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.4423 - val_loss: 12.3089\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 7.86645\n",
      "Epoch 496/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3392 - val_loss: 11.3596\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 7.86645\n",
      "Epoch 497/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2785 - val_loss: 10.2399\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 7.86645\n",
      "Epoch 498/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3396 - val_loss: 10.0325\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 7.86645\n",
      "Epoch 499/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3171 - val_loss: 11.1774\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 7.86645\n",
      "Epoch 500/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2549 - val_loss: 10.7357\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 7.86645\n",
      "Epoch 501/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.6331 - val_loss: 13.4706\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 7.86645\n",
      "Epoch 502/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.1819 - val_loss: 10.0862\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 7.86645\n",
      "Epoch 503/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.0925 - val_loss: 11.7010\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 7.86645\n",
      "Epoch 504/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.1663 - val_loss: 12.5787\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 7.86645\n",
      "Epoch 505/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3453 - val_loss: 12.3989\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 7.86645\n",
      "Epoch 506/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2146 - val_loss: 10.9994\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 7.86645\n",
      "Epoch 507/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0792 - val_loss: 11.8690\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 7.86645\n",
      "Epoch 508/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3120 - val_loss: 20.2258\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 7.86645\n",
      "Epoch 509/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.3047 - val_loss: 10.4272\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 7.86645\n",
      "Epoch 510/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2909 - val_loss: 12.8431\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 7.86645\n",
      "Epoch 511/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.3036 - val_loss: 11.2417\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 7.86645\n",
      "Epoch 512/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2135 - val_loss: 10.4853\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 7.86645\n",
      "Epoch 513/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2043 - val_loss: 11.6896\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 7.86645\n",
      "Epoch 514/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3414 - val_loss: 11.1135\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 7.86645\n",
      "Epoch 515/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1598 - val_loss: 10.1311\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 7.86645\n",
      "Epoch 516/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2002 - val_loss: 10.0605\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 7.86645\n",
      "Epoch 517/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2128 - val_loss: 10.8388\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 7.86645\n",
      "Epoch 518/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.1135 - val_loss: 9.6475\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 7.86645\n",
      "Epoch 519/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.3384 - val_loss: 11.3471\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 7.86645\n",
      "Epoch 520/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.2762 - val_loss: 11.2138\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 7.86645\n",
      "Epoch 521/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0643 - val_loss: 13.3380\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 7.86645\n",
      "Epoch 522/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2486 - val_loss: 10.7863\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 7.86645\n",
      "Epoch 523/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2525 - val_loss: 10.4398\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 7.86645\n",
      "Epoch 524/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.4005 - val_loss: 18.2353\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 7.86645\n",
      "Epoch 525/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1661 - val_loss: 17.8965\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 7.86645\n",
      "Epoch 526/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.3252 - val_loss: 12.0051\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 7.86645\n",
      "Epoch 527/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0250 - val_loss: 12.1615\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 7.86645\n",
      "Epoch 528/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2505 - val_loss: 13.7708\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 7.86645\n",
      "Epoch 529/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0058 - val_loss: 10.4693\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 7.86645\n",
      "Epoch 530/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1671 - val_loss: 11.2832\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 7.86645\n",
      "Epoch 531/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2101 - val_loss: 10.6692\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 7.86645\n",
      "Epoch 532/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9607 - val_loss: 22.0334\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 7.86645\n",
      "Epoch 533/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.2297 - val_loss: 11.6946\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 7.86645\n",
      "Epoch 534/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2459 - val_loss: 10.3697\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 7.86645\n",
      "Epoch 535/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.3586 - val_loss: 10.9797\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 7.86645\n",
      "Epoch 536/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.0659 - val_loss: 10.4514\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 7.86645\n",
      "Epoch 537/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.1055 - val_loss: 18.6034\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 7.86645\n",
      "Epoch 538/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0255 - val_loss: 12.5948\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 7.86645\n",
      "Epoch 539/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1296 - val_loss: 11.3009\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 7.86645\n",
      "Epoch 540/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3363 - val_loss: 10.2671\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 7.86645\n",
      "Epoch 541/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0773 - val_loss: 12.8866\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 7.86645\n",
      "Epoch 542/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0818 - val_loss: 10.9065\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 7.86645\n",
      "Epoch 543/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.3062 - val_loss: 11.0103\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 7.86645\n",
      "Epoch 544/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2919 - val_loss: 12.2007\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 7.86645\n",
      "Epoch 545/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9971 - val_loss: 14.6592\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 7.86645\n",
      "Epoch 546/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0235 - val_loss: 12.8110\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 7.86645\n",
      "Epoch 547/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.3069 - val_loss: 11.1654\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 7.86645\n",
      "Epoch 548/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2016 - val_loss: 9.9269\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 7.86645\n",
      "Epoch 549/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0112 - val_loss: 18.4012\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 7.86645\n",
      "Epoch 550/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9548 - val_loss: 9.7146\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 7.86645\n",
      "Epoch 551/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0951 - val_loss: 17.2040\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 7.86645\n",
      "Epoch 552/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.2049 - val_loss: 11.5089\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 7.86645\n",
      "Epoch 553/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.0524 - val_loss: 11.7022\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 7.86645\n",
      "Epoch 554/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8924 - val_loss: 9.7903\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 7.86645\n",
      "Epoch 555/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2892 - val_loss: 10.0606\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 7.86645\n",
      "Epoch 556/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0293 - val_loss: 10.6390\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 7.86645\n",
      "Epoch 557/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1539 - val_loss: 13.0200\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 7.86645\n",
      "Epoch 558/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1900 - val_loss: 12.3091\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 7.86645\n",
      "Epoch 559/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1551 - val_loss: 10.1882\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 7.86645\n",
      "Epoch 560/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.1600 - val_loss: 14.5188\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 7.86645\n",
      "Epoch 561/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.0561 - val_loss: 10.7693\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 7.86645\n",
      "Epoch 562/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0596 - val_loss: 10.7877\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 7.86645\n",
      "Epoch 563/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 1.8845 - val_loss: 16.5906\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 7.86645\n",
      "Epoch 564/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2733 - val_loss: 13.1841\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 7.86645\n",
      "Epoch 565/2000\n",
      "34103/34103 [==============================] - 4s 123us/step - loss: 2.2925 - val_loss: 10.1526\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 7.86645\n",
      "Epoch 566/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.0648 - val_loss: 11.1616\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 7.86645\n",
      "Epoch 567/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 1.9052 - val_loss: 10.8946\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 7.86645\n",
      "Epoch 568/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0459 - val_loss: 13.9925\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 7.86645\n",
      "Epoch 569/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.2217 - val_loss: 10.4982\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 7.86645\n",
      "Epoch 570/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.8848 - val_loss: 12.7503\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 7.86645\n",
      "Epoch 571/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.2714 - val_loss: 10.6743\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 7.86645\n",
      "Epoch 572/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8508 - val_loss: 14.3348\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 7.86645\n",
      "Epoch 573/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.3256 - val_loss: 9.8335\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 7.86645\n",
      "Epoch 574/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8329 - val_loss: 10.3551\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 7.86645\n",
      "Epoch 575/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.1406 - val_loss: 10.1482\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 7.86645\n",
      "Epoch 576/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.9855 - val_loss: 10.5916\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 7.86645\n",
      "Epoch 577/2000\n",
      "34103/34103 [==============================] - 4s 110us/step - loss: 2.0460 - val_loss: 9.7550\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 7.86645\n",
      "Epoch 578/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0790 - val_loss: 13.8756\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 7.86645\n",
      "Epoch 579/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.1483 - val_loss: 16.4483\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 7.86645\n",
      "Epoch 580/2000\n",
      "34103/34103 [==============================] - 4s 110us/step - loss: 2.0818 - val_loss: 11.2898\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 7.86645\n",
      "Epoch 581/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9058 - val_loss: 11.8369\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 7.86645\n",
      "Epoch 582/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 1.9210 - val_loss: 10.7432\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 7.86645\n",
      "Epoch 583/2000\n",
      "34103/34103 [==============================] - 4s 119us/step - loss: 2.1101 - val_loss: 13.1589\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 7.86645\n",
      "Epoch 584/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9241 - val_loss: 13.4690\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 7.86645\n",
      "Epoch 585/2000\n",
      "34103/34103 [==============================] - 4s 118us/step - loss: 2.0296 - val_loss: 10.3805\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 7.86645\n",
      "Epoch 586/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.9815 - val_loss: 11.5615\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 7.86645\n",
      "Epoch 587/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1586 - val_loss: 11.9792\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 7.86645\n",
      "Epoch 588/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.8509 - val_loss: 11.6037\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 7.86645\n",
      "Epoch 589/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.2622 - val_loss: 11.0322\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 7.86645\n",
      "Epoch 590/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9272 - val_loss: 11.4804\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 7.86645\n",
      "Epoch 591/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9930 - val_loss: 13.3518\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 7.86645\n",
      "Epoch 592/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8802 - val_loss: 10.2345\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 7.86645\n",
      "Epoch 593/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.1594 - val_loss: 10.6292\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 7.86645\n",
      "Epoch 594/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.8390 - val_loss: 11.3617\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 7.86645\n",
      "Epoch 595/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.2294 - val_loss: 11.3632\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 7.86645\n",
      "Epoch 596/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0102 - val_loss: 10.5909\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 7.86645\n",
      "Epoch 597/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8473 - val_loss: 13.9133\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 7.86645\n",
      "Epoch 598/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 2.0740 - val_loss: 11.0657\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 7.86645\n",
      "Epoch 599/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1812 - val_loss: 16.9798\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 7.86645\n",
      "Epoch 600/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 1.8753 - val_loss: 10.8452\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 7.86645\n",
      "Epoch 601/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1667 - val_loss: 10.0293\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 7.86645\n",
      "Epoch 602/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0041 - val_loss: 11.1850\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 7.86645\n",
      "Epoch 603/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.8445 - val_loss: 10.3669\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 7.86645\n",
      "Epoch 604/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.9099 - val_loss: 10.2678\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 7.86645\n",
      "Epoch 605/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9968 - val_loss: 10.7596\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 7.86645\n",
      "Epoch 606/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9930 - val_loss: 11.9621\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 7.86645\n",
      "Epoch 607/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7328 - val_loss: 10.6050\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 7.86645\n",
      "Epoch 608/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0509 - val_loss: 13.8157\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 7.86645\n",
      "Epoch 609/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0927 - val_loss: 10.6699\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 7.86645\n",
      "Epoch 610/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9885 - val_loss: 11.5980\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 7.86645\n",
      "Epoch 611/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.0200 - val_loss: 11.5032\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 7.86645\n",
      "Epoch 612/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.7847 - val_loss: 18.5014\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 7.86645\n",
      "Epoch 613/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0717 - val_loss: 10.5646\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 7.86645\n",
      "Epoch 614/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8575 - val_loss: 10.5940\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 7.86645\n",
      "Epoch 615/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0220 - val_loss: 11.5636\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 7.86645\n",
      "Epoch 616/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 1.9435 - val_loss: 14.5066\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 7.86645\n",
      "Epoch 617/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0950 - val_loss: 11.8315\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 7.86645\n",
      "Epoch 618/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9717 - val_loss: 10.1894\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 7.86645\n",
      "Epoch 619/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.7761 - val_loss: 11.2961\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 7.86645\n",
      "Epoch 620/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.8133 - val_loss: 10.7022\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 7.86645\n",
      "Epoch 621/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.1270 - val_loss: 10.9492\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 7.86645\n",
      "Epoch 622/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8097 - val_loss: 15.7170\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 7.86645\n",
      "Epoch 623/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8536 - val_loss: 12.4394\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 7.86645\n",
      "Epoch 624/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9845 - val_loss: 11.8830\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 7.86645\n",
      "Epoch 625/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8452 - val_loss: 11.0961\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 7.86645\n",
      "Epoch 626/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9635 - val_loss: 10.6649\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 7.86645\n",
      "Epoch 627/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.1436 - val_loss: 11.9713\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 7.86645\n",
      "Epoch 628/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 2.0140 - val_loss: 12.1099\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 7.86645\n",
      "Epoch 629/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0870 - val_loss: 11.1075\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 7.86645\n",
      "Epoch 630/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7378 - val_loss: 10.9795\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 7.86645\n",
      "Epoch 631/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.7882 - val_loss: 11.7438\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 7.86645\n",
      "Epoch 632/2000\n",
      "34103/34103 [==============================] - 4s 109us/step - loss: 2.0843 - val_loss: 10.4688\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 7.86645\n",
      "Epoch 633/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0593 - val_loss: 13.8396\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 7.86645\n",
      "Epoch 634/2000\n",
      "34103/34103 [==============================] - 4s 116us/step - loss: 1.7573 - val_loss: 10.4644\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 7.86645\n",
      "Epoch 635/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.9825 - val_loss: 11.5137\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 7.86645\n",
      "Epoch 636/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9202 - val_loss: 11.8422\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 7.86645\n",
      "Epoch 637/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8381 - val_loss: 10.3665\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 7.86645\n",
      "Epoch 638/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 2.0159 - val_loss: 9.7793\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 7.86645\n",
      "Epoch 639/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9224 - val_loss: 10.3220\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 7.86645\n",
      "Epoch 640/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.7907 - val_loss: 10.9014\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 7.86645\n",
      "Epoch 641/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.8850 - val_loss: 11.6855\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 7.86645\n",
      "Epoch 642/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.8870 - val_loss: 10.3111\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 7.86645\n",
      "Epoch 643/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9597 - val_loss: 10.2986\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 7.86645\n",
      "Epoch 644/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9389 - val_loss: 11.8680\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 7.86645\n",
      "Epoch 645/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8702 - val_loss: 11.4400\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 7.86645\n",
      "Epoch 646/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9882 - val_loss: 15.5088\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 7.86645\n",
      "Epoch 647/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.9929 - val_loss: 10.9770\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 7.86645\n",
      "Epoch 648/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9969 - val_loss: 10.7059\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 7.86645\n",
      "Epoch 649/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.7082 - val_loss: 17.6648\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 7.86645\n",
      "Epoch 650/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9331 - val_loss: 10.7308\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 7.86645\n",
      "Epoch 651/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9092 - val_loss: 10.5044\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 7.86645\n",
      "Epoch 652/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8618 - val_loss: 10.5077\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 7.86645\n",
      "Epoch 653/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.7930 - val_loss: 12.7790\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 7.86645\n",
      "Epoch 654/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8309 - val_loss: 10.6336\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 7.86645\n",
      "Epoch 655/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.7437 - val_loss: 12.2948\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 7.86645\n",
      "Epoch 656/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 2.0645 - val_loss: 10.7726\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 7.86645\n",
      "Epoch 657/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.7981 - val_loss: 10.6180\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 7.86645\n",
      "Epoch 658/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8926 - val_loss: 10.9608\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 7.86645\n",
      "Epoch 659/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7148 - val_loss: 11.7904\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 7.86645\n",
      "Epoch 660/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.0146 - val_loss: 10.5088\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 7.86645\n",
      "Epoch 661/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.6779 - val_loss: 10.3106\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 7.86645\n",
      "Epoch 662/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8464 - val_loss: 10.3082\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 7.86645\n",
      "Epoch 663/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9349 - val_loss: 10.8705\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 7.86645\n",
      "Epoch 664/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 2.0153 - val_loss: 12.2951\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 7.86645\n",
      "Epoch 665/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.7456 - val_loss: 10.7096\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 7.86645\n",
      "Epoch 666/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7822 - val_loss: 10.9505\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 7.86645\n",
      "Epoch 667/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 2.1140 - val_loss: 13.3691\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 7.86645\n",
      "Epoch 668/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.7717 - val_loss: 11.7992\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 7.86645\n",
      "Epoch 669/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9844 - val_loss: 12.7054\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 7.86645\n",
      "Epoch 670/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7263 - val_loss: 10.4874\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 7.86645\n",
      "Epoch 671/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9030 - val_loss: 10.2158\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 7.86645\n",
      "Epoch 672/2000\n",
      "34103/34103 [==============================] - 4s 117us/step - loss: 1.7627 - val_loss: 10.8193\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 7.86645\n",
      "Epoch 673/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.9130 - val_loss: 10.8335\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 7.86645\n",
      "Epoch 674/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.6803 - val_loss: 10.7084\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 7.86645\n",
      "Epoch 675/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.8924 - val_loss: 11.7999\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 7.86645\n",
      "Epoch 676/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9055 - val_loss: 11.8893\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 7.86645\n",
      "Epoch 677/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8125 - val_loss: 10.8782\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 7.86645\n",
      "Epoch 678/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.6812 - val_loss: 11.7640\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 7.86645\n",
      "Epoch 679/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.9850 - val_loss: 10.0693\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 7.86645\n",
      "Epoch 680/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 1.8714 - val_loss: 10.1897\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 7.86645\n",
      "Epoch 681/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.6991 - val_loss: 12.4293\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 7.86645\n",
      "Epoch 682/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.8307 - val_loss: 10.7567\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 7.86645\n",
      "Epoch 683/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7865 - val_loss: 11.0822\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 7.86645\n",
      "Epoch 684/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.6571 - val_loss: 11.8403\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 7.86645\n",
      "Epoch 685/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8607 - val_loss: 11.1398\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 7.86645\n",
      "Epoch 686/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.7980 - val_loss: 10.8115\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 7.86645\n",
      "Epoch 687/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 1.7867 - val_loss: 11.0290\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 7.86645\n",
      "Epoch 688/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 2.0079 - val_loss: 12.4532\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 7.86645\n",
      "Epoch 689/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.8327 - val_loss: 9.9065\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 7.86645\n",
      "Epoch 690/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8437 - val_loss: 12.4699\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 7.86645\n",
      "Epoch 691/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.7932 - val_loss: 10.5834\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 7.86645\n",
      "Epoch 692/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8835 - val_loss: 10.3501\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 7.86645\n",
      "Epoch 693/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.6670 - val_loss: 10.8810\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 7.86645\n",
      "Epoch 694/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.8930 - val_loss: 12.2629\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 7.86645\n",
      "Epoch 695/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.7931 - val_loss: 11.8519\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 7.86645\n",
      "Epoch 696/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8058 - val_loss: 10.9204\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 7.86645\n",
      "Epoch 697/2000\n",
      "34103/34103 [==============================] - 4s 111us/step - loss: 1.8482 - val_loss: 10.7866\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 7.86645\n",
      "Epoch 698/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.8975 - val_loss: 10.7879\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 7.86645\n",
      "Epoch 699/2000\n",
      "34103/34103 [==============================] - 4s 115us/step - loss: 1.6960 - val_loss: 10.6347\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 7.86645\n",
      "Epoch 700/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.5554 - val_loss: 10.1900\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 7.86645\n",
      "Epoch 701/2000\n",
      "34103/34103 [==============================] - 4s 113us/step - loss: 1.8109 - val_loss: 10.8521\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 7.86645\n",
      "Epoch 702/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.7687 - val_loss: 10.7673\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 7.86645\n",
      "Epoch 703/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.9700 - val_loss: 11.8734\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 7.86645\n",
      "Epoch 704/2000\n",
      "34103/34103 [==============================] - 4s 114us/step - loss: 1.6936 - val_loss: 9.8296\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 7.86645\n",
      "Epoch 705/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8657 - val_loss: 10.3929\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 7.86645\n",
      "Epoch 706/2000\n",
      "34103/34103 [==============================] - 4s 112us/step - loss: 1.8315 - val_loss: 20.4843\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 7.86645\n",
      "Epoch 00706: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val),\n",
    "                     callbacks=[tensorboard,model_check,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "model.save_weights('../redes_CNN_R/defs/CNN_regression_R_{}_{}_{}_{}_{}'.format(nb_epoch,batch_size,experimento,algoritmo,dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mse: 8.0072436439235\n",
      "[169.68594 177.3615  187.54044 190.32393 171.22044 176.22856 170.68408\n",
      " 174.53091 180.98355 180.26646]\n",
      "[167.54586826 172.27025214 182.43029028 186.86762463 170.28600154\n",
      " 172.83575396 169.46205434 168.6455402  178.11627523 179.77045368]\n",
      "[-2.14007534 -5.09124383 -5.11014551 -3.4563042  -0.93444218 -3.39280744\n",
      " -1.2220277  -5.88537411 -2.86727579 -0.49601056]\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(best_model_name)\n",
    "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test mse:', score)\n",
    "# print('Test mae:', score[1])\n",
    "Y_test_predicted=model.predict(X_test)\n",
    "print(Y_test_predicted[:10].flatten())\n",
    "print(Y_test[:10])\n",
    "error_prediction=Y_test-Y_test_predicted.flatten()\n",
    "\n",
    "print(error_prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(error_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n",
      "(401,)\n",
      "[[Model]]\n",
      "    Model(gaussian)\n",
      "[[Fit Statistics]]\n",
      "    # fitting method   = leastsq\n",
      "    # function evals   = 45\n",
      "    # data points      = 400\n",
      "    # variables        = 3\n",
      "    chi-square         = 23208.4855\n",
      "    reduced chi-square = 58.4596612\n",
      "    Akaike info crit   = 1630.32348\n",
      "    Bayesian info crit = 1642.29787\n",
      "[[Variables]]\n",
      "    amp:  296.381377 +/- 1.89310399 (0.64%) (init = 1000)\n",
      "    cen: -2.77464885 +/- 0.01330488 (0.48%) (init = 0)\n",
      "    wid:  2.55108957 +/- 0.01881594 (0.74%) (init = 1)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(amp, wid) = -0.577\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(error_prediction, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWHM=result.params['wid'].value*2*sqrt(log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.247842772636119\n"
     ]
    }
   ],
   "source": [
    "print(FWHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169.68594]\n",
      " [171.22044]\n",
      " [170.68408]\n",
      " ...\n",
      " [171.92656]\n",
      " [172.8454 ]\n",
      " [169.95454]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.075605766001324\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin0_predicted=model.predict(X_test_bin0)\n",
    "print(Y_test_bin0_predicted)\n",
    "error_prediction_bin0=Y_test_bin0-Y_test_bin0_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin0, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin0=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.998254247617004\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin1_predicted=model.predict(X_test_bin1)\n",
    "#print(Y_test_bin1_predicted)\n",
    "error_prediction_bin1=Y_test_bin1-Y_test_bin1_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin1, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin1=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.562616000401002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin2_predicted=model.predict(X_test_bin2)\n",
    "#print(Y_test_bin2_predicted)\n",
    "error_prediction_bin2=Y_test_bin2-Y_test_bin2_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin2, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin2=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.911884620802414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin3_predicted=model.predict(X_test_bin3)\n",
    "#print(Y_test_bin3_predicted)\n",
    "error_prediction_bin3=Y_test_bin3-Y_test_bin3_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin3, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin3=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7113122928110354\n",
      "3.911884620802414\n",
      "4.562616000401002\n",
      "4.998254247617004\n",
      "4.075605766001324\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPTklEQVR4nO3db6hkd33H8fen2fgHFUyaa9gmoWsltQbBjdxuhRRx45/GPEkCWswD2ULKWjBgwBb/PWiEFmxR86hIV5J6W9LY4B8SxFZjGglCid5N182uW5sY0zZm2b2SismTtEm+fXDPlut15s6582dnfnffLxhm5nfOufOZs7OfnT33N2dSVUiS2vUr8w4gSZqMRS5JjbPIJalxFrkkNc4il6TG7TqbD3bRRRfVnj17zuZDSlLzDh8+/NOqWhq2/KwW+Z49e1hdXT2bDylJzUvyH1st99CKJDXOIpekxlnkktQ4i1ySGmeRS1LjLHJJatzIIk/ysiTfTfL9JMeTfLIb/0KSHyc50l32zj6uJGmzPvPInwOurqpnk5wPfCfJP3bL/qSqvjS7eJKkUUYWea2fsPzZ7u753cWTmEvSguh1jDzJeUmOAKeB+6rqoW7Rnyc5muS2JC8dsu3BJKtJVtfW1qYUW33sX9n/C9eSdqZeRV5VL1TVXuBSYF+SNwIfA34L+G3gQuAjQ7Y9VFXLVbW8tDT0VAGSpDFta9ZKVf0M+DZwTVWdrHXPAX8D7JtBPknSCH1mrSwleXV3++XAO4B/S7K7GwtwPXBslkElSYP1mbWyG1hJch7rxX93VX0tyT8nWQICHAH+aIY5JUlD9Jm1chS4csD41TNJJEnaFj/Z2Shnokg6wyKXpMZZ5JLUOItckhpnkUtS4yxySWqcRb5DOatFOndY5JLUOItckhpnkUtS4yxySWqcRb4D+YtO6dxikUtS4yxySWqcRS5JjbPIJalxFrkkNc4i30G2mq2yf2W/s1mkHcoil6TGWeSS1LiRRZ7kZUm+m+T7SY4n+WQ3/tokDyV5NMk/JHnJ7ONKkjbr8478OeDqqnoTsBe4JslbgL8Abquqy4H/Bm6aXUxJ0jAji7zWPdvdPb+7FHA18KVufAW4fiYJJUlb6nWMPMl5SY4Ap4H7gB8BP6uq57tVngQuGbLtwSSrSVbX1tamkVnb4EwVaefrVeRV9UJV7QUuBfYBbxi02pBtD1XVclUtLy0tjZ9UkjTQtmatVNXPgG8DbwFenWRXt+hS4KnpRpMk9dFn1spSkld3t18OvAM4ATwAvKdb7QBwz6xCSpKG2zV6FXYDK0nOY734766qryX5AfDFJH8G/Ctw+wxzSpKGGFnkVXUUuHLA+OOsHy+XJM2Rn+xskDNRJG1kkUtS4yxySWqcRS5JjbPIJalxFrkkNc4ib8ykM1bObO/MF2nnsMglqXEWuSQ1ziKXpMZZ5JLUOItckhrX5+yHWhCDZppsHtvubJT9K/t54MADE+WSNF++I5ekxlnkktQ4i1ySGmeRS1LjLHJJapxFrv/n+VekNlnkktQ4i1ySGjeyyJNcluSBJCeSHE/yoW781iQ/SXKku1w7+7iSpM36fLLzeeDDVfVwklcBh5Pc1y27rao+Pbt4kqRRRhZ5VZ0ETna3n0lyArhk1sEkSf1s6xh5kj3AlcBD3dDNSY4muSPJBUO2OZhkNcnq2traRGHPZc4okTRM7yJP8krgy8AtVfVz4HPA64C9rL9j/8yg7arqUFUtV9Xy0tLSFCJLkjbqVeRJzme9xO+sqq8AVNWpqnqhql4EPg/sm11MSdIwfWatBLgdOFFVn90wvnvDajcAx6YfT5I0Sp9ZK1cB7wceSXKkG/s4cGOSvUABTwAfmElCSdKW+sxa+Q6QAYu+Pv04kqTt8pOdDXMmiySwyCWpeRa5JDXOIpekxlnkktQ4i1ySGmeRy9kvUuMscklqnEUuSY2zyCWpcRa5JDXOIpekxlnk57BBs1WcwSK1xyKXpMZZ5JLUOItckhpnkUtS4yxySWqcRS5JjbPIJalxI4s8yWVJHkhyIsnxJB/qxi9Mcl+SR7vrC2YfV5K0WZ935M8DH66qNwBvAT6Y5Argo8D9VXU5cH93X5J0lo0s8qo6WVUPd7efAU4AlwDXASvdaivA9bMKKUkablvHyJPsAa4EHgIurqqTsF72wGuGbHMwyWqS1bW1tcnSSpJ+Se8iT/JK4MvALVX1877bVdWhqlququWlpaVxMkqSttCryJOcz3qJ31lVX+mGTyXZ3S3fDZyeTURJ0lb6zFoJcDtwoqo+u2HRvcCB7vYB4J7px5MkjbKrxzpXAe8HHklypBv7OPAp4O4kNwH/Cbx3NhElSVsZWeRV9R0gQxa/fbpxJEnb5Sc7z0GjvjzCL5eQ2mKRS1LjLHJJapxFLkmNs8glqXEWuSQ1ziJfcGdrBokzVaR2WeSS1DiLXJIaZ5FLUuMscklqnEUuSY2zyBfUvGeRzPvxJfVnkUtS4yxySWqcRS5JjbPIJalxFrkkNc4il6TGWeSS1DiLXJIaN7LIk9yR5HSSYxvGbk3ykyRHusu1s40pSRqmzzvyLwDXDBi/rar2dpevTzeWJKmvkUVeVQ8CT5+FLJKkMUxyjPzmJEe7Qy8XDFspycEkq0lW19bWJni4c8+8z3cy78eX1M+4Rf454HXAXuAk8JlhK1bVoaparqrlpaWlMR9OkjTMWEVeVaeq6oWqehH4PLBvurEkSX2NVeRJdm+4ewNwbNi6kqTZ2jVqhSR3AW8DLkryJPCnwNuS7AUKeAL4wAwzSpK2MLLIq+rGAcO3zyCLJGkMfrJTkhpnkUtS4yxySWqcRS5JjbPIJalxFrkkNc4il6TGWeSS1DiLXJIaZ5FLUuMscklqnEUuSY2zyBfQIn0zz+Ysi5RN0jqLXJIaZ5FLUuMscklqnEUuSY2zyCWpcRa5JuZMFmm+LHJJatzIIk9yR5LTSY5tGLswyX1JHu2uL5htTEnSMH3ekX8BuGbT2EeB+6vqcuD+7r4kaQ5GFnlVPQg8vWn4OmClu70CXD/lXJKknsY9Rn5xVZ0E6K5fM71IkqTtmPkvO5McTLKaZHVtbW3WD6cZGTYzxRkr0vyNW+SnkuwG6K5PD1uxqg5V1XJVLS8tLY35cJKkYcYt8nuBA93tA8A904kjSdquPtMP7wL+BXh9kieT3AR8CnhnkkeBd3b3JUlzsGvUClV145BFb59yFknSGPxkpyQ1ziJfEPtX9i/sDBC/JUhabBa5JDXOIpekxlnkktQ4i1ySGmeRL5iWfpHYUlZpJ7PIJalxFrkkNc4il6TGWeSS1DiLXJIaZ5E3YFFmhyxKDlisLNK8WeSS1DiLXJIaZ5FLUuMscklqnEUuSY2zyBdAizMwBmVu8XlIO4FFLkmNs8glqXG7Jtk4yRPAM8ALwPNVtTyNUJKk/iYq8s7+qvrpFH6OJGkMHlqRpMZNWuQFfDPJ4SQHB62Q5GCS1SSra2trEz7cznBmdsdOneWx8fmduQxb3ufnDLsvad2kRX5VVb0ZeDfwwSRv3bxCVR2qquWqWl5aWprw4SRJm01U5FX1VHd9GvgqsG8aoSRJ/Y1d5ElekeRVZ24D7wKOTSuYJKmfSWatXAx8NcmZn/P3VfVPU0klSept7CKvqseBN00xiyRpDE4/nLGNMy0G3d5pMzGGPa+d9jylRWKRS1LjLHJJapxFLkmNs8glqXEWuSQ1ziLXwpj0W4f6rusMGu00FrkkNc4il6TGWeSS1DiLXJIaZ5FLUuMs8ikY9C0421m+E/R5fsO+LWjzOWi22l99z+Gy1TcT9TGNP6++z0GalEUuSY2zyCWpcRa5JDXOIpekxlnkktS4Zop8Xr/p3+qbfAbNuNg4Pum5Q84lo/bLsG9aOnN/874ftGzQNlv9GW31M7fKOGoG03ZzjGPUt1GNmh3UN8egfTVuzlmsO84+nMXf0Vn/vW+myCVJg1nkktS4iYo8yTVJfpjksSQfnVYoSVJ/Yxd5kvOAvwLeDVwB3JjkimkFkyT1M8k78n3AY1X1eFX9D/BF4LrpxJIk9ZWqGm/D5D3ANVX1h9399wO/U1U3b1rvIHCwu/t64Ifjxx3pIuCnM/z502be2WkpK5h3llrKCoPz/npVLQ3bYNcED5YBY7/0r0JVHQIOTfA4vSVZrarls/FY02De2WkpK5h3llrKCuPlneTQypPAZRvuXwo8NcHPkySNYZIi/x5weZLXJnkJ8D7g3unEkiT1Nfahlap6PsnNwDeA84A7qur41JKN56wcwpki885OS1nBvLPUUlYYI+/Yv+yUJC0GP9kpSY2zyCWpcTuiyJO8N8nxJC8mWd607GPdKQR+mOT35pVxmCS3JvlJkiPd5dp5Z9qstVMxJHkiySPd/lydd57NktyR5HSSYxvGLkxyX5JHu+sL5pnxjCFZF/Y1m+SyJA8kOdF1woe68YXbv1tk3fb+3RHHyJO8AXgR+Gvgj6tqtRu/AriL9U+h/hrwLeA3q+qFeWXdLMmtwLNV9el5ZxmkOxXDvwPvZH3K6feAG6vqB3MNtoUkTwDLVbWQHwJJ8lbgWeBvq+qN3dhfAk9X1ae6fywvqKqPzDNnl2tQ1ltZ0Ndskt3A7qp6OMmrgMPA9cAfsGD7d4usv8829++OeEdeVSeqatAnRq8DvlhVz1XVj4HHWC919eepGKasqh4Ent40fB2w0t1eYf0v9NwNybqwqupkVT3c3X4GOAFcwgLu3y2ybtuOKPItXAL814b7TzLmjpqxm5Mc7f4bO/f/8m3Syj7cqIBvJjncnSKiBRdX1UlY/wsOvGbOeUZZ5NcsAEn2AFcCD7Hg+3dTVtjm/m2myJN8K8mxAZet3h32Oo3ArI3I/jngdcBe4CTwmbOdb4SF2IfbdFVVvZn1M3N+sDs8oOlZ9NcsSV4JfBm4pap+Pu88WxmQddv7d5JzrZxVVfWOMTZbiNMI9M2e5PPA12YcZ7sWYh9uR1U91V2fTvJV1g8PPTjfVCOdSrK7qk52x05PzzvQMFV16sztRXzNJjmf9WK8s6q+0g0v5P4dlHWc/dvMO/Ix3Qu8L8lLk7wWuBz47pwz/YLuRXXGDcCxYevOSVOnYkjyiu4XRyR5BfAuFm+fDnIvcKC7fQC4Z45ZtrTIr9kkAW4HTlTVZzcsWrj9OyzrWPu3qpq/dE/2SeA54BTwjQ3LPgH8iPXT57573lkHZP874BHgKOsvtt3zzjQg47Wsz1z5EfCJeecZkfU3gO93l+OLmJf1mVQngf/tXrc3Ab8K3A882l1fOO+cW2Rd2Ncs8LusH/o7ChzpLtcu4v7dIuu29++OmH4oSeeynX5oRZJ2PItckhpnkUtS4yxySWqcRS5JjbPIJalxFrkkNe7/ANkVGP8RdCV+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin4_predicted=model.predict(X_test_bin4)\n",
    "#print(Y_test_bin4_predicted)\n",
    "error_prediction_bin4=Y_test_bin4-Y_test_bin4_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin4, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin4=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin4)\n",
    "print(FWHM_bin3)\n",
    "print(FWHM_bin2)\n",
    "print(FWHM_bin1)\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora los histogramnas 2d que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3] *",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "909px",
    "right": "57px",
    "top": "246px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
