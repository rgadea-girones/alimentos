{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN for PETALO\n",
    "\n",
    "Esta red la vamos a utilizar para obtener el radio.\n",
    "\n",
    "Es una soluci√≥n mixta CNN y MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rgadea/anaconda3/envs/tensorflow3/lib/python36.zip', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/lib-dynload', '', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages', '/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/IPython/extensions', '/volumedisk0/home/rgadea/.ipython', '/home/rgadea/lmfit-py/', '/home/rgadea/experimentos/viherbos/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/rgadea/lmfit-py/\")\n",
    "sys.path.append(\"/home/rgadea/experimentos/viherbos/\")\n",
    "\n",
    "print(sys.path)\n",
    "import json \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,Conv3D, MaxPooling3D, MaxPooling2D,BatchNormalization\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
    "from math import floor\n",
    "from lmfit.models import  GaussianModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos introducir los datos de petalo preprocesados en pyhton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto_datos_entradas A shape: (39012, 20, 175)\n",
      "conjunto_datos_entradas B shape: (39012, 20, 175)\n",
      "conjunto_datos_salidas shape: (39012, 3)\n"
     ]
    }
   ],
   "source": [
    "filtro=2\n",
    "if filtro==1:\n",
    "    npzfile = np.load('../conjuntos_datos_nuevos_2020/20_12_2019_comptom_filt.npz')\n",
    "    npzfile.files\n",
    "    conjunto_datos_entradasA=npzfile['arr_0']\n",
    "    conjunto_datos_entradasB=npzfile['arr_1']\n",
    "    conjunto_datos_salidas=npzfile['arr_2']\n",
    "else:\n",
    "    if filtro==2:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/29_12_2019_comptom_filt4.npz')\n",
    "        npzfile.files\n",
    "        conjunto_datos_entradasA=npzfile['arr_0']\n",
    "        conjunto_datos_entradasB=npzfile['arr_1']\n",
    "        conjunto_datos_salidas=npzfile['arr_2']\n",
    "    else:\n",
    "        npzfile = np.load('../conjuntos_datos_nuevos_2020/11_12_2019.npz')\n",
    "        npzfile.files\n",
    "        entradas_sensorsA1=npzfile['arr_0']\n",
    "        entradas_sensorsB1=npzfile['arr_1']\n",
    "        coordenadas1=npzfile['arr_2']\n",
    "        entradas_sensorsA2=npzfile['arr_3']\n",
    "        entradas_sensorsB2=npzfile['arr_4']\n",
    "        coordenadas2=npzfile['arr_5']\n",
    "        conjunto_datos_entradasA=np.concatenate((entradas_sensorsA1,entradas_sensorsA2),axis=0)\n",
    "        conjunto_datos_entradasB=np.concatenate((entradas_sensorsB1,entradas_sensorsB2),axis=0)\n",
    "        conjunto_datos_salidas=np.concatenate((coordenadas1,coordenadas2),axis=0)\n",
    "\n",
    "\n",
    "print('conjunto_datos_entradas A shape:', conjunto_datos_entradasA.shape)\n",
    "print('conjunto_datos_entradas B shape:', conjunto_datos_entradasB.shape)\n",
    "print('conjunto_datos_salidas shape:', conjunto_datos_salidas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183.54915174  40.95964471 -44.64479446]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176.36438521  69.8816522  -59.44823456]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167.72985114  14.3559163  -61.6903038 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167.43773228  37.55415919 -52.74623489]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193.80790581 111.63402003  59.24087143]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170.1396366  122.10030154 -68.41673279]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177.35349883  34.02375858  57.26340866]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182.86118891 277.85039367   4.13560057]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABICAYAAADvR65LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAKa0lEQVR4nO3da4xcZR3H8e+v2+6W1pbeKF0o0kKAyBsFq4C3IFC5hEA0vighsSqGxETjJSptmpj4DtAYYiBgIygqglgRmgZDoKK+MeWmQAuUlptdKJQKtA3Qdmn/vjjPwOx0tjuzM3Mu7e+TNDPnOWfO+c9/9jz9zzPPnFFEYGZm1TOh6ADMzGx83IGbmVWUO3Azs4pyB25mVlHuwM3MKsoduJlZRXXUgUu6QNJGSZslLetWUGZmNjaNdx64pD7gWWAxMAQ8DFwWEU91LzwzMxtNJxX4J4HNEfF8ROwF7gAu7U5YZmY2lokdPPZYYEvd8hBwRuNGkq4ErgToo+/jU5jewSHNrBSk7LaTb3J3Yx/j3Xcvj90Du3hze0Qc1djeSQeuJm0HZCMiVgIrAaZrVpyhczs4pJmVSrNeoIh9jHffvTx2Fz0Qq15q1t7JEMoQcFzd8nzglQ72Z2ZmbeikA38YOEnSQkn9wBJgdXfCMrPx0qR+NKn/sNzn4WbcQygR8Z6kbwH3AX3ALRGxoWuRmZnZQXUyBk5E3Avc26VYzA55tYozhvf27Bi92Hen+2z2vBv3OVZu+ubMBmDf9v91FEtRevHa+5uYZmYV1VEFbmbt6WXlXYRWq8pWnvdY21S18q7pxWvvCtzMrKLcgZvlYLQZF4f6TIxD/fkVzR24mVlFeQzcLAe18c/GMeN2x0V7MZOhk32O9pg8ZtuYK3Azs8pyBW6Wo04r0vE8vrEaHutdQP2Y9WiPmTBtWra8e0/TfbQ7x7uVuDvROA4/2vOqGlfgZmYV5QrcrERGqxSbbddq1ThWddx4bE0eeL+yrrX1zZ0DwPCH0xVNd2XrJ+wdzpbfeAsYfa52K7G2+tybPa7VeejjPUZZjVmBS7pF0jZJ6+vaZkm6X9KmdDuzt2GamVmjVirw3wDXA7+ta1sGrI2Iq9NvYS4Drup+eGbl1Yvx0/FW1d0Uu/egyQMj2vbNy2q03XOz9olTR3YdfYPZD7VMfGzv+/uoj7OVXI13Rks7uah6xd1ozA48Iv4paUFD86XA2en+rcDfcQduh5k8ptAV8SFb3/xBSEMj735kEIC3j8niWHf1jQDcvGMeANff8CUAjnr0bQAmTM8+3NyXOvBG9cMd3fwa/li6MXRSxg88x/sh5tERsRUg3c7tXkhmZtaKnn+IWf+bmJOZ0uvDmXVdu5VXEUMq3bR/+xvvV9L9b+4GYOfxWQ5OXPs1AKavOwKA3bMbHjszG0rpqy3v3AU0n26Y53PrxrHKVHnXjLcCf03SIEC63TbahhGxMiIWRcSiSQyMtpmZmbVpvBX4amApcHW6vadrEZmVTBkrr3amEbYrdu+BObOy4+zZB8Csp94BYHj6VAAmDGe/Xz57w34A+tK0whjoo5ky5nC8yjQW3so0wtuBfwGnSBqSdAVZx71Y0iZgcVo2M7MctTIL5bJRVp3b5VjMKqEMFVgvfzatfsbGhDd3ArBnXjYbZdqWrCJ/d3ZW+9UqcaVZK7Hx+ez2yGwMff+uXSOO0ct3DnnpxmUCusVfpTczqyh/ld6sTWW/hGo34tg3tHXE8uQ92Rj3EQPZRIQj+ydlK1LlXZttUvsC0P4dIyvvmvqYypKvThUZvytwM7OKcgVu1iVVryTrNT6X2kWqalXzhDTGXdM41l1zsCq77O9kqsAVuJlZRbkCNzvEtHMZ1mbbN3t8/aVm4cDLxra6r0NhFkqZuAI3M6soV+Bmh6h2L8PabPtWf4S5m9eJKdM867JzBW5mVlGuwM1aVLVKsN0425kpkqcyxFBWrsDNzCrKHbhZi2J4b8fVoCb1H/DrMHZ46sbfgTtwM7OKUkTkdzDpdeBtYHtuB23dHBxXu8oam+NqT1njgvLGlndcx0fEUY2NuXbgAJIeiYhFuR60BY6rfWWNzXG1p6xxQXljK0tcHkIxM6sod+BmZhVVRAe+soBjtsJxta+ssTmu9pQ1LihvbKWIK/cxcDMz6w4PoZiZVZQ7cDOzisqtA5d0gaSNkjZLWpbXcZvEcZykByU9LWmDpO+k9lmS7pe0Kd3OLCi+Pkn/lrQmLS+UtC7F9UdJhXyNT9IMSaskPZNyd1YZcibpe+l1XC/pdkmTi8qZpFskbZO0vq6taY6U+UU6H56QdHrOcf00vZZPSPqLpBl165anuDZKOj/PuOrW/UBSSJqTlnPL18Fik/TtlJcNkq6ta88lZweIiJ7/A/qA54ATgH7gceDUPI7dJJZB4PR0fxrwLHAqcC2wLLUvA64pKL7vA38A1qTlO4El6f5NwDcLiutW4Bvpfj8wo+icAccCLwBH1OXqq0XlDPgccDqwvq6taY6Ai4C/AgLOBNblHNcXgInp/jV1cZ2azs8BYGE6b/vyiiu1HwfcB7wEzMk7XwfJ2eeBB4CBtDw375wdEGcuB4GzgPvqlpcDy/M4dgux3QMsBjYCg6ltENhYQCzzgbXAOcCa9Me6ve5EG5HHHOOanjpKNbQXmrPUgW8BZpFdWXMNcH6ROQMWNJz0TXME/BK4rNl2ecTVsO6LwG3p/ohzM3WkZ+UZF7AK+CjwYl0Hnmu+Rnkt7wTOa7Jdrjmr/5fXEErtRKsZSm2FkrQAOA1YBxwdEVsB0u3cAkK6DvgRsD8tzwbeioj30nJReTsBeB34dRre+ZWkqRScs4h4GfgZ8F9gK7ADeJRy5KxmtByV6Zz4Oll1CwXHJekS4OWIeLxhVRnydTLw2TQ89w9Jnyg6trw6cDVpK3T+oqQPAX8GvhsRO4uMJcVzMbAtIh6tb26yaRF5m0j2dvLGiDiN7Ho2hX2OUZPGky8le9t6DDAVuLDJpmWcK1uK11bSCuA94LZaU5PNcolL0hRgBfDjZqubtOWdr4nATLIhnB8Cd0oSBcaWVwc+RDauVTMfeCWnYx9A0iSyzvu2iLgrNb8maTCtHwS25RzWp4FLJL0I3EE2jHIdMENS7Yc3isrbEDAUEevS8iqyDr3onJ0HvBARr0fEMHAX8CnKkbOa0XJU+DkhaSlwMXB5pPf+Bcd1Itl/xo+n82A+8JikeQXHVTME3BWZh8jeKc8pMra8OvCHgZPS7IB+YAmwOqdjj5D+x7wZeDoifl63ajWwNN1fSjY2npuIWB4R8yNiAVl+/hYRlwMPAl8uKq4U26vAFkmnpKZzgacoOGdkQydnSpqSXtdaXIXnrM5oOVoNfCXNrjgT2FEbasmDpAuAq4BLIuKdhniXSBqQtBA4CXgoj5gi4smImBsRC9J5MEQ24eBVCs5XcjdZYYWkk8k+zN9OgTnr+SB73cD+RWQzPp4DVuR13CZxfIbs7c0TwH/Sv4vIxpvXApvS7awCYzybD2ahnJD+GDYDfyJ9Al5ATB8DHkl5u5vsrWThOQN+AjwDrAd+RzYToJCcAbeTjcUPk3U+V4yWI7K33Tek8+FJYFHOcW0mG7etnQM31W2/IsW1Ebgwz7ga1r/IBx9i5pavg+SsH/h9+lt7DDgn75w1/vNX6c3MKsrfxDQzqyh34GZmFeUO3MysotyBm5lVlDtwM7OKcgduZlZR7sDNzCrq//kI9hNUPAhIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190.85612767 257.27218138   2.89327669]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193.53654707 341.76042583  37.26935196]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    idea=np.random.randint(1,conjunto_datos_entradasB.shape[0])\n",
    "    plt.imshow(conjunto_datos_entradasB[idea], cmap='viridis')\n",
    "    plt.show()\n",
    "    print(conjunto_datos_salidas[idea,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "nb_classes = 10\n",
    "nb_epoch = 2000\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 20, 41\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (1,2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (2, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "muestras=conjunto_datos_entradasA.shape[0]\n",
    "# veamos=idea.reshape(idea.shape[0],175, 20)\n",
    "\n",
    "\n",
    "veamos2A=np.zeros([muestras,20,175])\n",
    "veamos2_3A=np.zeros([muestras,20,525])\n",
    "veamos2B=np.zeros([muestras,20,175])\n",
    "veamos2_3B=np.zeros([muestras,20,525])\n",
    "sector2A=np.zeros([muestras,20,img_cols])\n",
    "sector2B=np.zeros([muestras,20,img_cols])\n",
    "veamos3=np.zeros([muestras,175])\n",
    "# for i in range(idea.shape[0]):\n",
    "for i in range(muestras):\n",
    "    veamos2A[i]=conjunto_datos_entradasA[i]\n",
    "    veamos2B[i]=conjunto_datos_entradasB[i]\n",
    "    veamos3[i]=np.sum(veamos2A[i], axis=0)\n",
    "    indice=np.argmax(veamos3[i], axis=0)\n",
    "    indice_inferior=int(indice-((img_cols-1)/2)+175)\n",
    "    indice_superior=int(indice+((img_cols+1)/2)+175)\n",
    "    veamos2_3A[i]=np.concatenate((veamos2A[i],veamos2A[i],veamos2A[i]),axis=1) \n",
    "    veamos2_3B[i]=np.concatenate((veamos2B[i],veamos2B[i],veamos2B[i]),axis=1) \n",
    "    sector2A[i]=veamos2_3A[i,:,indice_inferior:indice_superior]\n",
    "    sector2B[i]=veamos2_3B[i,:,indice_inferior:indice_superior]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation mediante flip horizontal y vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atencion: la z es copiada pero en realidad es incorrecta. Podemos asegurar que el radio y phi si que son las mismas; pero la z claramente al hacer un flip vertical no puede ser la misma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "veamosA=np.zeros([3*muestras,20,img_cols])\n",
    "veamosB=np.zeros([3*muestras,20,img_cols])\n",
    "conjunto_datos_salidas_nuevo=np.zeros([3*muestras,3])\n",
    "for i in range(muestras):\n",
    "    veamosA[i*3]=sector2A[i]   \n",
    "    veamosA[i*3+1]=np.flipud(sector2A[i])   \n",
    "    veamosA[i*3+2]=np.fliplr(sector2A[i])     \n",
    "    veamosB[i*3]=sector2B[i]   \n",
    "    veamosB[i*3+1]=np.flipud(sector2B[i])   \n",
    "    veamosB[i*3+2]=np.fliplr(sector2B[i])   \n",
    "    conjunto_datos_salidas_nuevo[i*3]=conjunto_datos_salidas[i]\n",
    "    conjunto_datos_salidas_nuevo[i*3+1]=conjunto_datos_salidas[i]    \n",
    "    conjunto_datos_salidas_nuevo[i*3+2]=conjunto_datos_salidas[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector A shape: (117036, 20, 41)\n",
      "conjunto_datos_nuevo A: (117036, 820)\n",
      "sector B shape: (117036, 20, 41)\n",
      "conjunto_datos_nuevo B: (117036, 820)\n",
      "conjunto_datos_salidas_nuevo: (117036, 3)\n"
     ]
    }
   ],
   "source": [
    "print('sector A shape:', veamosA.shape)\n",
    "conjunto_datos_nuevoA=veamosA.reshape(veamosA.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo A:', conjunto_datos_nuevoA.shape)\n",
    "\n",
    "print('sector B shape:', veamosB.shape)\n",
    "conjunto_datos_nuevoB=veamosB.reshape(veamosB.shape[0], img_rows*img_cols)\n",
    "print('conjunto_datos_nuevo B:', conjunto_datos_nuevoB.shape)\n",
    "print('conjunto_datos_salidas_nuevo:', conjunto_datos_salidas_nuevo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5  # how many digits we will display\n",
    "\n",
    "fig = plt.figure(figsize=(8,20))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ideas=np.random.randint(1,muestras)\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+1)\n",
    "    plt.imshow(sector2A[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = fig.add_subplot(n, 2, (i)*2+2)\n",
    "    plt.imshow(sector2B[ideas], cmap='viridis')\n",
    "    plt.viridis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(x_test[idea])\n",
    "# print(decoded_imgs[idea])\n",
    "# print(decoded_imgs_scaled[idea])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "#Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "for i in range(1,5):\n",
    "    idea=np.random.randint(1,sector2B.shape[0])\n",
    "    plt.imshow(sector2B[idea], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70221\n",
      "23407\n",
      "23408\n",
      "(8211, 1640)\n",
      "(4509, 1640)\n",
      "(3807, 1640)\n",
      "(3236, 1640)\n",
      "(3645, 1640)\n"
     ]
    }
   ],
   "source": [
    "numero_muestras=3*muestras\n",
    "tr_size=60\n",
    "val_size=20\n",
    "test_size=100-val_size-tr_size\n",
    "conjunto_datos_nuevo2=np.concatenate((conjunto_datos_salidas_nuevo,conjunto_datos_nuevoB, conjunto_datos_nuevoA), axis=1)\n",
    "\n",
    "tamanyo_tr=floor(tr_size*numero_muestras/100)\n",
    "tamanyo_val=floor(val_size*numero_muestras/100)\n",
    "tamanyo_test=numero_muestras-tamanyo_tr-tamanyo_val\n",
    "print(tamanyo_tr)\n",
    "print(tamanyo_val)\n",
    "print(tamanyo_test)\n",
    "\n",
    "\n",
    "XY_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,:]\n",
    "\n",
    "\n",
    "XY_test_bin0=XY_test[np.where((XY_test[:,0]>=164.9999) * (XY_test[:,0]<171.000))]\n",
    "XY_test_bin1=XY_test[np.where((XY_test[:,0]>=171.000) * (XY_test[:,0]<177.000))]\n",
    "XY_test_bin2=XY_test[np.where((XY_test[:,0]>=177.000) * (XY_test[:,0]<183.0000))]\n",
    "XY_test_bin3=XY_test[np.where((XY_test[:,0]>=183.000) * (XY_test[:,0]<189.0000))]\n",
    "XY_test_bin4=XY_test[np.where((XY_test[:,0]>=189.0000))]\n",
    "\n",
    "X_train=conjunto_datos_nuevo2[:tamanyo_tr,3:]\n",
    "X_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,3:]\n",
    "X_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,3:]\n",
    "\n",
    "X_test_bin0=XY_test_bin0[:,3:]\n",
    "Y_test_bin0=XY_test_bin0[:,0]\n",
    "print(X_test_bin0.shape)\n",
    "X_test_bin1=XY_test_bin1[:,3:]\n",
    "Y_test_bin1=XY_test_bin1[:,0]\n",
    "print(X_test_bin1.shape)\n",
    "X_test_bin2=XY_test_bin2[:,3:]\n",
    "Y_test_bin2=XY_test_bin2[:,0]\n",
    "print(X_test_bin2.shape)\n",
    "X_test_bin3=XY_test_bin3[:,3:]\n",
    "Y_test_bin3=XY_test_bin3[:,0]\n",
    "print(X_test_bin3.shape)\n",
    "X_test_bin4=XY_test_bin4[:,3:]\n",
    "Y_test_bin4=XY_test_bin4[:,0]\n",
    "print(X_test_bin4.shape)\n",
    "\n",
    "\n",
    "\n",
    "Y_train=conjunto_datos_nuevo2[:tamanyo_tr,0] #elijo la coordenada radius\n",
    "Y_val=conjunto_datos_nuevo2[tamanyo_tr:tamanyo_tr+tamanyo_val,0] #elijo la corrdenada radius\n",
    "Y_test=conjunto_datos_nuevo2[tamanyo_tr+tamanyo_val:numero_muestras,0] #elijo la corrdenada radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_total=conjunto_datos_nuevo2[:numero_muestras,0]\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(Y_total.reshape(-1, 1))\n",
    "\n",
    "X_total=conjunto_datos_nuevo2[:numero_muestras,3:]\n",
    "min_max_scalerx = preprocessing.MinMaxScaler().fit(X_total)\n",
    "\n",
    "X_train = min_max_scalerx.transform(X_train)\n",
    "X_val = min_max_scalerx.transform(X_val)\n",
    "X_test = min_max_scalerx.transform(X_test)\n",
    "\n",
    "Y_train_scaled = min_max_scaler.transform(Y_train.reshape(-1, 1))\n",
    "Y_val_scaled = min_max_scaler.transform(Y_val.reshape(-1, 1))\n",
    "Y_test_scaled = min_max_scaler.transform(Y_test.reshape(-1, 1))\n",
    "\n",
    "Y_test_bin4_scaled=min_max_scaler.transform(Y_test_bin4.reshape(-1, 1))\n",
    "Y_test_bin3_scaled=min_max_scaler.transform(Y_test_bin3.reshape(-1, 1))\n",
    "Y_test_bin2_scaled=min_max_scaler.transform(Y_test_bin2.reshape(-1, 1))\n",
    "Y_test_bin1_scaled=min_max_scaler.transform(Y_test_bin1.reshape(-1, 1))\n",
    "Y_test_bin0_scaled=min_max_scaler.transform(Y_test_bin0.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],2, img_rows, img_cols,1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 2,img_rows, img_cols,1)\n",
    "\n",
    "X_test_bin0 = X_test_bin0.reshape(X_test_bin0.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin1 = X_test_bin1.reshape(X_test_bin1.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin2 = X_test_bin2.reshape(X_test_bin2.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin3 = X_test_bin3.reshape(X_test_bin3.shape[0], 2, img_rows, img_cols,1)\n",
    "X_test_bin4 = X_test_bin4.reshape(X_test_bin4.shape[0], 2, img_rows, img_cols,1)\n",
    "\n",
    "input_shape = (2, img_rows, img_cols,1)\n",
    "#input_shape=input_shape.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (70221, 2, 20, 41, 1)\n",
      "70221 train samples\n",
      "23407 validation samples\n",
      "23408 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_val= X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'validation samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                            vertical_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=kernel_size,\n",
    "                        padding='same',\n",
    "                        data_format='channels_last',\n",
    "                        input_shape=(2,img_rows,img_cols,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "\n",
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "experimento=\"CNNMLP_kernel_{}x{}x{}_sector_{}x{}x{}_elu\".format(kernel_size[0],kernel_size[1],kernel_size[2],img_rows,img_cols,1)\n",
    "algoritmo='Nadam'\n",
    "optimizador='Nadam'\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"../logs/defs/{}{}{}\".format(experimento,algoritmo,dt))\n",
    "best_model_name='../redes_CNN_R/models_best/CNN_regression_R_{}_{}_{}_{}_{}.h5'.format(nb_epoch,batch_size,experimento,algoritmo,dt)\n",
    "model_check=ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "early_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=600, verbose=1, mode='auto', baseline=None)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizador)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a summary of the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 2, 20, 41, 16)     528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 20, 41, 16)     64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 20, 41, 16)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 2, 10, 20, 16)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                320050    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 321,683\n",
      "Trainable params: 321,651\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now train the model and evaluate on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 70221 samples, validate on 23407 samples\n",
      "Epoch 1/2000\n",
      "70221/70221 [==============================] - 5s 71us/step - loss: 0.0643 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01897, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_1000_CNNMLP_kernel_2x4x4_sector_20x41x1_elu_Nadam_2019-12-24 17:45:00.h5\n",
      "Epoch 2/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0167 - val_loss: 0.0219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01897\n",
      "Epoch 3/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0143 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01897 to 0.01755, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_1000_CNNMLP_kernel_2x4x4_sector_20x41x1_elu_Nadam_2019-12-24 17:45:00.h5\n",
      "Epoch 4/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0134 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01755\n",
      "Epoch 5/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0123 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01755 to 0.01634, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_1000_CNNMLP_kernel_2x4x4_sector_20x41x1_elu_Nadam_2019-12-24 17:45:00.h5\n",
      "Epoch 6/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0116 - val_loss: 0.0152\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01634 to 0.01516, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_1000_CNNMLP_kernel_2x4x4_sector_20x41x1_elu_Nadam_2019-12-24 17:45:00.h5\n",
      "Epoch 7/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0110 - val_loss: 0.0310\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01516\n",
      "Epoch 8/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0106 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01516\n",
      "Epoch 9/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0098 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01516\n",
      "Epoch 10/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0099 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01516 to 0.01234, saving model to ../redes_CNN_R/models_best/CNN_regression_R_2000_1000_CNNMLP_kernel_2x4x4_sector_20x41x1_elu_Nadam_2019-12-24 17:45:00.h5\n",
      "Epoch 11/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0095 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01234\n",
      "Epoch 12/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0090 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01234\n",
      "Epoch 13/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0090 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01234\n",
      "Epoch 14/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0086 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01234\n",
      "Epoch 15/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0083 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01234\n",
      "Epoch 16/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0082 - val_loss: 0.0146\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01234\n",
      "Epoch 17/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0079 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01234\n",
      "Epoch 18/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0078 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01234\n",
      "Epoch 19/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0076 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01234\n",
      "Epoch 20/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0071 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01234\n",
      "Epoch 21/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0072 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01234\n",
      "Epoch 22/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0071 - val_loss: 0.0144\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01234\n",
      "Epoch 23/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0067 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01234\n",
      "Epoch 24/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0066 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01234\n",
      "Epoch 25/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0065 - val_loss: 0.0137\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01234\n",
      "Epoch 26/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0062 - val_loss: 0.0128\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01234\n",
      "Epoch 27/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0064 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01234\n",
      "Epoch 28/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0061 - val_loss: 0.0401\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01234\n",
      "Epoch 29/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0059 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01234\n",
      "Epoch 30/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0056 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01234\n",
      "Epoch 31/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0057 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01234\n",
      "Epoch 32/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0055 - val_loss: 0.0329\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01234\n",
      "Epoch 33/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0054 - val_loss: 0.0428\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01234\n",
      "Epoch 34/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0055 - val_loss: 0.0243\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01234\n",
      "Epoch 35/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0053 - val_loss: 0.0412\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01234\n",
      "Epoch 36/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0052 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01234\n",
      "Epoch 37/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0049 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01234\n",
      "Epoch 38/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0048 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01234\n",
      "Epoch 39/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0047 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01234\n",
      "Epoch 40/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0046 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01234\n",
      "Epoch 41/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0047 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01234\n",
      "Epoch 42/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0047 - val_loss: 0.0151\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01234\n",
      "Epoch 43/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0042 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01234\n",
      "Epoch 44/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0043 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01234\n",
      "Epoch 45/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0042 - val_loss: 0.0152\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01234\n",
      "Epoch 46/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0046 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01234\n",
      "Epoch 47/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0041 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01234\n",
      "Epoch 48/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0043 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01234\n",
      "Epoch 49/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0038 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01234\n",
      "Epoch 50/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0039 - val_loss: 0.0136\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01234\n",
      "Epoch 51/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0037 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01234\n",
      "Epoch 52/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0037 - val_loss: 0.0152\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01234\n",
      "Epoch 53/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0040 - val_loss: 0.0434\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01234\n",
      "Epoch 54/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0041 - val_loss: 0.0511\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01234\n",
      "Epoch 55/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0039 - val_loss: 0.0657\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01234\n",
      "Epoch 56/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0035 - val_loss: 0.0413\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01234\n",
      "Epoch 57/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0035 - val_loss: 0.0478\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01234\n",
      "Epoch 58/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0036 - val_loss: 0.0454\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01234\n",
      "Epoch 59/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0033 - val_loss: 0.0298\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01234\n",
      "Epoch 60/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0032 - val_loss: 0.0422\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01234\n",
      "Epoch 61/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0035 - val_loss: 0.0331\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01234\n",
      "Epoch 62/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0033 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01234\n",
      "Epoch 63/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0036 - val_loss: 0.0305\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01234\n",
      "Epoch 64/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0035 - val_loss: 0.0562\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01234\n",
      "Epoch 65/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0034 - val_loss: 0.0335\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01234\n",
      "Epoch 66/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0030 - val_loss: 0.0248\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01234\n",
      "Epoch 67/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0031 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01234\n",
      "Epoch 68/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0030 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01234\n",
      "Epoch 69/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0028 - val_loss: 0.0161\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01234\n",
      "Epoch 70/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0028 - val_loss: 0.0144\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01234\n",
      "Epoch 71/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0030 - val_loss: 0.0242\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01234\n",
      "Epoch 72/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0031 - val_loss: 0.0355\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01234\n",
      "Epoch 73/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0029 - val_loss: 0.0627\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01234\n",
      "Epoch 74/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0028 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01234\n",
      "Epoch 75/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0043 - val_loss: 0.0691\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01234\n",
      "Epoch 76/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0036 - val_loss: 0.0444\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01234\n",
      "Epoch 77/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0033 - val_loss: 0.0292\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01234\n",
      "Epoch 78/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0029 - val_loss: 0.0271\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01234\n",
      "Epoch 79/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0028 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01234\n",
      "Epoch 80/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0026 - val_loss: 0.0142\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01234\n",
      "Epoch 81/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0027 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01234\n",
      "Epoch 82/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0026 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01234\n",
      "Epoch 83/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0024 - val_loss: 0.0345\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01234\n",
      "Epoch 84/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0026 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01234\n",
      "Epoch 85/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0024 - val_loss: 0.0204\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01234\n",
      "Epoch 86/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0217\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01234\n",
      "Epoch 87/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0024 - val_loss: 0.0396\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01234\n",
      "Epoch 88/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01234\n",
      "Epoch 89/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0024 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01234\n",
      "Epoch 90/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0026 - val_loss: 0.0673\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01234\n",
      "Epoch 91/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0023 - val_loss: 0.0578\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01234\n",
      "Epoch 92/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0024 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01234\n",
      "Epoch 93/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0023 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01234\n",
      "Epoch 94/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0022 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01234\n",
      "Epoch 95/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0023 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01234\n",
      "Epoch 96/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01234\n",
      "Epoch 97/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0024 - val_loss: 0.0594\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01234\n",
      "Epoch 98/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0238\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01234\n",
      "Epoch 99/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0023 - val_loss: 0.0227\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01234\n",
      "Epoch 100/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01234\n",
      "Epoch 101/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0023 - val_loss: 0.0336\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.01234\n",
      "Epoch 102/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0242\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.01234\n",
      "Epoch 103/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0022 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.01234\n",
      "Epoch 104/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0020 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.01234\n",
      "Epoch 105/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0022 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.01234\n",
      "Epoch 106/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0020 - val_loss: 0.0464\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.01234\n",
      "Epoch 107/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0021 - val_loss: 0.0238\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.01234\n",
      "Epoch 108/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0210\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.01234\n",
      "Epoch 109/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0021 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.01234\n",
      "Epoch 110/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0307\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.01234\n",
      "Epoch 111/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0022 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.01234\n",
      "Epoch 112/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0022 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.01234\n",
      "Epoch 113/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0021 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.01234\n",
      "Epoch 114/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.01234\n",
      "Epoch 115/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.01234\n",
      "Epoch 116/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0021 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.01234\n",
      "Epoch 117/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0021 - val_loss: 0.0600\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.01234\n",
      "Epoch 118/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0020 - val_loss: 0.0488\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.01234\n",
      "Epoch 119/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0280\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.01234\n",
      "Epoch 120/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0019 - val_loss: 0.0255\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.01234\n",
      "Epoch 121/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0025 - val_loss: 0.0404\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.01234\n",
      "Epoch 122/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0023 - val_loss: 0.0548\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01234\n",
      "Epoch 123/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0287\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.01234\n",
      "Epoch 124/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.01234\n",
      "Epoch 125/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0022 - val_loss: 0.0250\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01234\n",
      "Epoch 126/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0022 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01234\n",
      "Epoch 127/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0021 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01234\n",
      "Epoch 128/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0023 - val_loss: 0.0282\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01234\n",
      "Epoch 129/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0022 - val_loss: 0.0282\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01234\n",
      "Epoch 130/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0020 - val_loss: 0.0148\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01234\n",
      "Epoch 131/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0019 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01234\n",
      "Epoch 132/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0322\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01234\n",
      "Epoch 133/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0018 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01234\n",
      "Epoch 134/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0018 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01234\n",
      "Epoch 135/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0018 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01234\n",
      "Epoch 136/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0151\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01234\n",
      "Epoch 137/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0018 - val_loss: 0.0309\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01234\n",
      "Epoch 138/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0325\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01234\n",
      "Epoch 139/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0432\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01234\n",
      "Epoch 140/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0017 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01234\n",
      "Epoch 141/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0020 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01234\n",
      "Epoch 142/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0342\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01234\n",
      "Epoch 143/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0348\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.01234\n",
      "Epoch 144/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0017 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01234\n",
      "Epoch 145/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0017 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01234\n",
      "Epoch 146/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01234\n",
      "Epoch 147/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0018 - val_loss: 0.0429\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01234\n",
      "Epoch 148/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0557\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01234\n",
      "Epoch 149/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0260\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01234\n",
      "Epoch 150/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0018 - val_loss: 0.0405\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01234\n",
      "Epoch 151/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0018 - val_loss: 0.0250\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01234\n",
      "Epoch 152/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0017 - val_loss: 0.0148\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01234\n",
      "Epoch 153/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0298\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.01234\n",
      "Epoch 154/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01234\n",
      "Epoch 155/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0211\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01234\n",
      "Epoch 156/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0018 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01234\n",
      "Epoch 157/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0017 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01234\n",
      "Epoch 158/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0209\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01234\n",
      "Epoch 159/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0016 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01234\n",
      "Epoch 160/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.01234\n",
      "Epoch 161/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0291\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01234\n",
      "Epoch 162/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0018 - val_loss: 0.0343\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01234\n",
      "Epoch 163/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0222\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01234\n",
      "Epoch 164/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01234\n",
      "Epoch 165/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.01234\n",
      "Epoch 166/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01234\n",
      "Epoch 167/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0300\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01234\n",
      "Epoch 168/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0279\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.01234\n",
      "Epoch 169/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0238\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01234\n",
      "Epoch 170/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0016 - val_loss: 0.0321\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.01234\n",
      "Epoch 171/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0344\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.01234\n",
      "Epoch 172/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0148\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01234\n",
      "Epoch 173/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0257\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.01234\n",
      "Epoch 174/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0016 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.01234\n",
      "Epoch 175/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0015 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.01234\n",
      "Epoch 176/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.01234\n",
      "Epoch 177/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0386\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01234\n",
      "Epoch 178/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0286\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01234\n",
      "Epoch 179/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0015 - val_loss: 0.0397\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.01234\n",
      "Epoch 180/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0015 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01234\n",
      "Epoch 181/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0016 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01234\n",
      "Epoch 182/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0282\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01234\n",
      "Epoch 183/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0015 - val_loss: 0.0313\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01234\n",
      "Epoch 184/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01234\n",
      "Epoch 185/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0015 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01234\n",
      "Epoch 186/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0469\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01234\n",
      "Epoch 187/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01234\n",
      "Epoch 188/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01234\n",
      "Epoch 189/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0015 - val_loss: 0.0334\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01234\n",
      "Epoch 190/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0016 - val_loss: 0.0209\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01234\n",
      "Epoch 191/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01234\n",
      "Epoch 192/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0258\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01234\n",
      "Epoch 193/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0292\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01234\n",
      "Epoch 194/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01234\n",
      "Epoch 195/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0148\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01234\n",
      "Epoch 196/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0014 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01234\n",
      "Epoch 197/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.01234\n",
      "Epoch 198/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01234\n",
      "Epoch 199/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0447\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01234\n",
      "Epoch 200/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0015 - val_loss: 0.0400\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01234\n",
      "Epoch 201/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0015 - val_loss: 0.0347\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.01234\n",
      "Epoch 202/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.01234\n",
      "Epoch 203/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0015 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.01234\n",
      "Epoch 204/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.01234\n",
      "Epoch 205/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.01234\n",
      "Epoch 206/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0298\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.01234\n",
      "Epoch 207/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0339\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.01234\n",
      "Epoch 208/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0244\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.01234\n",
      "Epoch 209/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0404\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.01234\n",
      "Epoch 210/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.01234\n",
      "Epoch 211/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.01234\n",
      "Epoch 212/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.01234\n",
      "Epoch 213/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0306\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.01234\n",
      "Epoch 214/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 0.0014 - val_loss: 0.0402\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.01234\n",
      "Epoch 215/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0015 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.01234\n",
      "Epoch 216/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.01234\n",
      "Epoch 217/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0017 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.01234\n",
      "Epoch 218/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0017 - val_loss: 0.0225\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.01234\n",
      "Epoch 219/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0015 - val_loss: 0.0266\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.01234\n",
      "Epoch 220/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0014 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.01234\n",
      "Epoch 221/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0414\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.01234\n",
      "Epoch 222/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0013 - val_loss: 0.0403\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.01234\n",
      "Epoch 223/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0320\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.01234\n",
      "Epoch 224/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0406\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.01234\n",
      "Epoch 225/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0304\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.01234\n",
      "Epoch 226/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.01234\n",
      "Epoch 227/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0013 - val_loss: 0.0236\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.01234\n",
      "Epoch 228/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0441\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.01234\n",
      "Epoch 229/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0704\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.01234\n",
      "Epoch 230/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0502\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.01234\n",
      "Epoch 231/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0233\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.01234\n",
      "Epoch 232/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.01234\n",
      "Epoch 233/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0013 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.01234\n",
      "Epoch 234/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0013 - val_loss: 0.0236\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.01234\n",
      "Epoch 235/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.01234\n",
      "Epoch 236/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.01234\n",
      "Epoch 237/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.01234\n",
      "Epoch 238/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.01234\n",
      "Epoch 239/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.01234\n",
      "Epoch 240/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0322\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.01234\n",
      "Epoch 241/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0305\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.01234\n",
      "Epoch 242/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0452\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.01234\n",
      "Epoch 243/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.01234\n",
      "Epoch 244/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.01234\n",
      "Epoch 245/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.01234\n",
      "Epoch 246/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0012 - val_loss: 0.0300\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.01234\n",
      "Epoch 247/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0013 - val_loss: 0.0540\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.01234\n",
      "Epoch 248/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0387\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.01234\n",
      "Epoch 249/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.01234\n",
      "Epoch 250/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.01234\n",
      "Epoch 251/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0291\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.01234\n",
      "Epoch 252/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0311\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.01234\n",
      "Epoch 253/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0363\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.01234\n",
      "Epoch 254/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0012 - val_loss: 0.0247\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.01234\n",
      "Epoch 255/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.01234\n",
      "Epoch 256/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.01234\n",
      "Epoch 257/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0214\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.01234\n",
      "Epoch 258/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0250\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.01234\n",
      "Epoch 259/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0013 - val_loss: 0.0210\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.01234\n",
      "Epoch 260/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0013 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.01234\n",
      "Epoch 261/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.01234\n",
      "Epoch 262/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0265\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.01234\n",
      "Epoch 263/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0355\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.01234\n",
      "Epoch 264/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.01234\n",
      "Epoch 265/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.01234\n",
      "Epoch 266/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.01234\n",
      "Epoch 267/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.01234\n",
      "Epoch 268/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.01234\n",
      "Epoch 269/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.01234\n",
      "Epoch 270/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0012 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.01234\n",
      "Epoch 271/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0014 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.01234\n",
      "Epoch 272/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0013 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.01234\n",
      "Epoch 273/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.01234\n",
      "Epoch 274/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0011 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.01234\n",
      "Epoch 275/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.01234\n",
      "Epoch 276/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.01234\n",
      "Epoch 277/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.01234\n",
      "Epoch 278/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.01234\n",
      "Epoch 279/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0223\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.01234\n",
      "Epoch 280/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0011 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.01234\n",
      "Epoch 281/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0011 - val_loss: 0.0300\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.01234\n",
      "Epoch 282/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0227\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.01234\n",
      "Epoch 283/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0012 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.01234\n",
      "Epoch 284/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0012 - val_loss: 0.0402\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.01234\n",
      "Epoch 285/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0381\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.01234\n",
      "Epoch 286/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0408\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.01234\n",
      "Epoch 287/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0499\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.01234\n",
      "Epoch 288/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.01234\n",
      "Epoch 289/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.01234\n",
      "Epoch 290/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0261\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.01234\n",
      "Epoch 291/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.01234\n",
      "Epoch 292/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0346\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.01234\n",
      "Epoch 293/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0446\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.01234\n",
      "Epoch 294/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.01234\n",
      "Epoch 295/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0477\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.01234\n",
      "Epoch 296/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.01234\n",
      "Epoch 297/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.01234\n",
      "Epoch 298/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0011 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.01234\n",
      "Epoch 299/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.01234\n",
      "Epoch 300/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0161\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.01234\n",
      "Epoch 301/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.01234\n",
      "Epoch 302/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.01234\n",
      "Epoch 303/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0215\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.01234\n",
      "Epoch 304/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0298\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.01234\n",
      "Epoch 305/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0250\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.01234\n",
      "Epoch 306/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 0.0011 - val_loss: 0.0300\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.01234\n",
      "Epoch 307/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0012 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.01234\n",
      "Epoch 308/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0011 - val_loss: 0.0157\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.01234\n",
      "Epoch 309/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.01234\n",
      "Epoch 310/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0265\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.01234\n",
      "Epoch 311/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0212\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.01234\n",
      "Epoch 312/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0012 - val_loss: 0.0161\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.01234\n",
      "Epoch 313/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0012 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.01234\n",
      "Epoch 314/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0155\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.01234\n",
      "Epoch 315/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0010 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.01234\n",
      "Epoch 316/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.01234\n",
      "Epoch 317/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.01234\n",
      "Epoch 318/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.01234\n",
      "Epoch 319/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.01234\n",
      "Epoch 320/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.01234\n",
      "Epoch 321/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.01234\n",
      "Epoch 322/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.01234\n",
      "Epoch 323/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0163\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.01234\n",
      "Epoch 324/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.01234\n",
      "Epoch 325/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.01234\n",
      "Epoch 326/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.01234\n",
      "Epoch 327/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0392\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.01234\n",
      "Epoch 328/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0267\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.01234\n",
      "Epoch 329/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.01234\n",
      "Epoch 330/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.01234\n",
      "Epoch 331/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.01234\n",
      "Epoch 332/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0010 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.01234\n",
      "Epoch 333/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.01234\n",
      "Epoch 334/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.01234\n",
      "Epoch 335/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.8661e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.01234\n",
      "Epoch 336/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.9667e-04 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.01234\n",
      "Epoch 337/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0211\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.01234\n",
      "Epoch 338/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.7560e-04 - val_loss: 0.0214\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.01234\n",
      "Epoch 339/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0280\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.01234\n",
      "Epoch 340/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.7171e-04 - val_loss: 0.0244\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.01234\n",
      "Epoch 341/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.6630e-04 - val_loss: 0.0249\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.01234\n",
      "Epoch 342/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.9598e-04 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.01234\n",
      "Epoch 343/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.9359e-04 - val_loss: 0.0237\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.01234\n",
      "Epoch 344/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.9182e-04 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.01234\n",
      "Epoch 345/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.9891e-04 - val_loss: 0.0326\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.01234\n",
      "Epoch 346/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.7799e-04 - val_loss: 0.0287\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.01234\n",
      "Epoch 347/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.9505e-04 - val_loss: 0.0332\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.01234\n",
      "Epoch 348/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0438\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.01234\n",
      "Epoch 349/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 9.9487e-04 - val_loss: 0.0268\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.01234\n",
      "Epoch 350/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.8031e-04 - val_loss: 0.0327\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.01234\n",
      "Epoch 351/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.9998e-04 - val_loss: 0.0249\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.01234\n",
      "Epoch 352/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0328\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.01234\n",
      "Epoch 353/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 0.0010 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.01234\n",
      "Epoch 354/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.01234\n",
      "Epoch 355/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0294\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.01234\n",
      "Epoch 356/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.8989e-04 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.01234\n",
      "Epoch 357/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.8573e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.01234\n",
      "Epoch 358/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.5653e-04 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.01234\n",
      "Epoch 359/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.5758e-04 - val_loss: 0.0160\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.01234\n",
      "Epoch 360/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.5414e-04 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.01234\n",
      "Epoch 361/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.6683e-04 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.01234\n",
      "Epoch 362/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 9.7061e-04 - val_loss: 0.0257\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.01234\n",
      "Epoch 363/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.4308e-04 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.01234\n",
      "Epoch 364/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.4641e-04 - val_loss: 0.0161\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.01234\n",
      "Epoch 365/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 0.0011 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.01234\n",
      "Epoch 366/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.01234\n",
      "Epoch 367/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.01234\n",
      "Epoch 368/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.7043e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.01234\n",
      "Epoch 369/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.6855e-04 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.01234\n",
      "Epoch 370/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.4741e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.01234\n",
      "Epoch 371/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.3861e-04 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.01234\n",
      "Epoch 372/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.3088e-04 - val_loss: 0.0266\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.01234\n",
      "Epoch 373/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.2886e-04 - val_loss: 0.0288\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.01234\n",
      "Epoch 374/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.3820e-04 - val_loss: 0.0161\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.01234\n",
      "Epoch 375/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.4071e-04 - val_loss: 0.0167\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.01234\n",
      "Epoch 376/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 9.4738e-04 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.01234\n",
      "Epoch 377/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.4590e-04 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.01234\n",
      "Epoch 378/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.5909e-04 - val_loss: 0.0311\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.01234\n",
      "Epoch 379/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 9.3205e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.01234\n",
      "Epoch 380/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.2941e-04 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.01234\n",
      "Epoch 381/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.6133e-04 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.01234\n",
      "Epoch 382/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.3261e-04 - val_loss: 0.0242\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.01234\n",
      "Epoch 383/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.1951e-04 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.01234\n",
      "Epoch 384/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.0479e-04 - val_loss: 0.0256\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.01234\n",
      "Epoch 385/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.9578e-04 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.01234\n",
      "Epoch 386/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.1439e-04 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.01234\n",
      "Epoch 387/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.0481e-04 - val_loss: 0.0273\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.01234\n",
      "Epoch 388/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.3761e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.01234\n",
      "Epoch 389/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 9.3846e-04 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.01234\n",
      "Epoch 390/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 9.0237e-04 - val_loss: 0.0235\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.01234\n",
      "Epoch 391/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0011 - val_loss: 0.0644\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.01234\n",
      "Epoch 392/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0473\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.01234\n",
      "Epoch 393/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0396\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.01234\n",
      "Epoch 394/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 9.3139e-04 - val_loss: 0.0225\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.01234\n",
      "Epoch 395/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.9709e-04 - val_loss: 0.0222\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.01234\n",
      "Epoch 396/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 0.0010 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.01234\n",
      "Epoch 397/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.7029e-04 - val_loss: 0.0262\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.01234\n",
      "Epoch 398/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.1404e-04 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.01234\n",
      "Epoch 399/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.7692e-04 - val_loss: 0.0271\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.01234\n",
      "Epoch 400/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.6766e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.01234\n",
      "Epoch 401/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.9318e-04 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.01234\n",
      "Epoch 402/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.4299e-04 - val_loss: 0.0213\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.01234\n",
      "Epoch 403/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 9.5603e-04 - val_loss: 0.0231\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.01234\n",
      "Epoch 404/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.5056e-04 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.01234\n",
      "Epoch 405/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.0893e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.01234\n",
      "Epoch 406/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.1711e-04 - val_loss: 0.0291\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.01234\n",
      "Epoch 407/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 9.2749e-04 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.01234\n",
      "Epoch 408/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.2725e-04 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.01234\n",
      "Epoch 409/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.9727e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.01234\n",
      "Epoch 410/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.4373e-04 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.01234\n",
      "Epoch 411/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.0632e-04 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.01234\n",
      "Epoch 412/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6867e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.01234\n",
      "Epoch 413/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6991e-04 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.01234\n",
      "Epoch 414/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.5762e-04 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.01234\n",
      "Epoch 415/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.5792e-04 - val_loss: 0.0159\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.01234\n",
      "Epoch 416/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.6300e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.01234\n",
      "Epoch 417/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.0636e-04 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.01234\n",
      "Epoch 418/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.9323e-04 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.01234\n",
      "Epoch 419/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.8127e-04 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.01234\n",
      "Epoch 420/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.1770e-04 - val_loss: 0.0204\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.01234\n",
      "Epoch 421/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.3379e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.01234\n",
      "Epoch 422/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.9045e-04 - val_loss: 0.0224\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.01234\n",
      "Epoch 423/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 9.0197e-04 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.01234\n",
      "Epoch 424/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.3802e-04 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.01234\n",
      "Epoch 425/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.3852e-04 - val_loss: 0.0275\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.01234\n",
      "Epoch 426/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.4173e-04 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.01234\n",
      "Epoch 427/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.5875e-04 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.01234\n",
      "Epoch 428/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.7412e-04 - val_loss: 0.0213\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.01234\n",
      "Epoch 429/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.7405e-04 - val_loss: 0.0251\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.01234\n",
      "Epoch 430/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6538e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.01234\n",
      "Epoch 431/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.8366e-04 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.01234\n",
      "Epoch 432/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.8773e-04 - val_loss: 0.0232\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.01234\n",
      "Epoch 433/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.7492e-04 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.01234\n",
      "Epoch 434/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.6013e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.01234\n",
      "Epoch 435/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6933e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.01234\n",
      "Epoch 436/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.5846e-04 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.01234\n",
      "Epoch 437/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.4907e-04 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.01234\n",
      "Epoch 438/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.3731e-04 - val_loss: 0.0259\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.01234\n",
      "Epoch 439/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 8.6041e-04 - val_loss: 0.0223\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.01234\n",
      "Epoch 440/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2070e-04 - val_loss: 0.0213\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.01234\n",
      "Epoch 441/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.5407e-04 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.01234\n",
      "Epoch 442/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.3099e-04 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.01234\n",
      "Epoch 443/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.4981e-04 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.01234\n",
      "Epoch 444/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.5979e-04 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.01234\n",
      "Epoch 445/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.2318e-04 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.01234\n",
      "Epoch 446/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.3372e-04 - val_loss: 0.0234\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.01234\n",
      "Epoch 447/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.2559e-04 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.01234\n",
      "Epoch 448/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.7928e-04 - val_loss: 0.0212\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.01234\n",
      "Epoch 449/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2080e-04 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.01234\n",
      "Epoch 450/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.3650e-04 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.01234\n",
      "Epoch 451/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.4830e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.01234\n",
      "Epoch 452/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.3042e-04 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.01234\n",
      "Epoch 453/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.0397e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.01234\n",
      "Epoch 454/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2297e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.01234\n",
      "Epoch 455/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6285e-04 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.01234\n",
      "Epoch 456/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.3540e-04 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.01234\n",
      "Epoch 457/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.4147e-04 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.01234\n",
      "Epoch 458/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.2518e-04 - val_loss: 0.0162\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.01234\n",
      "Epoch 459/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 8.2934e-04 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.01234\n",
      "Epoch 460/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.6297e-04 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.01234\n",
      "Epoch 461/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.6556e-04 - val_loss: 0.0252\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.01234\n",
      "Epoch 462/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.4171e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.01234\n",
      "Epoch 463/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.3709e-04 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.01234\n",
      "Epoch 464/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.1440e-04 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.01234\n",
      "Epoch 465/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9982e-04 - val_loss: 0.0327\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.01234\n",
      "Epoch 466/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9129e-04 - val_loss: 0.0376\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.01234\n",
      "Epoch 467/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2715e-04 - val_loss: 0.0416\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.01234\n",
      "Epoch 468/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 8.4289e-04 - val_loss: 0.0212\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.01234\n",
      "Epoch 469/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.9497e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.01234\n",
      "Epoch 470/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.5740e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.01234\n",
      "Epoch 471/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2812e-04 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.01234\n",
      "Epoch 472/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.3081e-04 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.01234\n",
      "Epoch 473/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.3624e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.01234\n",
      "Epoch 474/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.1868e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.01234\n",
      "Epoch 475/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2678e-04 - val_loss: 0.0256\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.01234\n",
      "Epoch 476/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.4765e-04 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.01234\n",
      "Epoch 477/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.1965e-04 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.01234\n",
      "Epoch 478/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2139e-04 - val_loss: 0.0235\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.01234\n",
      "Epoch 479/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9116e-04 - val_loss: 0.0269\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.01234\n",
      "Epoch 480/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.9915e-04 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.01234\n",
      "Epoch 481/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9604e-04 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.01234\n",
      "Epoch 482/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.2316e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.01234\n",
      "Epoch 483/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9469e-04 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.01234\n",
      "Epoch 484/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.2230e-04 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.01234\n",
      "Epoch 485/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.2338e-04 - val_loss: 0.0263\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.01234\n",
      "Epoch 486/2000\n",
      "70221/70221 [==============================] - 3s 39us/step - loss: 7.9257e-04 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.01234\n",
      "Epoch 487/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7239e-04 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.01234\n",
      "Epoch 488/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9723e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.01234\n",
      "Epoch 489/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.4167e-04 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.01234\n",
      "Epoch 490/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.4176e-04 - val_loss: 0.0440\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.01234\n",
      "Epoch 491/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.1128e-04 - val_loss: 0.0214\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.01234\n",
      "Epoch 492/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.0805e-04 - val_loss: 0.0260\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.01234\n",
      "Epoch 493/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.5627e-04 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.01234\n",
      "Epoch 494/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 7.9037e-04 - val_loss: 0.0309\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.01234\n",
      "Epoch 495/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.0609e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.01234\n",
      "Epoch 496/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 8.0323e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.01234\n",
      "Epoch 497/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9871e-04 - val_loss: 0.0252\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.01234\n",
      "Epoch 498/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.9745e-04 - val_loss: 0.0319\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.01234\n",
      "Epoch 499/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.9076e-04 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.01234\n",
      "Epoch 500/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.9257e-04 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.01234\n",
      "Epoch 501/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.6200e-04 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.01234\n",
      "Epoch 502/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.6335e-04 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.01234\n",
      "Epoch 503/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.6554e-04 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.01234\n",
      "Epoch 504/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.7706e-04 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.01234\n",
      "Epoch 505/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 8.5074e-04 - val_loss: 0.0574\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.01234\n",
      "Epoch 506/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.0534e-04 - val_loss: 0.0379\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.01234\n",
      "Epoch 507/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.5967e-04 - val_loss: 0.0331\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.01234\n",
      "Epoch 508/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.7266e-04 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.01234\n",
      "Epoch 509/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.7812e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.01234\n",
      "Epoch 510/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.5057e-04 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.01234\n",
      "Epoch 511/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.2837e-04 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.01234\n",
      "Epoch 512/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.6118e-04 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.01234\n",
      "Epoch 513/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7331e-04 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.01234\n",
      "Epoch 514/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.8547e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.01234\n",
      "Epoch 515/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7186e-04 - val_loss: 0.0293\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.01234\n",
      "Epoch 516/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.6539e-04 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.01234\n",
      "Epoch 517/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7817e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.01234\n",
      "Epoch 518/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.4951e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.01234\n",
      "Epoch 519/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.1273e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.01234\n",
      "Epoch 520/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.8639e-04 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.01234\n",
      "Epoch 521/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.5927e-04 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.01234\n",
      "Epoch 522/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7162e-04 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.01234\n",
      "Epoch 523/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.8151e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.01234\n",
      "Epoch 524/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.8323e-04 - val_loss: 0.0189\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.01234\n",
      "Epoch 525/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7416e-04 - val_loss: 0.0284\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.01234\n",
      "Epoch 526/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 7.8099e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.01234\n",
      "Epoch 527/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.4196e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.01234\n",
      "Epoch 528/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.5518e-04 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.01234\n",
      "Epoch 529/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.6139e-04 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.01234\n",
      "Epoch 530/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.4656e-04 - val_loss: 0.0189\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.01234\n",
      "Epoch 531/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.4048e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.01234\n",
      "Epoch 532/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.5786e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.01234\n",
      "Epoch 533/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.6857e-04 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.01234\n",
      "Epoch 534/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.6928e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.01234\n",
      "Epoch 535/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.5364e-04 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.01234\n",
      "Epoch 536/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 7.3564e-04 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.01234\n",
      "Epoch 537/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3403e-04 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.01234\n",
      "Epoch 538/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2553e-04 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.01234\n",
      "Epoch 539/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.6083e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.01234\n",
      "Epoch 540/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.6628e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.01234\n",
      "Epoch 541/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.4465e-04 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.01234\n",
      "Epoch 542/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3331e-04 - val_loss: 0.0244\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.01234\n",
      "Epoch 543/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.6710e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.01234\n",
      "Epoch 544/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.3386e-04 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.01234\n",
      "Epoch 545/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.7076e-04 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.01234\n",
      "Epoch 546/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3920e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.01234\n",
      "Epoch 547/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2725e-04 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.01234\n",
      "Epoch 548/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2166e-04 - val_loss: 0.0292\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.01234\n",
      "Epoch 549/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.1543e-04 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.01234\n",
      "Epoch 550/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.4531e-04 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.01234\n",
      "Epoch 551/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.4110e-04 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.01234\n",
      "Epoch 552/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.1767e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.01234\n",
      "Epoch 553/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.0645e-04 - val_loss: 0.0210\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.01234\n",
      "Epoch 554/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.8437e-04 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.01234\n",
      "Epoch 555/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.3557e-04 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.01234\n",
      "Epoch 556/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.2235e-04 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.01234\n",
      "Epoch 557/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.3109e-04 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.01234\n",
      "Epoch 558/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3275e-04 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.01234\n",
      "Epoch 559/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2829e-04 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.01234\n",
      "Epoch 560/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.0803e-04 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.01234\n",
      "Epoch 561/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.1413e-04 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.01234\n",
      "Epoch 562/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.0280e-04 - val_loss: 0.0234\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.01234\n",
      "Epoch 563/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.4751e-04 - val_loss: 0.0266\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.01234\n",
      "Epoch 564/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 9.7739e-04 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.01234\n",
      "Epoch 565/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 0.0012 - val_loss: 0.0224\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.01234\n",
      "Epoch 566/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.8649e-04 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.01234\n",
      "Epoch 567/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.8715e-04 - val_loss: 0.0222\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.01234\n",
      "Epoch 568/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2334e-04 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.01234\n",
      "Epoch 569/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 7.0431e-04 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.01234\n",
      "Epoch 570/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 8.3188e-04 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.01234\n",
      "Epoch 571/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.8551e-04 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.01234\n",
      "Epoch 572/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.4868e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.01234\n",
      "Epoch 573/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.1069e-04 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.01234\n",
      "Epoch 574/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.0147e-04 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.01234\n",
      "Epoch 575/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2928e-04 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.01234\n",
      "Epoch 576/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 8.0040e-04 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.01234\n",
      "Epoch 577/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3170e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.01234\n",
      "Epoch 578/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.1592e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.01234\n",
      "Epoch 579/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.9023e-04 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.01234\n",
      "Epoch 580/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.1199e-04 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.01234\n",
      "Epoch 581/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.0061e-04 - val_loss: 0.0257\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.01234\n",
      "Epoch 582/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.1263e-04 - val_loss: 0.0215\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.01234\n",
      "Epoch 583/2000\n",
      "70221/70221 [==============================] - 2s 36us/step - loss: 7.1202e-04 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.01234\n",
      "Epoch 584/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.8498e-04 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.01234\n",
      "Epoch 585/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.1629e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.01234\n",
      "Epoch 586/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.8031e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.01234\n",
      "Epoch 587/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.4545e-04 - val_loss: 0.0283\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.01234\n",
      "Epoch 588/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.0608e-04 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.01234\n",
      "Epoch 589/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2497e-04 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.01234\n",
      "Epoch 590/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.3699e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.01234\n",
      "Epoch 591/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.3730e-04 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.01234\n",
      "Epoch 592/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.3683e-04 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.01234\n",
      "Epoch 593/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.1099e-04 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.01234\n",
      "Epoch 594/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.0825e-04 - val_loss: 0.0348\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.01234\n",
      "Epoch 595/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.2073e-04 - val_loss: 0.0240\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.01234\n",
      "Epoch 596/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.4766e-04 - val_loss: 0.0247\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.01234\n",
      "Epoch 597/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 7.0885e-04 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.01234\n",
      "Epoch 598/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 6.8920e-04 - val_loss: 0.0189\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.01234\n",
      "Epoch 599/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.8385e-04 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.01234\n",
      "Epoch 600/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.7065e-04 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.01234\n",
      "Epoch 601/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.9229e-04 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.01234\n",
      "Epoch 602/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.7998e-04 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.01234\n",
      "Epoch 603/2000\n",
      "70221/70221 [==============================] - 2s 35us/step - loss: 6.9449e-04 - val_loss: 0.0236\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.01234\n",
      "Epoch 604/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.9629e-04 - val_loss: 0.0213\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.01234\n",
      "Epoch 605/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 7.2286e-04 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.01234\n",
      "Epoch 606/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 7.1674e-04 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.01234\n",
      "Epoch 607/2000\n",
      "70221/70221 [==============================] - 3s 37us/step - loss: 6.8355e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.01234\n",
      "Epoch 608/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.7154e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.01234\n",
      "Epoch 609/2000\n",
      "70221/70221 [==============================] - 3s 38us/step - loss: 6.6912e-04 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.01234\n",
      "Epoch 610/2000\n",
      "70221/70221 [==============================] - 3s 36us/step - loss: 6.6974e-04 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.01234\n",
      "Epoch 00610: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train_scaled, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1, validation_data=(X_val, Y_val_scaled),\n",
    "                     callbacks=[tensorboard,model_check,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now().replace(second=0, microsecond=0)\n",
    "model.save_weights('../redes_CNN_R/defs/CNN_regression_R_{}_{}_{}_{}_{}'.format(nb_epoch,batch_size,experimento,algoritmo,dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mse: 0.013199782612022478\n",
      "[185.94308 185.07607 190.39127 190.20207 190.84041 171.10771 171.65938\n",
      " 171.05229 186.79358 187.04997]\n",
      "[[0.65554837]\n",
      " [0.65554837]\n",
      " [0.90391751]\n",
      " [0.90391751]\n",
      " [0.90391751]\n",
      " [0.26925994]\n",
      " [0.26925994]\n",
      " [0.26925994]\n",
      " [0.80368228]\n",
      " [0.80368228]]\n",
      "[-1.27658863 -0.40956898  1.72616278  1.91535651  1.27702032  1.97035179\n",
      "  1.41868553  2.02577171  2.31684981  2.06045637]\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(best_model_name)\n",
    "score = best_model.evaluate(X_test, Y_test_scaled, verbose=0)\n",
    "print('Test mse:', score)\n",
    "# print('Test mae:', score[1])\n",
    "Y_test_predicted_scaled=best_model.predict(X_test)\n",
    "\n",
    "\n",
    "Y_test_predicted = min_max_scaler.inverse_transform(Y_test_predicted_scaled)\n",
    "error_prediction=Y_test-Y_test_predicted.flatten()\n",
    "print(Y_test_predicted[:10].flatten())\n",
    "print(Y_test_scaled[:10])\n",
    "print(error_prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(error_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n",
      "(401,)\n",
      "[[Model]]\n",
      "    Model(gaussian)\n",
      "[[Fit Statistics]]\n",
      "    # fitting method   = leastsq\n",
      "    # function evals   = 29\n",
      "    # data points      = 400\n",
      "    # variables        = 3\n",
      "    chi-square         = 135197.274\n",
      "    reduced chi-square = 340.547289\n",
      "    Akaike info crit   = 2335.21029\n",
      "    Bayesian info crit = 2347.18469\n",
      "[[Variables]]\n",
      "    amp:  702.982689 +/- 4.96193182 (0.71%) (init = 1000)\n",
      "    cen:  0.03670732 +/- 0.01320357 (35.97%) (init = 0)\n",
      "    wid:  2.29111107 +/- 0.01867267 (0.82%) (init = 1)\n",
      "[[Correlations]] (unreported correlations are < 0.100)\n",
      "    C(amp, wid) = -0.577\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(error_prediction, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "plt.title(\"Histogram with 400 bins\")\n",
    "plt.show()\n",
    "x=bins[:400]\n",
    "y=n\n",
    "print(n.shape)\n",
    "print(bins.shape)\n",
    "from numpy import exp, loadtxt, pi, sqrt, log\n",
    "\n",
    "from lmfit import Model\n",
    "def gaussian(x, amp, cen, wid):\n",
    "    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "    return amp * exp(-((x-cen)/wid)**2 )\n",
    "\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "\n",
    "print(result.fit_report())\n",
    "\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, result.init_fit, 'k--')\n",
    "plt.plot(x, result.best_fit, 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FWHM=result.params['wid'].value*2*sqrt(log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8149501690948777\n"
     ]
    }
   ],
   "source": [
    "print(FWHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35356084]\n",
      " [ 0.5765002 ]\n",
      " [ 0.17945427]\n",
      " ...\n",
      " [ 0.40980875]\n",
      " [ 0.41639408]\n",
      " [-0.19213305]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6651092471274562\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin0_predicted_scaled=best_model.predict(X_test_bin0)\n",
    "print(Y_test_bin0_predicted)\n",
    "Y_test_bin0_predicted = min_max_scaler.inverse_transform(Y_test_bin0_predicted_scaled)\n",
    "error_prediction_bin0=Y_test_bin0-Y_test_bin0_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin0, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin0=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6651092471274562\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPsklEQVR4nO3dcYykdX3H8c9H1kJEE1EWekV0qVIitHro9miCULcWi/4DtNpyaeg1oTlNuUQTTYsmbfWfxjYF+oeNzRmI+4dibZFKIpWSy0VCQ6h7sBx3vRLgem0PT26RJiBUm4Nv/5hnZZib2Xlm5nnmeb4771eymZnneeZ5PvPMzGfnnv3Nc44IAQDyeU3TAQAA46HAASApChwAkqLAASApChwAkpqb5sbOPPPMWFhYmOYmASC9ffv2PRMR873Tp1rgCwsLWllZmeYmASA92//ZbzqHUAAgKQocAJKiwAEgKQocAJKiwAEgKQocAJKiwAEgKQocAJKiwAEgKQocM2dpeamWZYFpo8ABICkKHACSosABICkKHACSosABICkKHACSosABICkKHACSosABICkKHACSosABIKmhBW77XNt7bR+yfdD2J4rpn7P9lO3V4ufD9ccFAKybK7HMCUmfioiHbL9B0j7b9xbzbomIv6ovHgBgkKEFHhHHJB0rrj9v+5Ckc+oOBgDY2EjHwG0vSLpY0oPFpF2299u+zfYZA+6z0/aK7ZW1tbWJwmLz4rStwOhKF7jt10u6Q9InI+I5SV+S9HZJW9X5hH5Tv/tFxO6IWIyIxfn5+QoiAwCkkgVu+7XqlPdXI+KbkhQRT0fESxHxsqQvS9pWX0wAQK8yo1As6VZJhyLi5q7pW7oWu0bSgerjAQAGKTMK5VJJ10l61PZqMe2zkrbb3iopJB2R9LFaEgIA+iozCuV+Se4z6+7q4wAAyuKbmACQFAUOAElR4DOCcdbA5kOBA0BSFDgAJEWBA0BSFDgAJEWBA0BSFDgAJEWB41WaGG640TbrzsPwSmRGgQNAUhQ4ACRFgQNAUhQ4ACRFgQNAUhQ4ACRFgQNAUhQ4Wosx2sDGKHAASIoCB4CkKHAASIoCB4CkKHAASIoCB4CkKHAASIoCH9HS8lLp8cmbfRzzsMfXPX/QslXuo82+v6tQ5jkpOx/No8ABICkKHACSosABIKmhBW77XNt7bR+yfdD2J4rpb7J9r+3Hi8sz6o8LAFhX5hP4CUmfioh3SvoVSTfYvlDSjZL2RMT5kvYUtwEAUzK0wCPiWEQ8VFx/XtIhSedIukrScrHYsqSr6woJADjZSMfAbS9IuljSg5LOjohjUqfkJZ014D47ba/YXllbW5ssLSbWOwxy0LDIcYaQjXqftg1Ty55/XJvlccyi0gVu+/WS7pD0yYh4ruz9ImJ3RCxGxOL8/Pw4GQEAfZQqcNuvVae8vxoR3ywmP217SzF/i6Tj9UQEAPRTZhSKJd0q6VBE3Nw16y5JO4rrOyR9q/p4AIBB5kosc6mk6yQ9anu1mPZZSV+Q9A3b10v6L0kfrSciAKCfoQUeEfdL8oDZH6g2DgCgLL6JCQBJUeAAkBQFXoHNelrOMqfOreM0sXXsr/V1tv25aHu+Ks3SY60LBQ4ASVHgAJAUBQ4ASVHgAJAUBQ4ASVHgAJAUBQ4ASVHgI2jLuNW25GiDOseMt2U9bdlOlTJmbiMKHACSosABICkKHACSosABICkKHACSosABICkKvGb9hktt1tPP1ql3n5TdR2VOiTtplqqXn0Rd22rDa7INGdqGAgeApChwAEiKAgeApChwAEiKAgeApChwAEiKAgeApCjwHk2Mo617nPL67Sq3072uNo7PrSLTKOsYtOwkOcq8Zpoe417l+tv4Omo7ChwAkqLAASApChwAkqLAASCpoQVu+zbbx20f6Jr2OdtP2V4tfj5cb0wAQK8yn8C/IunKPtNviYitxc/d1cYCAAwztMAj4j5Jz04hCwBgBJMcA99le39xiOWMQQvZ3ml7xfbK2traBJtr1jjn9R5nnZOsq85xtOOuu8nx4tPY3qD9XtV45yr2+6TrQnuNW+BfkvR2SVslHZN006AFI2J3RCxGxOL8/PyYmwMA9BqrwCPi6Yh4KSJelvRlSduqjQUAGGasAre9pevmNZIODFoWAFCPuWEL2L5d0vslnWn7qKQ/k/R+21slhaQjkj5WY0YAQB9DCzwitveZfGsNWQAAI+CbmACQFAU+xLinCS07f9CpXyfJNo6qhvsNG742zvC6SfZRGeOsb1r3qVuVwxQnWR/GQ4EDQFIUOAAkRYEDQFIUOAAkRYEDQFIUOAAkRYEDQFIUeJdxxkJPa9xr2bHQdYyZLrOOQftu9QerlW9ro2Wrej7aNp55mnkGPZeDXlujZqt7+VlCgQNAUhQ4ACRFgQNAUhQ4ACRFgQNAUhQ4ACRFgQNAUjNZ4MPGe1d9juQy86s+v/JmHTvb5Hmol5aXajsXeVXnY580x6jzyi4z7r7brK/jqsxkgQPAZkCBA0BSFDgAJEWBA0BSFDgAJEWBA0BSFPgGmh7CV3aI4zhD6yYZ0jWtU9TWua46Tmk67qlwq8pSdkhilcMhmxrm1/Swy7agwAEgKQocAJKiwAEgKQocAJIaWuC2b7N93PaBrmlvsn2v7ceLyzPqjQkA6FXmE/hXJF3ZM+1GSXsi4nxJe4rbAIApGlrgEXGfpGd7Jl8labm4vizp6opzAQCGGPcY+NkRcUySisuzBi1oe6ftFdsra2trY26uet1jYSc5DWzvMlWsq4r7jKPqsc6zaJL90+SpcutSxymW8Yra/4gZEbsjYjEiFufn5+veHADMjHEL/GnbWySpuDxeXSQAQBnjFvhdknYU13dI+lY1cQAAZZUZRni7pAckXWD7qO3rJX1B0hW2H5d0RXEbADBFc8MWiIjtA2Z9oOIsAIAR8E1MAEiKAgeApIYeQkEzRjnvN8ZTdn82ce7yKsbkj3r+9jac23vYMkvLS9q7Y2/dkdLgEzgAJEWBA0BSFDgAJEWBA0BSFDgAJEWBA0BSFHhyszS0MMtjrWvY3jQffxWnth11GOOosrwe6kSBA0BSFDgAJEWBA0BSFDgAJEWBA0BSFDgAJEWBA0BSM1PgZU+9OUs262Me53HVPWa5KlWMz65LVRmWlpc2XFf3vDY87ibNTIEDwGZDgQNAUhQ4ACRFgQNAUhQ4ACRFgQNAUhQ4ACQ1EwWeZYzvrGjqeWjy+Z+l196wcdx1brff9c1sJgocADYjChwAkqLAASCpuUnubPuIpOclvSTpREQsVhEKADDcRAVeWIqIZypYDwBgBBxCAYCkJi3wkPTPtvfZ3tlvAds7ba/YXllbW5twc6PrHU40K8OLsuN5mk0876OZtMAvjYj3SPqQpBtsX967QETsjojFiFicn5+fcHMAgHUTFXhEfL+4PC7pTknbqggFABhu7AK3fbrtN6xfl/RBSQeqCgYA2Ngko1DOlnSn7fX1fC0ivlNJKgDAUGMXeEQclvTuCrMAAEbAMEIASIoCB4CkNlWBz+LpJAG8Wr/3/mbtg01V4AAwSyhwAEiKAgeApChwAEiKAgeApChwAEiKAgeApDZ1gWcb+5ktL2ZTG1+nVWZq4+MbZFMXOABsZhQ4ACRFgQNAUhQ4ACRFgQNAUhQ4ACSVssDXh/n0XvZeB4BeS8tLA3tio/7Y6H5NSVngAAAKHADSosABICkKHACSosABICkKHACSosABIClHxNQ2tri4GCsrK2Pdd3385d4de1s3FhODrf5gVVt/dmvTMTCD+nVF77S9O/ZK2nj890b3Wb8+zCjL9mN7X0Qs9k7nEzgAJEWBA0BSFDgAJEWBA0BSExW47SttP2b7Cds3VhUKADDc2AVu+xRJfyPpQ5IulLTd9oVVBQMAbGyST+DbJD0REYcj4v8kfV3SVdXEAgAMM/Y4cNsfkXRlRPxBcfs6SZdExK6e5XZK2lncvEDSYyU3caakZ8YKN13krBY5q0XOajWV820RMd87cW6CFbrPtJN+G0TEbkm7R165vdJv4HrbkLNa5KwWOavVtpyTHEI5KuncrttvkfT9yeIAAMqapMC/J+l82+fZ/hlJ10q6q5pYAIBhxj6EEhEnbO+SdI+kUyTdFhEHK0s2xmGXhpCzWuSsFjmr1aqcUz2ZFQCgOnwTEwCSosABIKlGCtz2bbaP2z7QNe3vbK8WP0dsr3bN+0zxdf3HbP9GG3PafrPtvbZ/ZPuL08o4Rs4rbO+z/Whx+Wstzbmta/ojtq9pY86u+W8tnvtPtzGn7QXb/9s172/bmLOY9y7bD9g+WLxOT2tbTtu/2zV91fbLtqd/4vuImPqPpMslvUfSgQHzb5L0p8X1CyU9IulUSedJelLSKS3Mebqk90n6uKQvtnh/Xizp54rrvyjpqZbmfJ2kueL6FknH12+3KWfXtDsk/b2kT7d0fy4MWq5lOeck7Zf07uL2m9v4fu+Z/kuSDjexbyf5Is/YIuI+2wv95tm2pN+WtP7J8CpJX4+In0j6D9tPqPM1/gfalDMiXpB0v+131J2r14g5H+6afVDSabZPLfZvm3K+2DX7NPX5klhdRnx9yvbVkg5LemEa+daNmrMpI+b8oKT9EfFIcd8fTiNjsa1x9+d2SbfXl2ywNh4Dv0zS0xHxeHH7HEn/3TX/aDGtab0522qjnL8l6eFplHcJJ+W0fYntg5IelfTxiDjRWLpXvCqn7dMl/bGkzzea6mT9nvfzbD9s+7u2L2sqWI/enL8gKWzfY/sh23/UYLZuG72PfkcNFXgjn8CH6P1tVuor+w1o7LfuiPrmtH2RpL9Q5xNPG5yUMyIelHSR7XdKWrb9TxHx40bSvaI35+cl3RIRP+p8SGuN3pzHJL01In5o+72S/tH2RRHxXDPxfqo355w6hyJ/WdKLkva48/9B7mkiXJdB76NLJL0YEQdOvkv9WlXgtuck/aak93ZNbt1X9gfkbJ1BOW2/RdKdkn4vIp5sIltPng33Z0Qcsv2COsfsx/tfsSswIOclkj5i+y8lvVHSy7Z/HBFT/UN2t345i39l/aS4vs/2k+p82m3b/jwq6bsR8UyxzN3qHJdurMCHvD6vVYMf5Np2COXXJf17RBztmnaXpGttn2r7PEnnS/rXRtK9ol/ONjopp+03Svq2pM9ExL80luzV+uU8r3jjyPbb1DmT5ZFm4v3USTkj4rKIWIiIBUl/LenPmyzvQr/9Oe/OOfxl++fVeR8dbijfun7vo3skvcv264rn/1cl/Vsj6V7R9/1u+zWSPqrOqbQb0dQwwtvV+SPkBbaP2r6+mHXSb7PofD3/G+o8id+RdENEvNS2nMXyRyTdLOn3i+Wn8h9cjJhzl6R3SPqTriFQZ7Uw5/skPVIM27pT0h+ufyprWc7GjJjzckn7bT8i6R/U+ZvCs23LGRH/o8576HuSViU9FBHfblvOwuWSjkZEY78I+So9ACTVtkMoAICSKHAASIoCB4CkKHAASIoCB4CkKHAASIoCB4Ck/h8UCnAcrp8gCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin1_predicted_scaled=best_model.predict(X_test_bin1)\n",
    "#print(Y_test_bin1_predicted)\n",
    "Y_test_bin1_predicted = min_max_scaler.inverse_transform(Y_test_bin1_predicted_scaled)\n",
    "error_prediction_bin1=Y_test_bin1-Y_test_bin1_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin1, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin1=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6651092471274562\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVW0lEQVR4nO3df4xl5X3f8ffH/Ghr7AQcBoz54XVjSgopbMh0IcFxwQ4EqJW1IzeBuA5tiTa2sBRbTmWnlUxly2qaxkmVOAqiQMCVi5ETkyAFGyhNS1x+mFm0wGJwwIiUzSJ2MARsEdVe59s/7ll5PNw7c+eeM3Pv7Hm/pKs55znPOed7z8585u6595knVYUkqV9eNe0CJEkbz/CXpB4y/CWphwx/Seohw1+SeujQaRcwzNFHH11btmyZdhmStGns3LnzuaqaG7f/TIb/li1bWFhYmHYZkrRpJPmrtfT3to8k9ZDhL0k9ZPhLUg8Z/pLUQ4a/JPWQ4S9JPWT4S1IPGf6S1EOGvyT1kOEvST1k+EtSDxn+ktRDhr8k9ZDhL0k9ZPhLUg+tGv5JTkzy50keTfJIkl9t2l+X5I4kjzdfjxqx/2VNn8eTXNb1E5Akrd04r/z3Ax+uqn8MnA1ckeRU4KPAnVV1MnBns/59krwOuBI4C9gGXDnql4QkaeOsGv5V9UxVPdAsfxN4FDge2A7c0HS7AXjnkN1/Brijqp6vqheAO4ALuyhckjS5Nd3zT7IF+DHgPuDYqnoGBr8ggGOG7HI88PSS9T1NmyRpisYO/ySvAf4Y+GBVvTTubkPaasTxdyRZSLKwuLg4blmSpAmMFf5JDmMQ/J+tqi80zc8mOa7Zfhywb8iue4ATl6yfAOwddo6qurqq5qtqfm5u7AnoJUkTGOfTPgGuBR6tqt9esukW4MCndy4D/nTI7rcBFyQ5qnmj94KmTZI0ReO88j8HeC/wtiS7msfFwG8A5yd5HDi/WSfJfJJrAKrqeeATwP3N4+NNmyRpilI19Bb8VM3Pz9fCwsK0y5CkTSPJzqqaH7e/I3wlqYcMf0nqIcNfknrI8JekHjL8DxLn3XDetEs46HmNdTAx/CWphwx/Seohw1+Sesjwl6QeMvwlqYcMf0nqIcNfknrI8JekHjL8JamHDH+NZZqjW2dhZO1qNfT9+iw1a/VoOMNfknro0NU6JLkOeAewr6p+tGm7CTil6XIk8DdVtXXIvk8B3wS+C+xfy0QDkqT1s2r4A9cDnwY+c6Chqn7hwHKSTwEvrrD/eVX13KQFSpK6t2r4V9VdSbYM29ZM7v7zwNu6LUuStJ7a3vP/KeDZqnp8xPYCbk+yM8mOlQ6UZEeShSQLi4uLLcuSJK2kbfhfCty4wvZzqupM4CLgiiRvHdWxqq6uqvmqmp+bm2tZliRpJROHf5JDgZ8DbhrVp6r2Nl/3ATcD2yY9nySpO21e+f808FhV7Rm2MckRSV57YBm4ANjd4nySpI6sGv5JbgTuAU5JsifJ5c2mS1h2yyfJG5Lc2qweC3w5yYPAV4A/q6ovdVe6JGlSq4Z/VV1aVcdV1WFVdUJVXdu0/6uqumpZ371VdXGz/GRVndE8TquqT67PU5iu9RzNOIsjJTe6pmlfg2mfv2sH2/PR5BzhK0k9ZPhLUg8Z/pLUQ4a/JPWQ4S9JPWT4S1IPGf6S1EOGvyT1kOEvST1k+HdklkZODqtlvesb5/gbVVdXx5zWv+kk552l779J9fV5T4vhL0k9ZPhLUg8Z/pLUQ4a/JPWQ4S9JPWT4S1IPjTOT13VJ9iXZvaTtPyT56yS7msfFI/a9MMnXkjyR5KNdFi5Jmtw4r/yvBy4c0v47VbW1edy6fGOSQ4DfBy4CTgUuTXJqm2IlSd0YZxrHu4DnJzj2NuCJZjrHbwOfA7ZPcBxJUsfa3PP/QJKHmttCRw3Zfjzw9JL1PU3bUEl2JFlIsrC4uNiirO4tH0U4C6Nlp3G8pcc5sDzq2JOO+F3LuSfVpuauapglG/lcuvx37Mqs1LHRJg3/PwB+GNgKPAN8akifDGmrUQesqqurar6q5ufm5iYsS5I0jonCv6qerarvVtXfAf+VwS2e5fYAJy5ZPwHYO8n5JEndmij8kxy3ZPVdwO4h3e4HTk7ypiSHA5cAt0xyPklStw5drUOSG4FzgaOT7AGuBM5NspXBbZyngF9p+r4BuKaqLq6q/Uk+ANwGHAJcV1WPrMuzkCStyarhX1WXDmm+dkTfvcDFS9ZvBV7xMVBJ0nQ5wleSesjwl6QeMvwlqYcMf0nqIcN/TOs1L+xmHl243s+l61HCXRxnM/97rWS1Edvj7Dus/WC9XgcDw1+Sesjwl6QeMvwlqYcMf0nqIcNfknrI8JekHjL8JamHDH9J6iHDX5J6yPBvYdjoxTYjGjdybtn1HHnZZpRo21Ghq+07zvY2NcziHLWjjHqe61X3Rn5vzvq1nwWrhn8zQfu+JLuXtP3nJI81E7jfnOTIEfs+leThJLuSLHRZuCRpcuO88r8euHBZ2x3Aj1bV6cBfAr++wv7nVdXWqpqfrERJUtdWDf+qugt4flnb7VW1v1m9l8Hk7JKkTaKLe/7/BvjiiG0F3J5kZ5IdHZxLktSBVefwXUmSfw/sBz47oss5VbU3yTHAHUkea/4nMexYO4AdACeddFKbsiRJq5j4lX+Sy4B3AO+pqhrWp5nQnaraB9wMbBt1vKq6uqrmq2p+bm5u0rIkSWOYKPyTXAh8BPjZqnp5RJ8jkrz2wDJwAbB7WF9J0sYa56OeNwL3AKck2ZPkcuDTwGsZ3MrZleSqpu8bktza7Hos8OUkDwJfAf6sqr60Ls9CkrQm43za59KqOq6qDquqE6rq2qp6c1Wd2HyEc2tVva/pu7eqLm6Wn6yqM5rHaVX1yfV+MrNo2PR46z2QabVzrHX6xbbbV6trvQfkrDRF4UZMNdjFOboYdNZ2vy6vUxcDGrvUx0FhjvCVpB4y/CWphwx/Seohw1+Sesjwl6QeMvwlqYcMf0nqIcNfknrI8JekHjL8V7GeIxHXeoxpjRLt4hzjjkpeS/9JztHm+JNYy+jojR75PI6NuqYbte8sXuNpMfwlqYcMf0nqIcNfknrI8JekHjL8JamHDH9J6qGxwj/JdUn2Jdm9pO11Se5I8njz9agR+17W9Hm8mfdXkjRl477yvx64cFnbR4E7q+pk4M5m/fskeR1wJXAWg8nbrxz1S0KStHHGCv+qugt4flnzduCGZvkG4J1Ddv0Z4I6qer6qXgDu4JW/RCRJG6zNPf9jq+oZgObrMUP6HA88vWR9T9P2Ckl2JFlIsrC4uNiirNm1UXPVtu2/EXO1TnqslebjneR4bfsNq2kjRjPPirbzSY97jKX9xp1feJLzjvp32cz/RqOs9xu+GdJWwzpW1dVVNV9V83Nzc+tcliT1W5vwfzbJcQDN131D+uwBTlyyfgKwt8U5JUkdaBP+twAHPr1zGfCnQ/rcBlyQ5Kjmjd4LmjZJ0hSN+1HPG4F7gFOS7ElyOfAbwPlJHgfOb9ZJMp/kGoCqeh74BHB/8/h40yZJmqJDx+lUVZeO2PT2IX0XgF9esn4dcN1E1UmS1oUjfCWphwx/Seohw1+Sesjwl6Qe6n34jxoF2MXo10lGo44752sXo1PH6buWa7OWfhs1n3CXI4KHHX+9rfb9Oc629fhemaT/pLo+zzg/2wfjiN7leh/+ktRHhr8k9ZDhL0k9ZPhLUg8Z/pLUQ4a/JPWQ4S9JPWT4S1IPGf6S1EOGf6Prkax9GS05C+c8GEdjTjo6epxtq81t20Vt0xpVfDB+L6wXw1+Semji8E9ySpJdSx4vJfngsj7nJnlxSZ+PtS9ZktTWWDN5DVNVXwO2AiQ5BPhr4OYhXf+iqt4x6XkkSd3r6rbP24GvV9VfdXQ8SdI66ir8LwFuHLHtJ5I8mOSLSU4bdYAkO5IsJFlYXFzsqCxJ0jCtwz/J4cDPAp8fsvkB4I1VdQbwe8CfjDpOVV1dVfNVNT83N9e2LEnSCrp45X8R8EBVPbt8Q1W9VFXfapZvBQ5LcnQH55QktdBF+F/KiFs+SV6fJM3ytuZ83+jgnJKkFib+tA9AklcD5wO/sqTtfQBVdRXwbuD9SfYDfwtcUlXV5pySpPZavfKvqper6oeq6sUlbVc1wU9VfbqqTquqM6rq7Kq6u23B663t6McDx2jbf3lbF6OI12rWR0tOq75Jz9tFvesx8rcLXc5T3fY4o352+jZH72oc4StJPWT4S1IPGf6S1EOGvyT1kOEvST1k+EtSDxn+ktRDhr8k9ZDhL0k9ZPjPOOcs3Zy6nOd21DGnMWp30tq7rGGS8/tz8kqGvyT1kOEvST1k+EtSDxn+ktRDhr8k9VAXc/g+leThJLuSLAzZniS/m+SJJA8lObPtOSVJ7bSayWuJ86rquRHbLgJObh5nAX/QfJUkTclG3PbZDnymBu4Fjkxy3AacV5I0QhfhX8DtSXYm2TFk+/HA00vW9zRt3yfJjiQLSRYWFxc7KGu0LqZqnCVdTg+40YN41stmq7/r697l85/GtVz+MzpLU3YurW2zfZ8t1UX4n1NVZzK4vXNFkrcu254h+7xiEvequrqq5qtqfm5uroOyJEmjtA7/qtrbfN0H3AxsW9ZlD3DikvUTgL1tzytJmlyr8E9yRJLXHlgGLgB2L+t2C/BLzad+zgZerKpn2pxXktRO20/7HAvcnOTAsf57VX0pyfsAquoq4FbgYuAJ4GXgX7c8pySppVbhX1VPAmcMab9qyXIBV7Q5jySpW47wlaQeMvwlqYcMf0nqIcNfknqoV+G/2jR4m3m03qT6+JynabNe781a9yTaPtfNcq16Ff6SpAHDX5J6yPCXpB4y/CWphwx/Seohw1+Sesjwl6QeMvwlqYcMf0nqoYMu/EfNuTmtc2tjeO21Xt8Do+brXWke31HzhM/S/OEHXfhLklY3cfgnOTHJnyd5NMkjSX51SJ9zk7yYZFfz+Fi7ciVJXWgzk9d+4MNV9UAzj+/OJHdU1VeX9fuLqnpHi/NIkjo28Sv/qnqmqh5olr8JPAoc31VhkqT108k9/yRbgB8D7huy+SeSPJjki0lOW+EYO5IsJFlYXFzsoixJ0gitwz/Ja4A/Bj5YVS8t2/wA8MaqOgP4PeBPRh2nqq6uqvmqmp+bm2tbliRpBa3CP8lhDIL/s1X1heXbq+qlqvpWs3wrcFiSo9ucU5LUXptP+wS4Fni0qn57RJ/XN/1Isq053zcmPackqRttPu1zDvBe4OEku5q2fwecBFBVVwHvBt6fZD/wt8AlVVUtzilJ6kCbT/t8uapSVadX1dbmcWtVXdUEP1X16ao6rarOqKqzq+ru7kofbaVRvm1G183KyDxJ07FSBozaNk5uTCNbHOErST1k+EtSDxn+ktRDhr8k9ZDhL0k9ZPhLUg8Z/pLUQ4a/JPWQ4S9JPXRQh/8szZcp6eC11r8gMKz/RmfVQR3+kqThDH9J6iHDX5J6yPCXpB4y/CWphwx/SeqhtnP4Xpjka0meSPLRIdv/XpKbmu33JdnS5nySpG60mcP3EOD3gYuAU4FLk5y6rNvlwAtV9Wbgd4D/NOn5JEndafPKfxvwRFU9WVXfBj4HbF/WZztwQ7P8R8DbD0zoLkmankw6n3qSdwMXVtUvN+vvBc6qqg8s6bO76bOnWf960+e5IcfbAexoVk8BvrZKCUcDrzjOJrAZ696MNYN1bzTr3jjDan5jVc2Ne4BDW5x82Cv45b9JxukzaKy6Grh67JMnC1U1P27/WbEZ696MNYN1bzTr3jhd1Nzmts8e4MQl6ycAe0f1SXIo8IPA8y3OKUnqQJvwvx84OcmbkhwOXALcsqzPLcBlzfK7gf9Zk95nkiR1ZuLbPlW1P8kHgNuAQ4DrquqRJB8HFqrqFuBa4L8leYLBK/5Luii6MfYtohmzGevejDWDdW806944rWue+A1fSdLm5QhfSeohw1+Semgmwz/JdUn2NeMEDrTdlGRX83gqya4l205Pck+SR5I8nOTvz3rdSd6zpH1Xkr9LsnUT1H1Ykhua6/xokl+fRs0T1H14kj9s6n4wybkzVvfWJPc2dS8k2da0J8nvNn8i5aEkZ26Cmn+k+Xn8f0l+bRr1LqlxLXW/p7nGDyW5O8kZm6Tu7U3NB9rfMtZJqmrmHsBbgTOB3SO2fwr4WLN8KPAQcEaz/kPAIbNe97L2fwI8uUmu9y8Cn2uWXw08BWzZBHVfAfxhs3wMsBN41azUDdwOXNQsXwz8ryXLX2QwZuZs4L5NUPMxwD8FPgn82jTqnbDunwSOapYvmta1nqDu1/C9929PBx4b5xwz+cq/qu5ixHiAJAF+HrixaboAeKiqHmz2/UZVfXdDCl1mjXUvdemI9g2xxroLOKIZt/EPgG8DL21Encutse5TgTub/fYBfwNMZWDPiLoL+IFm+Qf53piZ7cBnauBe4Mgkx21MpUuKW0PNVbWvqu4HvrNxFQ63xrrvrqoXmvZ7GYxdmoo11v2tapIfOIIRA2mXazPCd1p+Cni2qh5v1v8RUEluA+YYvCr9zalVN9ryupf6BV75d5FmxfK6/4hBrc8weOX/oaqaxYF7y+t+ENie5HMMBh7+ePP1K1Oqb7kPArcl+S0Gt2N/smk/Hnh6Sb89TdszG1veUKNqnnXj1H05g/9xzZKRdSd5F/AfGfyv65+Pc7CZfOW/iuWvkg8F3gK8p/n6riRvn0Zhqxj66j7JWcDLVbX7lbvMhOV1bwO+C7wBeBPw4ST/cBqFrWJ53dcxCM4F4L8AdwP7p1DXKO9n8Iv0ROBDDMbIwBr+RMoUjKp51q1Yd5LzGIT/R6ZQ20pG1l1VN1fVjwDvBD4xzsE2Vfg3txp+DrhpSfMe4H9X1XNV9TJwK4N7ZTNjRN0HXMIUb/msZETdvwh8qaq+09w++T9M6fbJKMPqrqr9VfWhqtpaVduBI4Fh/wublsuALzTLn2fwSxbG+zMq0zKq5lk3su4kpwPXANur6htTqG0lq17v5nbRDyc5erWDbarwB36awZsZe5a03QacnuTVzQ/9PwO+OpXqRhtWN0leBfwLBn8OexYNq/v/Am9rPoVyBIM3IR+bSnWjvaLu5vvjiGb5fGB/Vc3S98leBt+7AG/je7+YbgF+qbneZwMvVtUs3PKB0TXPuqF1JzmJQbi+t6r+ckq1rWRU3W9u3uOi+TTY4cDqv7im9W72Ku9038jgnuZ3GLzyubxpvx5435D+/xJ4BNgN/OYmqvtc4N7NdL0ZfLLg8831/irwbzdJ3VsY/JnwR4H/weDP385M3QxuWe5k8N7EfcCPN33DYNKkrwMPA/OboObXN31eYvDG+h7gBzZB3dcALwC7msfCJvke+Ujz87gLuAd4yzjn8M87SFIPbbbbPpKkDhj+ktRDhr8k9ZDhL0k9ZPhLUg8Z/pLUQ4a/JPXQ/wdIylFa5vAncQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin2_predicted_scaled=best_model.predict(X_test_bin2)\n",
    "#print(Y_test_bin2_predicted)\n",
    "Y_test_bin2_predicted = min_max_scaler.inverse_transform(Y_test_bin2_predicted_scaled)\n",
    "error_prediction_bin2=Y_test_bin2-Y_test_bin2_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin2, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin2=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6651092471274562\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin3_predicted_scaled=best_model.predict(X_test_bin3)\n",
    "#print(Y_test_bin3_predicted)\n",
    "Y_test_bin3_predicted = min_max_scaler.inverse_transform(Y_test_bin3_predicted_scaled)\n",
    "error_prediction_bin3=Y_test_bin3-Y_test_bin3_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin3, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin3=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgadea/anaconda3/envs/tensorflow3/lib/python3.6/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6651092471274562\n",
      "1.6651092471274562\n",
      "1.6651092471274562\n",
      "1.6651092471274562\n",
      "1.6651092471274562\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_test_bin4_predicted_scaled=best_model.predict(X_test_bin4)\n",
    "#print(Y_test_bin4_predicted)\n",
    "Y_test_bin4_predicted = min_max_scaler.inverse_transform(Y_test_bin4_predicted_scaled)\n",
    "error_prediction_bin4=Y_test_bin4-Y_test_bin4_predicted.flatten()\n",
    "n, bins, patches = plt.hist(error_prediction_bin4, 400, normed=0, facecolor='green', alpha=0.75)\n",
    "\n",
    "x=bins[:400]\n",
    "y=n\n",
    "\n",
    "gmodel = Model(gaussian)\n",
    "result = gmodel.fit(y, x=x, amp=1000, cen=0, wid=1)\n",
    "FWHM_bin4=result.params['wid'].value*2*sqrt(log(2))\n",
    "print(FWHM_bin4)\n",
    "print(FWHM_bin3)\n",
    "print(FWHM_bin2)\n",
    "print(FWHM_bin1)\n",
    "print(FWHM_bin0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora los histogramnas 2d que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3] *",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "495px",
    "left": "909px",
    "right": "57px",
    "top": "246px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
